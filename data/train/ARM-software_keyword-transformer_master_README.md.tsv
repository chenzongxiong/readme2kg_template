#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Keyword Transformer: A Self-Attention Model for Keyword Spotting
#Text=
#Text=<img src="kwt.png" alt="drawing" width="200"/>
#Text=
#Text=This is the official repository for the paper [Keyword Transformer: A Self-Attention Model for Keyword Spotting](https://arxiv.org/abs/2104.00769), presented at Interspeech 2021.
1-1	0-1	#	_	_	
1-2	2-9	Keyword	*[1]	PUBLICATION[1]	
1-3	10-21	Transformer	*[1]	PUBLICATION[1]	
1-4	21-22	:	*[1]	PUBLICATION[1]	
1-5	23-24	A	*[1]	PUBLICATION[1]	
1-6	25-39	Self-Attention	*[1]	PUBLICATION[1]	
1-7	40-45	Model	*[1]	PUBLICATION[1]	
1-8	46-49	for	*[1]	PUBLICATION[1]	
1-9	50-57	Keyword	*[1]	PUBLICATION[1]	
1-10	58-66	Spotting	*[1]	PUBLICATION[1]	
1-11	68-69	<	_	_	
1-12	69-72	img	_	_	
1-13	73-76	src	_	_	
1-14	76-77	=	_	_	
1-15	77-78	"	_	_	
1-16	78-85	kwt.png	_	_	
1-17	85-86	"	_	_	
1-18	87-90	alt	_	_	
1-19	90-91	=	_	_	
1-20	91-92	"	_	_	
1-21	92-99	drawing	_	_	
1-22	99-100	"	_	_	
1-23	101-106	width	_	_	
1-24	106-107	=	_	_	
1-25	107-108	"	_	_	
1-26	108-111	200	_	_	
1-27	111-112	"	_	_	
1-28	112-113	/	_	_	
1-29	113-114	>	_	_	
1-30	116-120	This	_	_	
1-31	121-123	is	_	_	
1-32	124-127	the	_	_	
1-33	128-136	official	_	_	
1-34	137-147	repository	_	_	
1-35	148-151	for	_	_	
1-36	152-155	the	_	_	
1-37	156-161	paper	_	_	
1-38	162-163	[	_	_	
1-39	163-170	Keyword	*[2]	PUBLICATION[2]	
1-40	171-182	Transformer	*[2]	PUBLICATION[2]	
1-41	182-183	:	*[2]	PUBLICATION[2]	
1-42	184-185	A	*[2]	PUBLICATION[2]	
1-43	186-200	Self-Attention	*[2]	PUBLICATION[2]	
1-44	201-206	Model	*[2]	PUBLICATION[2]	
1-45	207-210	for	*[2]	PUBLICATION[2]	
1-46	211-218	Keyword	*[2]	PUBLICATION[2]	
1-47	219-227	Spotting	*[2]	PUBLICATION[2]	
1-48	227-228	]	_	_	
1-49	228-229	(	_	_	
1-50	229-234	https	_	_	
1-51	234-235	:	_	_	
1-52	235-236	/	_	_	
1-53	236-237	/	_	_	
1-54	237-246	arxiv.org	_	_	
1-55	246-247	/	_	_	
1-56	247-250	abs	_	_	
1-57	250-251	/	_	_	
1-58	251-261	2104.00769	_	_	
1-59	261-262	)	_	_	
1-60	262-263	,	_	_	
1-61	264-273	presented	_	_	
1-62	274-276	at	_	_	
1-63	277-288	Interspeech	*[3]	CONFERENCE[3]	
1-64	289-293	2021	*[3]	CONFERENCE[3]	
1-65	293-294	.	_	_	

#Text=Consider citing our paper if you find this work useful.
#Text=
#Text=```
#Text=@inproceedings{berg21_interspeech,
#Text=  author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},
#Text=  title={{Keyword Transformer: A Self-Attention Model for Keyword Spotting}},
#Text=  year=2021,
#Text=  booktitle={Proc.
2-1	295-303	Consider	_	_	
2-2	304-310	citing	_	_	
2-3	311-314	our	_	_	
2-4	315-320	paper	_	_	
2-5	321-323	if	_	_	
2-6	324-327	you	_	_	
2-7	328-332	find	_	_	
2-8	333-337	this	_	_	
2-9	338-342	work	_	_	
2-10	343-349	useful	_	_	
2-11	349-350	.	_	_	
2-12	352-353	`	_	_	
2-13	353-354	`	_	_	
2-14	354-355	`	_	_	
2-15	356-357	@	_	_	
2-16	357-370	inproceedings	_	_	
2-17	370-371	{	_	_	
2-18	371-377	berg21	_	_	
2-19	377-378	_	_	_	
2-20	378-389	interspeech	*	CONFERENCE	
2-21	389-390	,	_	_	
2-22	393-399	author	_	_	
2-23	399-400	=	_	_	
2-24	400-401	{	_	_	
2-25	401-405	Axel	_	_	
2-26	406-410	Berg	_	_	
2-27	411-414	and	_	_	
2-28	415-419	Mark	_	_	
2-29	420-421	O	_	_	
2-30	421-422	’	_	_	
2-31	422-428	Connor	_	_	
2-32	429-432	and	_	_	
2-33	433-439	Miguel	_	_	
2-34	440-446	Tairum	_	_	
2-35	447-451	Cruz	_	_	
2-36	451-452	}	_	_	
2-37	452-453	,	_	_	
2-38	456-461	title	_	_	
2-39	461-462	=	_	_	
2-40	462-463	{	_	_	
2-41	463-464	{	_	_	
2-42	464-471	Keyword	*[4]	PUBLICATION[4]	
2-43	472-483	Transformer	*[4]	PUBLICATION[4]	
2-44	483-484	:	*[4]	PUBLICATION[4]	
2-45	485-486	A	*[4]	PUBLICATION[4]	
2-46	487-501	Self-Attention	*[4]	PUBLICATION[4]	
2-47	502-507	Model	*[4]	PUBLICATION[4]	
2-48	508-511	for	*[4]	PUBLICATION[4]	
2-49	512-519	Keyword	*[4]	PUBLICATION[4]	
2-50	520-528	Spotting	*[4]	PUBLICATION[4]	
2-51	528-529	}	_	_	
2-52	529-530	}	_	_	
2-53	530-531	,	_	_	
2-54	534-538	year	_	_	
2-55	538-539	=	_	_	
2-56	539-543	2021	_	_	
2-57	543-544	,	_	_	
2-58	547-556	booktitle	_	_	
2-59	556-557	=	_	_	
2-60	557-558	{	_	_	
2-61	558-562	Proc	*[5]	PUBLICATION[5]	
2-62	562-563	.	*[5]	PUBLICATION[5]	

#Text=Interspeech 2021},
#Text=  pages={4249--4253},
#Text=  doi={10.21437/Interspeech.2021-1286}
#Text=}
#Text=```
#Text=
#Text=## Setup
#Text=
#Text=### Download Google Speech Commands
#Text=
#Text=There are two versions of the dataset, V1 and V2.
3-1	564-575	Interspeech	*[5]|*[6]	PUBLICATION[5]|CONFERENCE[6]	
3-2	576-580	2021	*[5]|*[6]	PUBLICATION[5]|CONFERENCE[6]	
3-3	580-581	}	_	_	
3-4	581-582	,	_	_	
3-5	585-590	pages	_	_	
3-6	590-591	=	_	_	
3-7	591-592	{	_	_	
3-8	592-596	4249	_	_	
3-9	596-597	-	_	_	
3-10	597-598	-	_	_	
3-11	598-602	4253	_	_	
3-12	602-603	}	_	_	
3-13	603-604	,	_	_	
3-14	607-610	doi	_	_	
3-15	610-611	=	_	_	
3-16	611-612	{	_	_	
3-17	612-620	10.21437	_	_	
3-18	620-621	/	_	_	
3-19	621-632	Interspeech	*	CONFERENCE	
3-20	632-637	.2021	_	_	
3-21	637-638	-	_	_	
3-22	638-642	1286	_	_	
3-23	642-643	}	_	_	
3-24	644-645	}	_	_	
3-25	646-647	`	_	_	
3-26	647-648	`	_	_	
3-27	648-649	`	_	_	
3-28	651-652	#	_	_	
3-29	652-653	#	_	_	
3-30	654-659	Setup	_	_	
3-31	661-662	#	_	_	
3-32	662-663	#	_	_	
3-33	663-664	#	_	_	
3-34	665-673	Download	_	_	
3-35	674-680	Google	*[7]	DATASET[7]	
3-36	681-687	Speech	*[7]	DATASET[7]	
3-37	688-696	Commands	*[7]	DATASET[7]	
3-38	698-703	There	_	_	
3-39	704-707	are	_	_	
3-40	708-711	two	_	_	
3-41	712-720	versions	_	_	
3-42	721-723	of	_	_	
3-43	724-727	the	_	_	
3-44	728-735	dataset	_	_	
3-45	735-736	,	_	_	
3-46	737-739	V1	_	_	
3-47	740-743	and	_	_	
3-48	744-746	V2	_	_	
3-49	746-747	.	_	_	

#Text=To download and extract dataset V2, run:
#Text=
#Text=```shell
#Text=wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz
#Text=mkdir data2
#Text=mv .
4-1	748-750	To	_	_	
4-2	751-759	download	_	_	
4-3	760-763	and	_	_	
4-4	764-771	extract	_	_	
4-5	772-779	dataset	_	_	
4-6	780-782	V2	_	_	
4-7	782-783	,	_	_	
4-8	784-787	run	_	_	
4-9	787-788	:	_	_	
4-10	790-791	`	_	_	
4-11	791-792	`	_	_	
4-12	792-793	`	_	_	
4-13	793-798	shell	_	_	
4-14	799-803	wget	*	SOFTWARE	
4-15	804-809	https	_	_	
4-16	809-810	:	_	_	
4-17	810-811	/	_	_	
4-18	811-812	/	_	_	
4-19	812-834	storage.googleapis.com	_	_	
4-20	834-835	/	_	_	
4-21	835-858	download.tensorflow.org	_	_	
4-21.1	844-854	tensorflow	*	SOFTWARE	
4-22	858-859	/	_	_	
4-23	859-863	data	_	_	
4-24	863-864	/	_	_	
4-25	864-885	speech_commands_v0.02	*	DATASET	
4-26	885-886	.	_	_	
4-27	886-892	tar.gz	_	_	
4-28	893-898	mkdir	_	_	
4-29	899-904	data2	_	_	
4-30	905-907	mv	_	_	
4-31	908-909	.	_	_	

#Text=/speech_commands_v0.02.tar.gz .
5-1	909-910	/	_	_	
5-2	910-931	speech_commands_v0.02	*	DATASET	
5-3	931-932	.	_	_	
5-4	932-938	tar.gz	_	_	
5-5	939-940	.	_	_	

#Text=/data2
#Text=cd .
6-1	940-941	/	_	_	
6-2	941-946	data2	_	_	
6-3	947-949	cd	_	_	
6-4	950-951	.	_	_	

#Text=/data2
#Text=tar -xf .
7-1	951-952	/	_	_	
7-2	952-957	data2	_	_	
7-3	958-961	tar	_	_	
7-4	962-963	-	_	_	
7-5	963-965	xf	_	_	
7-6	966-967	.	_	_	

#Text=/speech_commands_v0.02.tar.gz
#Text=cd ../
#Text=```
#Text=
#Text=And similarly for V1:
#Text=
#Text=```shell
#Text=wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz
#Text=mkdir data1
#Text=mv .
8-1	967-968	/	_	_	
8-2	968-989	speech_commands_v0.02	*	DATASET	
8-3	989-990	.	_	_	
8-4	990-996	tar.gz	_	_	
8-5	997-999	cd	_	_	
8-6	1000-1001	.	_	_	
8-7	1001-1002	.	_	_	
8-8	1002-1003	/	_	_	
8-9	1004-1005	`	_	_	
8-10	1005-1006	`	_	_	
8-11	1006-1007	`	_	_	
8-12	1009-1012	And	_	_	
8-13	1013-1022	similarly	_	_	
8-14	1023-1026	for	_	_	
8-15	1027-1029	V1	_	_	
8-16	1029-1030	:	_	_	
8-17	1032-1033	`	_	_	
8-18	1033-1034	`	_	_	
8-19	1034-1035	`	_	_	
8-20	1035-1040	shell	_	_	
8-21	1041-1045	wget	*	DATASET	
8-22	1046-1050	http	_	_	
8-23	1050-1051	:	_	_	
8-24	1051-1052	/	_	_	
8-25	1052-1053	/	_	_	
8-26	1053-1076	download.tensorflow.org	_	_	
8-26.1	1062-1072	tensorflow	*	SOFTWARE	
8-27	1076-1077	/	_	_	
8-28	1077-1081	data	_	_	
8-29	1081-1082	/	_	_	
8-30	1082-1103	speech_commands_v0.01	*	DATASET	
8-31	1103-1104	.	_	_	
8-32	1104-1110	tar.gz	_	_	
8-33	1111-1116	mkdir	_	_	
8-34	1117-1122	data1	_	_	
8-35	1123-1125	mv	_	_	
8-36	1126-1127	.	_	_	

#Text=/speech_commands_v0.01.tar.gz .
9-1	1127-1128	/	_	_	
9-2	1128-1149	speech_commands_v0.01	*	DATASET	
9-3	1149-1150	.	_	_	
9-4	1150-1156	tar.gz	_	_	
9-5	1157-1158	.	_	_	

#Text=/data1
#Text=cd .
10-1	1158-1159	/	_	_	
10-2	1159-1164	data1	_	_	
10-3	1165-1167	cd	_	_	
10-4	1168-1169	.	_	_	

#Text=/data1
#Text=tar -xf .
11-1	1169-1170	/	_	_	
11-2	1170-1175	data1	_	_	
11-3	1176-1179	tar	_	_	
11-4	1180-1181	-	_	_	
11-5	1181-1183	xf	_	_	
11-6	1184-1185	.	_	_	

#Text=/speech_commands_v0.01.tar.gz
#Text=cd ../
#Text=```
#Text=
#Text=### Install dependencies
#Text=
#Text=Set up a new virtual environment:
#Text=
#Text=```shell
#Text=pip install virtualenv
#Text=virtualenv --system-site-packages -p python3 .
12-1	1185-1186	/	_	_	
12-2	1186-1207	speech_commands_v0.01	*	DATASET	
12-3	1207-1208	.	_	_	
12-4	1208-1214	tar.gz	_	_	
12-5	1215-1217	cd	_	_	
12-6	1218-1219	.	_	_	
12-7	1219-1220	.	_	_	
12-8	1220-1221	/	_	_	
12-9	1222-1223	`	_	_	
12-10	1223-1224	`	_	_	
12-11	1224-1225	`	_	_	
12-12	1227-1228	#	_	_	
12-13	1228-1229	#	_	_	
12-14	1229-1230	#	_	_	
12-15	1231-1238	Install	_	_	
12-16	1239-1251	dependencies	_	_	
12-17	1253-1256	Set	_	_	
12-18	1257-1259	up	_	_	
12-19	1260-1261	a	_	_	
12-20	1262-1265	new	_	_	
12-21	1266-1273	virtual	_	_	
12-22	1274-1285	environment	_	_	
12-23	1285-1286	:	_	_	
12-24	1288-1289	`	_	_	
12-25	1289-1290	`	_	_	
12-26	1290-1291	`	_	_	
12-27	1291-1296	shell	_	_	
12-28	1297-1300	pip	*	SOFTWARE	
12-29	1301-1308	install	_	_	
12-30	1309-1319	virtualenv	*	SOFTWARE	
12-31	1320-1330	virtualenv	*	SOFTWARE	
12-32	1331-1332	-	_	_	
12-33	1332-1333	-	_	_	
12-34	1333-1353	system-site-packages	_	_	
12-35	1354-1355	-	_	_	
12-36	1355-1356	p	_	_	
12-37	1357-1364	python3	*	SOFTWARE	
12-38	1365-1366	.	_	_	

#Text=/venv3
#Text=source .
13-1	1366-1367	/	_	_	
13-2	1367-1372	venv3	_	_	
13-3	1373-1379	source	_	_	
13-4	1380-1381	.	_	_	

#Text=/venv3/bin/activate
#Text=```
#Text=
#Text=To install dependencies, run
#Text=
#Text=```shell
#Text=pip install -r requirements.txt
#Text=```
#Text=
#Text=Tested using Tensorflow 2.4.0rc1 with CUDA 11.
14-1	1381-1382	/	_	_	
14-2	1382-1387	venv3	_	_	
14-3	1387-1388	/	_	_	
14-4	1388-1391	bin	_	_	
14-5	1391-1392	/	_	_	
14-6	1392-1400	activate	_	_	
14-7	1401-1402	`	_	_	
14-8	1402-1403	`	_	_	
14-9	1403-1404	`	_	_	
14-10	1406-1408	To	_	_	
14-11	1409-1416	install	_	_	
14-12	1417-1429	dependencies	_	_	
14-13	1429-1430	,	_	_	
14-14	1431-1434	run	_	_	
14-15	1436-1437	`	_	_	
14-16	1437-1438	`	_	_	
14-17	1438-1439	`	_	_	
14-18	1439-1444	shell	_	_	
14-19	1445-1448	pip	*	SOFTWARE	
14-20	1449-1456	install	_	_	
14-21	1457-1458	-	_	_	
14-22	1458-1459	r	_	_	
14-23	1460-1476	requirements.txt	_	_	
14-24	1477-1478	`	_	_	
14-25	1478-1479	`	_	_	
14-26	1479-1480	`	_	_	
14-27	1482-1488	Tested	_	_	
14-28	1489-1494	using	_	_	
14-29	1495-1505	Tensorflow	*[8]	SOFTWARE[8]	
14-30	1506-1514	2.4.0rc1	*[8]	SOFTWARE[8]	
14-31	1515-1519	with	_	_	
14-32	1520-1524	CUDA	*[9]	SOFTWARE[9]	
14-33	1525-1527	11	*[9]	SOFTWARE[9]	
14-34	1527-1528	.	_	_	

#Text=**Note**: Installing the correct Tensorflow version is important for reproducibility!
15-1	1530-1531	*	_	_	
15-2	1531-1532	*	_	_	
15-3	1532-1536	Note	_	_	
15-4	1536-1537	*	_	_	
15-5	1537-1538	*	_	_	
15-6	1538-1539	:	_	_	
15-7	1540-1550	Installing	_	_	
15-8	1551-1554	the	_	_	
15-9	1555-1562	correct	_	_	
15-10	1563-1573	Tensorflow	*	SOFTWARE	
15-11	1574-1581	version	_	_	
15-12	1582-1584	is	_	_	
15-13	1585-1594	important	_	_	
15-14	1595-1598	for	_	_	
15-15	1599-1614	reproducibility	_	_	
15-16	1614-1615	!	_	_	

#Text=Using more recent versions of Tensorflow results in small accuracy differences each time the model is evaluated.
16-1	1616-1621	Using	_	_	
16-2	1622-1626	more	_	_	
16-3	1627-1633	recent	_	_	
16-4	1634-1642	versions	_	_	
16-5	1643-1645	of	_	_	
16-6	1646-1656	Tensorflow	*	SOFTWARE	
16-7	1657-1664	results	_	_	
16-8	1665-1667	in	_	_	
16-9	1668-1673	small	_	_	
16-10	1674-1682	accuracy	_	_	
16-11	1683-1694	differences	_	_	
16-12	1695-1699	each	_	_	
16-13	1700-1704	time	_	_	
16-14	1705-1708	the	_	_	
16-15	1709-1714	model	_	_	
16-16	1715-1717	is	_	_	
16-17	1718-1727	evaluated	_	_	
16-18	1727-1728	.	_	_	

#Text=This might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the "unknown"  keyword class.
#Text=
#Text=## Model
#Text=The Keyword-Transformer model is defined [here](kws_streaming/models/kws_transformer.py).
17-1	1729-1733	This	_	_	
17-2	1734-1739	might	_	_	
17-3	1740-1742	be	_	_	
17-4	1743-1746	due	_	_	
17-5	1747-1749	to	_	_	
17-6	1750-1751	a	_	_	
17-7	1752-1758	change	_	_	
17-8	1759-1761	in	_	_	
17-9	1762-1765	how	_	_	
17-10	1766-1769	the	_	_	
17-11	1770-1776	random	_	_	
17-12	1777-1781	seed	_	_	
17-13	1782-1791	generator	_	_	
17-14	1792-1794	is	_	_	
17-15	1795-1806	implemented	_	_	
17-16	1806-1807	,	_	_	
17-17	1808-1811	and	_	_	
17-18	1812-1821	therefore	_	_	
17-19	1822-1829	changes	_	_	
17-20	1830-1833	the	_	_	
17-21	1834-1842	sampling	_	_	
17-22	1843-1845	of	_	_	
17-23	1846-1849	the	_	_	
17-24	1850-1851	"	_	_	
17-25	1851-1858	unknown	_	_	
17-26	1858-1859	"	_	_	
17-27	1861-1868	keyword	_	_	
17-28	1869-1874	class	_	_	
17-29	1874-1875	.	_	_	
17-30	1877-1878	#	_	_	
17-31	1878-1879	#	_	_	
17-32	1880-1885	Model	_	_	
17-33	1886-1889	The	_	_	
17-34	1890-1909	Keyword-Transformer	_	_	
17-35	1910-1915	model	_	_	
17-36	1916-1918	is	_	_	
17-37	1919-1926	defined	_	_	
17-38	1927-1928	[	_	_	
17-39	1928-1932	here	_	_	
17-40	1932-1933	]	_	_	
17-41	1933-1934	(	_	_	
17-42	1934-1947	kws_streaming	_	_	
17-43	1947-1948	/	_	_	
17-44	1948-1954	models	_	_	
17-45	1954-1955	/	_	_	
17-46	1955-1973	kws_transformer.py	_	_	
17-47	1973-1974	)	_	_	
17-48	1974-1975	.	_	_	

#Text=It takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.
18-1	1976-1978	It	_	_	
18-2	1979-1984	takes	_	_	
18-3	1985-1988	the	_	_	
18-4	1989-1992	mel	_	_	
18-5	1993-1998	scale	_	_	
18-6	1999-2010	spectrogram	_	_	
18-7	2011-2013	as	_	_	
18-8	2014-2019	input	_	_	
18-9	2019-2020	,	_	_	
18-10	2021-2026	which	_	_	
18-11	2027-2030	has	_	_	
18-12	2031-2036	shape	_	_	
18-13	2037-2039	98	_	_	
18-14	2040-2041	x	_	_	
18-15	2042-2044	40	_	_	
18-16	2045-2050	using	_	_	
18-17	2051-2054	the	_	_	
18-18	2055-2062	default	_	_	
18-19	2063-2071	settings	_	_	
18-20	2071-2072	,	_	_	
18-21	2073-2086	corresponding	_	_	
18-22	2087-2089	to	_	_	
18-23	2090-2093	the	_	_	
18-24	2094-2096	98	_	_	
18-25	2097-2101	time	_	_	
18-26	2102-2109	windows	_	_	
18-27	2110-2114	with	_	_	
18-28	2115-2117	40	_	_	
18-29	2118-2127	frequency	_	_	
18-30	2128-2140	coefficients	_	_	
18-31	2140-2141	.	_	_	

#Text=There are three variants of the Keyword-Transformer model:
#Text=
#Text=* **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows
#Text=* **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies
#Text=* **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined
#Text=* **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.
#Text=
#Text=## Training a model from scratch
#Text=To train KWT-3 from scratch on Speech Commands V2, run  
#Text=
#Text=```shell
#Text=sh train.sh
#Text=```
#Text=
#Text=Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script.
19-1	2143-2148	There	_	_	
19-2	2149-2152	are	_	_	
19-3	2153-2158	three	_	_	
19-4	2159-2167	variants	_	_	
19-5	2168-2170	of	_	_	
19-6	2171-2174	the	_	_	
19-7	2175-2194	Keyword-Transformer	_	_	
19-8	2195-2200	model	_	_	
19-9	2200-2201	:	_	_	
19-10	2203-2204	*	_	_	
19-11	2205-2206	*	_	_	
19-12	2206-2207	*	_	_	
19-13	2207-2218	Time-domain	_	_	
19-14	2219-2228	attention	_	_	
19-15	2228-2229	*	_	_	
19-16	2229-2230	*	_	_	
19-17	2230-2231	:	_	_	
19-18	2232-2236	each	_	_	
19-19	2237-2248	time-window	_	_	
19-20	2249-2251	is	_	_	
19-21	2252-2259	treated	_	_	
19-22	2260-2262	as	_	_	
19-23	2263-2264	a	_	_	
19-24	2265-2270	patch	_	_	
19-25	2270-2271	,	_	_	
19-26	2272-2286	self-attention	_	_	
19-27	2287-2289	is	_	_	
19-28	2290-2298	computed	_	_	
19-29	2299-2306	between	_	_	
19-30	2307-2319	time-windows	_	_	
19-31	2320-2321	*	_	_	
19-32	2322-2323	*	_	_	
19-33	2323-2324	*	_	_	
19-34	2324-2340	Frequency-domain	_	_	
19-35	2341-2350	attention	_	_	
19-36	2350-2351	*	_	_	
19-37	2351-2352	*	_	_	
19-38	2352-2353	:	_	_	
19-39	2354-2358	each	_	_	
19-40	2359-2368	frequency	_	_	
19-41	2369-2371	is	_	_	
19-42	2372-2379	treated	_	_	
19-43	2380-2382	as	_	_	
19-44	2383-2384	a	_	_	
19-45	2385-2390	patch	_	_	
19-46	2391-2405	self-attention	_	_	
19-47	2406-2408	is	_	_	
19-48	2409-2417	computed	_	_	
19-49	2418-2425	between	_	_	
19-50	2426-2437	frequencies	_	_	
19-51	2438-2439	*	_	_	
19-52	2440-2441	*	_	_	
19-53	2441-2442	*	_	_	
19-54	2442-2453	Combination	_	_	
19-55	2454-2456	of	_	_	
19-56	2457-2461	both	_	_	
19-57	2461-2462	*	_	_	
19-58	2462-2463	*	_	_	
19-59	2463-2464	:	_	_	
19-60	2465-2468	The	_	_	
19-61	2469-2475	signal	_	_	
19-62	2476-2478	is	_	_	
19-63	2479-2482	fed	_	_	
19-64	2483-2487	into	_	_	
19-65	2488-2492	both	_	_	
19-66	2493-2494	a	_	_	
19-67	2495-2499	time	_	_	
19-68	2499-2500	-	_	_	
19-69	2501-2504	and	_	_	
19-70	2505-2506	a	_	_	
19-71	2507-2523	frequency-domain	_	_	
19-72	2524-2535	transformer	_	_	
19-73	2536-2539	and	_	_	
19-74	2540-2543	the	_	_	
19-75	2544-2551	outputs	_	_	
19-76	2552-2555	are	_	_	
19-77	2556-2564	combined	_	_	
19-78	2565-2566	*	_	_	
19-79	2567-2568	*	_	_	
19-80	2568-2569	*	_	_	
19-81	2569-2579	Patch-wise	_	_	
19-82	2580-2589	attention	_	_	
19-83	2589-2590	*	_	_	
19-84	2590-2591	*	_	_	
19-85	2591-2592	:	_	_	
19-86	2593-2600	Similar	_	_	
19-87	2601-2603	to	_	_	
19-88	2604-2607	the	_	_	
19-89	2608-2614	vision	_	_	
19-90	2615-2626	transformer	_	_	
19-91	2626-2627	,	_	_	
19-92	2628-2630	it	_	_	
19-93	2631-2639	extracts	_	_	
19-94	2640-2651	rectangular	_	_	
19-95	2652-2659	patches	_	_	
19-96	2660-2664	from	_	_	
19-97	2665-2668	the	_	_	
19-98	2669-2680	spectrogram	_	_	
19-99	2680-2681	,	_	_	
19-100	2682-2684	so	_	_	
19-101	2685-2694	attention	_	_	
19-102	2695-2702	happens	_	_	
19-103	2703-2707	both	_	_	
19-104	2708-2710	in	_	_	
19-105	2711-2714	the	_	_	
19-106	2715-2719	time	_	_	
19-107	2720-2723	and	_	_	
19-108	2724-2733	frequency	_	_	
19-109	2734-2740	domain	_	_	
19-110	2741-2755	simultaneously	_	_	
19-111	2755-2756	.	_	_	
19-112	2758-2759	#	_	_	
19-113	2759-2760	#	_	_	
19-114	2761-2769	Training	_	_	
19-115	2770-2771	a	_	_	
19-116	2772-2777	model	_	_	
19-117	2778-2782	from	_	_	
19-118	2783-2790	scratch	_	_	
19-119	2791-2793	To	_	_	
19-120	2794-2799	train	_	_	
19-121	2800-2803	KWT	_	_	
19-122	2803-2804	-	_	_	
19-123	2804-2805	3	_	_	
19-124	2806-2810	from	_	_	
19-125	2811-2818	scratch	_	_	
19-126	2819-2821	on	_	_	
19-127	2822-2828	Speech	*[10]	DATASET[10]	
19-128	2829-2837	Commands	*[10]	DATASET[10]	
19-129	2838-2840	V2	*[10]	DATASET[10]	
19-130	2840-2841	,	_	_	
19-131	2842-2845	run	_	_	
19-132	2849-2850	`	_	_	
19-133	2850-2851	`	_	_	
19-134	2851-2852	`	_	_	
19-135	2852-2857	shell	_	_	
19-136	2858-2860	sh	_	_	
19-137	2861-2869	train.sh	_	_	
19-138	2870-2871	`	_	_	
19-139	2871-2872	`	_	_	
19-140	2872-2873	`	_	_	
19-141	2875-2881	Please	_	_	
19-142	2882-2886	note	_	_	
19-143	2887-2891	that	_	_	
19-144	2892-2895	the	_	_	
19-145	2896-2901	train	_	_	
19-146	2902-2911	directory	_	_	
19-147	2912-2913	(	_	_	
19-148	2913-2918	given	_	_	
19-149	2919-2921	by	_	_	
19-150	2922-2925	the	_	_	
19-151	2926-2934	argument	_	_	
19-152	2936-2937	`	_	_	
19-153	2937-2938	-	_	_	
19-154	2938-2939	-	_	_	
19-155	2939-2948	train_dir	_	_	
19-156	2948-2949	`	_	_	
19-157	2949-2950	)	_	_	
19-158	2951-2957	cannot	_	_	
19-159	2958-2963	exist	_	_	
19-160	2964-2969	prior	_	_	
19-161	2970-2972	to	_	_	
19-162	2973-2978	start	_	_	
19-163	2979-2985	script	_	_	
19-164	2985-2986	.	_	_	

#Text=The model-specific arguments for KWT are:
#Text=
#Text=```shell
#Text=--num_layers 12 \\ #number of sequential transformer encoders
#Text=--heads 3 \\ #number of attentions heads
#Text=--d_model 192 \\ #embedding dimension
#Text=--mlp_dim 768 \\ #mlp-dimension
#Text=--dropout1 0. \\ #dropout in mlp/multi-head attention blocks
#Text=--attention_type 'time' \\ #attention type: 'time', 'freq', 'both' or 'patch'
#Text=--patch_size '1,40' \\ #spectrogram patch_size, if patch attention is used
#Text=--prenorm False \\ # if False, use postnorm
#Text=```
#Text=
#Text=## Training with distillation
#Text=
#Text=We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).
20-1	2988-2991	The	_	_	
20-2	2992-3006	model-specific	_	_	
20-3	3007-3016	arguments	_	_	
20-4	3017-3020	for	_	_	
20-5	3021-3024	KWT	*	SOFTWARE	
20-6	3025-3028	are	_	_	
20-7	3028-3029	:	_	_	
20-8	3031-3032	`	_	_	
20-9	3032-3033	`	_	_	
20-10	3033-3034	`	_	_	
20-11	3034-3039	shell	_	_	
20-12	3040-3041	-	_	_	
20-13	3041-3042	-	_	_	
20-14	3042-3052	num_layers	_	_	
20-15	3053-3055	12	_	_	
20-16	3056-3057	\	_	_	
20-17	3058-3059	#	_	_	
20-18	3059-3065	number	_	_	
20-19	3066-3068	of	_	_	
20-20	3069-3079	sequential	_	_	
20-21	3080-3091	transformer	_	_	
20-22	3092-3100	encoders	_	_	
20-23	3101-3102	-	_	_	
20-24	3102-3103	-	_	_	
20-25	3103-3108	heads	_	_	
20-26	3109-3110	3	_	_	
20-27	3111-3112	\	_	_	
20-28	3113-3114	#	_	_	
20-29	3114-3120	number	_	_	
20-30	3121-3123	of	_	_	
20-31	3124-3134	attentions	_	_	
20-32	3135-3140	heads	_	_	
20-33	3141-3142	-	_	_	
20-34	3142-3143	-	_	_	
20-35	3143-3150	d_model	_	_	
20-36	3151-3154	192	_	_	
20-37	3155-3156	\	_	_	
20-38	3157-3158	#	_	_	
20-39	3158-3167	embedding	_	_	
20-40	3168-3177	dimension	_	_	
20-41	3178-3179	-	_	_	
20-42	3179-3180	-	_	_	
20-43	3180-3187	mlp_dim	_	_	
20-44	3188-3191	768	_	_	
20-45	3192-3193	\	_	_	
20-46	3194-3195	#	_	_	
20-47	3195-3208	mlp-dimension	_	_	
20-48	3209-3210	-	_	_	
20-49	3210-3211	-	_	_	
20-50	3211-3219	dropout1	_	_	
20-51	3220-3221	0	_	_	
20-52	3221-3222	.	_	_	
20-53	3223-3224	\	_	_	
20-54	3225-3226	#	_	_	
20-55	3226-3233	dropout	_	_	
20-56	3234-3236	in	_	_	
20-57	3237-3240	mlp	_	_	
20-58	3240-3241	/	_	_	
20-59	3241-3251	multi-head	_	_	
20-60	3252-3261	attention	_	_	
20-61	3262-3268	blocks	_	_	
20-62	3269-3270	-	_	_	
20-63	3270-3271	-	_	_	
20-64	3271-3285	attention_type	_	_	
20-65	3286-3287	'	_	_	
20-66	3287-3291	time	_	_	
20-67	3291-3292	'	_	_	
20-68	3293-3294	\	_	_	
20-69	3295-3296	#	_	_	
20-70	3296-3305	attention	_	_	
20-71	3306-3310	type	_	_	
20-72	3310-3311	:	_	_	
20-73	3312-3313	'	_	_	
20-74	3313-3317	time	_	_	
20-75	3317-3318	'	_	_	
20-76	3318-3319	,	_	_	
20-77	3320-3321	'	_	_	
20-78	3321-3325	freq	_	_	
20-79	3325-3326	'	_	_	
20-80	3326-3327	,	_	_	
20-81	3328-3329	'	_	_	
20-82	3329-3333	both	_	_	
20-83	3333-3334	'	_	_	
20-84	3335-3337	or	_	_	
20-85	3338-3339	'	_	_	
20-86	3339-3344	patch	_	_	
20-87	3344-3345	'	_	_	
20-88	3346-3347	-	_	_	
20-89	3347-3348	-	_	_	
20-90	3348-3358	patch_size	_	_	
20-91	3359-3360	'	_	_	
20-92	3360-3364	1,40	_	_	
20-93	3364-3365	'	_	_	
20-94	3366-3367	\	_	_	
20-95	3368-3369	#	_	_	
20-96	3369-3380	spectrogram	_	_	
20-97	3381-3391	patch_size	_	_	
20-98	3391-3392	,	_	_	
20-99	3393-3395	if	_	_	
20-100	3396-3401	patch	_	_	
20-101	3402-3411	attention	_	_	
20-102	3412-3414	is	_	_	
20-103	3415-3419	used	_	_	
20-104	3420-3421	-	_	_	
20-105	3421-3422	-	_	_	
20-106	3422-3429	prenorm	_	_	
20-107	3430-3435	False	_	_	
20-108	3436-3437	\	_	_	
20-109	3438-3439	#	_	_	
20-110	3440-3442	if	_	_	
20-111	3443-3448	False	_	_	
20-112	3448-3449	,	_	_	
20-113	3450-3453	use	_	_	
20-114	3454-3462	postnorm	_	_	
20-115	3463-3464	`	_	_	
20-116	3464-3465	`	_	_	
20-117	3465-3466	`	_	_	
20-118	3468-3469	#	_	_	
20-119	3469-3470	#	_	_	
20-120	3471-3479	Training	_	_	
20-121	3480-3484	with	_	_	
20-122	3485-3497	distillation	_	_	
20-123	3499-3501	We	_	_	
20-124	3502-3508	employ	_	_	
20-125	3509-3513	hard	_	_	
20-126	3514-3526	distillation	_	_	
20-127	3527-3531	from	_	_	
20-128	3532-3533	a	_	_	
20-129	3534-3547	convolutional	_	_	
20-130	3548-3553	model	_	_	
20-131	3554-3555	(	_	_	
20-132	3555-3565	Att-MH-RNN	*	SOFTWARE	
20-133	3565-3566	)	_	_	
20-134	3566-3567	,	_	_	
20-135	3568-3575	similar	_	_	
20-136	3576-3578	to	_	_	
20-137	3579-3582	the	_	_	
20-138	3583-3591	approach	_	_	
20-139	3592-3594	in	_	_	
20-140	3595-3596	[	_	_	
20-141	3596-3600	DeIT	*	SOFTWARE	
20-142	3600-3601	]	_	_	
20-143	3601-3602	(	_	_	
20-144	3602-3607	https	_	_	
20-145	3607-3608	:	_	_	
20-146	3608-3609	/	_	_	
20-147	3609-3610	/	_	_	
20-148	3610-3620	github.com	_	_	
20-149	3620-3621	/	_	_	
20-150	3621-3637	facebookresearch	_	_	
20-151	3637-3638	/	_	_	
20-152	3638-3642	deit	*	SOFTWARE	
20-153	3642-3643	)	_	_	
20-154	3643-3644	.	_	_	

#Text=To train KWT-3 with hard distillation from a pre-trained model, run
#Text=
#Text=```shell
#Text=sh distill.sh
#Text=```
#Text=
#Text=## Run inference using a pre-trained model
#Text=
#Text=Pre-trained weights for KWT-3, KWT-2 and KWT-1 are provided in .
21-1	3646-3648	To	_	_	
21-2	3649-3654	train	_	_	
21-3	3655-3658	KWT	_	_	
21-4	3658-3659	-	_	_	
21-5	3659-3660	3	_	_	
21-6	3661-3665	with	_	_	
21-7	3666-3670	hard	_	_	
21-8	3671-3683	distillation	_	_	
21-9	3684-3688	from	_	_	
21-10	3689-3690	a	_	_	
21-11	3691-3702	pre-trained	_	_	
21-12	3703-3708	model	_	_	
21-13	3708-3709	,	_	_	
21-14	3710-3713	run	_	_	
21-15	3715-3716	`	_	_	
21-16	3716-3717	`	_	_	
21-17	3717-3718	`	_	_	
21-18	3718-3723	shell	_	_	
21-19	3724-3726	sh	_	_	
21-20	3727-3737	distill.sh	_	_	
21-21	3738-3739	`	_	_	
21-22	3739-3740	`	_	_	
21-23	3740-3741	`	_	_	
21-24	3743-3744	#	_	_	
21-25	3744-3745	#	_	_	
21-26	3746-3749	Run	_	_	
21-27	3750-3759	inference	_	_	
21-28	3760-3765	using	_	_	
21-29	3766-3767	a	_	_	
21-30	3768-3779	pre-trained	_	_	
21-31	3780-3785	model	_	_	
21-32	3787-3798	Pre-trained	_	_	
21-33	3799-3806	weights	_	_	
21-34	3807-3810	for	_	_	
21-35	3811-3814	KWT	_	_	
21-36	3814-3815	-	_	_	
21-37	3815-3816	3	_	_	
21-38	3816-3817	,	_	_	
21-39	3818-3821	KWT	_	_	
21-40	3821-3822	-	_	_	
21-41	3822-3823	2	_	_	
21-42	3824-3827	and	_	_	
21-43	3828-3831	KWT	_	_	
21-44	3831-3832	-	_	_	
21-45	3832-3833	1	_	_	
21-46	3834-3837	are	_	_	
21-47	3838-3846	provided	_	_	
21-48	3847-3849	in	_	_	
21-49	3850-3851	.	_	_	

#Text=/models_data_v2_12_labels.
22-1	3851-3852	/	_	_	
22-2	3852-3866	models_data_v2	_	_	
22-3	3866-3867	_	_	_	
22-4	3867-3869	12	_	_	
22-5	3869-3870	_	_	_	
22-6	3870-3876	labels	_	_	
22-7	3876-3877	.	_	_	

#Text=|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained|
#Text=|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
#Text=|KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)|
#Text=|KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)|
#Text=|KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|
#Text=
#Text=To perform inference on Google Speech Commands v2 with 12 labels, run
#Text=
#Text=```shell
#Text=sh eval.sh
#Text=```
#Text=
#Text=## Acknowledgements
#Text=
#Text=The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.
23-1	3879-3880	|	_	_	
23-2	3880-3885	Model	_	_	
23-3	3886-3890	name	_	_	
23-4	3890-3891	|	_	_	
23-5	3891-3900	embedding	_	_	
23-6	3901-3904	dim	_	_	
23-7	3904-3905	|	_	_	
23-8	3905-3912	mlp-dim	_	_	
23-9	3912-3913	|	_	_	
23-10	3913-3918	heads	_	_	
23-11	3918-3919	|	_	_	
23-12	3919-3924	depth	_	_	
23-13	3924-3925	|	_	_	
23-14	3925-3926	#	_	_	
23-15	3926-3932	params	_	_	
23-16	3932-3933	|	_	_	
23-17	3933-3935	V2	_	_	
23-18	3935-3936	-	_	_	
23-19	3936-3938	12	_	_	
23-20	3939-3947	accuracy	_	_	
23-21	3947-3948	|	_	_	
23-22	3948-3959	pre-trained	_	_	
23-23	3959-3960	|	_	_	
23-24	3961-3962	|	_	_	
23-25	3962-3963	:	_	_	
23-26	3963-3964	-	_	_	
23-27	3964-3965	:	_	_	
23-28	3965-3966	|	_	_	
23-29	3966-3967	:	_	_	
23-30	3967-3968	-	_	_	
23-31	3968-3969	:	_	_	
23-32	3969-3970	|	_	_	
23-33	3970-3971	:	_	_	
23-34	3971-3972	-	_	_	
23-35	3972-3973	:	_	_	
23-36	3973-3974	|	_	_	
23-37	3974-3975	:	_	_	
23-38	3975-3976	-	_	_	
23-39	3976-3977	:	_	_	
23-40	3977-3978	|	_	_	
23-41	3978-3979	:	_	_	
23-42	3979-3980	-	_	_	
23-43	3980-3981	:	_	_	
23-44	3981-3982	|	_	_	
23-45	3982-3983	:	_	_	
23-46	3983-3984	-	_	_	
23-47	3984-3985	:	_	_	
23-48	3985-3986	|	_	_	
23-49	3986-3987	:	_	_	
23-50	3987-3988	-	_	_	
23-51	3988-3989	:	_	_	
23-52	3989-3990	|	_	_	
23-53	3990-3991	:	_	_	
23-54	3991-3992	-	_	_	
23-55	3992-3993	:	_	_	
23-56	3993-3994	|	_	_	
23-57	3995-3996	|	_	_	
23-58	3996-3999	KWT	_	_	
23-59	3999-4000	-	_	_	
23-60	4000-4001	1	_	_	
23-61	4001-4002	|	_	_	
23-62	4002-4004	64	_	_	
23-63	4004-4005	|	_	_	
23-64	4005-4008	128	_	_	
23-65	4008-4009	|	_	_	
23-66	4009-4010	1	_	_	
23-67	4010-4011	|	_	_	
23-68	4011-4013	12	_	_	
23-69	4013-4014	|	_	_	
23-70	4014-4018	607K	_	_	
23-71	4018-4019	|	_	_	
23-72	4019-4023	97.7	_	_	
23-73	4023-4024	|	_	_	
23-74	4024-4025	[	_	_	
23-75	4025-4029	here	_	_	
23-76	4029-4030	]	_	_	
23-77	4030-4031	(	_	_	
23-78	4031-4045	models_data_v2	_	_	
23-79	4045-4046	_	_	_	
23-80	4046-4048	12	_	_	
23-81	4048-4049	_	_	_	
23-82	4049-4055	labels	_	_	
23-83	4055-4056	/	_	_	
23-84	4056-4060	kwt1	_	_	
23-85	4060-4061	)	_	_	
23-86	4061-4062	|	_	_	
23-87	4063-4064	|	_	_	
23-88	4064-4067	KWT	_	_	
23-89	4067-4068	-	_	_	
23-90	4068-4069	2	_	_	
23-91	4069-4070	|	_	_	
23-92	4070-4073	128	_	_	
23-93	4073-4074	|	_	_	
23-94	4074-4077	256	_	_	
23-95	4077-4078	|	_	_	
23-96	4078-4079	2	_	_	
23-97	4079-4080	|	_	_	
23-98	4080-4082	12	_	_	
23-99	4082-4083	|	_	_	
23-100	4083-4087	2.4M	_	_	
23-101	4087-4088	|	_	_	
23-102	4088-4092	98.2	_	_	
23-103	4092-4093	|	_	_	
23-104	4093-4094	[	_	_	
23-105	4094-4098	here	_	_	
23-106	4098-4099	]	_	_	
23-107	4099-4100	(	_	_	
23-108	4100-4114	models_data_v2	_	_	
23-109	4114-4115	_	_	_	
23-110	4115-4117	12	_	_	
23-111	4117-4118	_	_	_	
23-112	4118-4124	labels	_	_	
23-113	4124-4125	/	_	_	
23-114	4125-4129	kwt2	_	_	
23-115	4129-4130	)	_	_	
23-116	4130-4131	|	_	_	
23-117	4132-4133	|	_	_	
23-118	4133-4136	KWT	_	_	
23-119	4136-4137	-	_	_	
23-120	4137-4138	3	_	_	
23-121	4138-4139	|	_	_	
23-122	4139-4142	192	_	_	
23-123	4142-4143	|	_	_	
23-124	4143-4146	768	_	_	
23-125	4146-4147	|	_	_	
23-126	4147-4148	3	_	_	
23-127	4148-4149	|	_	_	
23-128	4149-4151	12	_	_	
23-129	4151-4152	|	_	_	
23-130	4152-4156	5.5M	_	_	
23-131	4156-4157	|	_	_	
23-132	4157-4161	98.7	_	_	
23-133	4161-4162	|	_	_	
23-134	4162-4163	[	_	_	
23-135	4163-4167	here	_	_	
23-136	4167-4168	]	_	_	
23-137	4168-4169	(	_	_	
23-138	4169-4183	models_data_v2	_	_	
23-139	4183-4184	_	_	_	
23-140	4184-4186	12	_	_	
23-141	4186-4187	_	_	_	
23-142	4187-4193	labels	_	_	
23-143	4193-4194	/	_	_	
23-144	4194-4198	kwt3	_	_	
23-145	4198-4199	)	_	_	
23-146	4199-4200	|	_	_	
23-147	4202-4204	To	_	_	
23-148	4205-4212	perform	_	_	
23-149	4213-4222	inference	_	_	
23-150	4223-4225	on	_	_	
23-151	4226-4232	Google	*[11]	DATASET[11]	
23-152	4233-4239	Speech	*[11]	DATASET[11]	
23-153	4240-4248	Commands	*[11]	DATASET[11]	
23-154	4249-4251	v2	*[11]	DATASET[11]	
23-155	4252-4256	with	_	_	
23-156	4257-4259	12	_	_	
23-157	4260-4266	labels	_	_	
23-158	4266-4267	,	_	_	
23-159	4268-4271	run	_	_	
23-160	4273-4274	`	_	_	
23-161	4274-4275	`	_	_	
23-162	4275-4276	`	_	_	
23-163	4276-4281	shell	_	_	
23-164	4282-4284	sh	_	_	
23-165	4285-4292	eval.sh	_	_	
23-166	4293-4294	`	_	_	
23-167	4294-4295	`	_	_	
23-168	4295-4296	`	_	_	
23-169	4298-4299	#	_	_	
23-170	4299-4300	#	_	_	
23-171	4301-4317	Acknowledgements	_	_	
23-172	4319-4322	The	_	_	
23-173	4323-4327	code	_	_	
23-174	4328-4335	heavily	_	_	
23-175	4336-4343	borrows	_	_	
23-176	4344-4348	from	_	_	
23-177	4349-4352	the	_	_	
23-178	4353-4354	[	_	_	
23-179	4354-4357	KWS	_	_	
23-180	4358-4367	streaming	_	_	
23-181	4368-4372	work	_	_	
23-182	4372-4373	]	_	_	
23-183	4373-4374	(	_	_	
23-184	4374-4379	https	_	_	
23-185	4379-4380	:	_	_	
23-186	4380-4381	/	_	_	
23-187	4381-4382	/	_	_	
23-188	4382-4392	github.com	_	_	
23-189	4392-4393	/	_	_	
23-190	4393-4408	google-research	_	_	
23-191	4408-4409	/	_	_	
23-192	4409-4424	google-research	_	_	
23-193	4424-4425	/	_	_	
23-194	4425-4429	tree	_	_	
23-195	4429-4430	/	_	_	
23-196	4430-4436	master	_	_	
23-197	4436-4437	/	_	_	
23-198	4437-4450	kws_streaming	_	_	
23-199	4450-4451	)	_	_	
23-200	4452-4454	by	_	_	
23-201	4455-4461	Google	_	_	
23-202	4462-4470	Research	_	_	
23-203	4470-4471	.	_	_	

#Text=For a more detailed description of the code structure, see the original authors' [README](kws_streaming/README.md).
24-1	4472-4475	For	_	_	
24-2	4476-4477	a	_	_	
24-3	4478-4482	more	_	_	
24-4	4483-4491	detailed	_	_	
24-5	4492-4503	description	_	_	
24-6	4504-4506	of	_	_	
24-7	4507-4510	the	_	_	
24-8	4511-4515	code	_	_	
24-9	4516-4525	structure	_	_	
24-10	4525-4526	,	_	_	
24-11	4527-4530	see	_	_	
24-12	4531-4534	the	_	_	
24-13	4535-4543	original	_	_	
24-14	4544-4551	authors	_	_	
24-15	4551-4552	'	_	_	
24-16	4553-4554	[	_	_	
24-17	4554-4560	README	_	_	
24-18	4560-4561	]	_	_	
24-19	4561-4562	(	_	_	
24-20	4562-4575	kws_streaming	_	_	
24-21	4575-4576	/	_	_	
24-22	4576-4585	README.md	_	_	
24-23	4585-4586	)	_	_	
24-24	4586-4587	.	_	_	

#Text=We also exploit training techniques from [DeiT](https://github.com/facebookresearch/deit).
25-1	4589-4591	We	_	_	
25-2	4592-4596	also	_	_	
25-3	4597-4604	exploit	_	_	
25-4	4605-4613	training	_	_	
25-5	4614-4624	techniques	_	_	
25-6	4625-4629	from	_	_	
25-7	4630-4631	[	_	_	
25-8	4631-4635	DeiT	*	PROJECT	
25-9	4635-4636	]	_	_	
25-10	4636-4637	(	_	_	
25-11	4637-4642	https	_	_	
25-12	4642-4643	:	_	_	
25-13	4643-4644	/	_	_	
25-14	4644-4645	/	_	_	
25-15	4645-4655	github.com	_	_	
25-16	4655-4656	/	_	_	
25-17	4656-4672	facebookresearch	_	_	
25-18	4672-4673	/	_	_	
25-19	4673-4677	deit	*	PROJECT	
25-20	4677-4678	)	_	_	
25-21	4678-4679	.	_	_	

#Text=We thank the authors for sharing their code.
26-1	4681-4683	We	_	_	
26-2	4684-4689	thank	_	_	
26-3	4690-4693	the	_	_	
26-4	4694-4701	authors	_	_	
26-5	4702-4705	for	_	_	
26-6	4706-4713	sharing	_	_	
26-7	4714-4719	their	_	_	
26-8	4720-4724	code	_	_	
26-9	4724-4725	.	_	_	

#Text=Please consider citing them as well if you use our code.
#Text=
#Text=## License
#Text=
#Text=The source files in this repository are released under the [Apache 2.0](LICENSE.txt) license.
27-1	4726-4732	Please	_	_	
27-2	4733-4741	consider	_	_	
27-3	4742-4748	citing	_	_	
27-4	4749-4753	them	_	_	
27-5	4754-4756	as	_	_	
27-6	4757-4761	well	_	_	
27-7	4762-4764	if	_	_	
27-8	4765-4768	you	_	_	
27-9	4769-4772	use	_	_	
27-10	4773-4776	our	_	_	
27-11	4777-4781	code	_	_	
27-12	4781-4782	.	_	_	
27-13	4784-4785	#	_	_	
27-14	4785-4786	#	_	_	
27-15	4787-4794	License	_	_	
27-16	4796-4799	The	_	_	
27-17	4800-4806	source	_	_	
27-18	4807-4812	files	_	_	
27-19	4813-4815	in	_	_	
27-20	4816-4820	this	_	_	
27-21	4821-4831	repository	_	_	
27-22	4832-4835	are	_	_	
27-23	4836-4844	released	_	_	
27-24	4845-4850	under	_	_	
27-25	4851-4854	the	_	_	
27-26	4855-4856	[	_	_	
27-27	4856-4862	Apache	*[12]	LICENSE[12]	
27-28	4863-4866	2.0	*[12]	LICENSE[12]	
27-29	4866-4867	]	_	_	
27-30	4867-4868	(	_	_	
27-31	4868-4879	LICENSE.txt	_	_	
27-32	4879-4880	)	_	_	
27-33	4881-4888	license	_	_	
27-34	4888-4889	.	_	_	

#Text=Some source files are derived from the [KWS streaming repository](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.
28-1	4891-4895	Some	_	_	
28-2	4896-4902	source	_	_	
28-3	4903-4908	files	_	_	
28-4	4909-4912	are	_	_	
28-5	4913-4920	derived	_	_	
28-6	4921-4925	from	_	_	
28-7	4926-4929	the	_	_	
28-8	4930-4931	[	_	_	
28-9	4931-4934	KWS	*[13]	SOFTWARE[13]	
28-10	4935-4944	streaming	*[13]	SOFTWARE[13]	
28-11	4945-4955	repository	*[13]	SOFTWARE[13]	
28-12	4955-4956	]	_	_	
28-13	4956-4957	(	_	_	
28-14	4957-4962	https	_	_	
28-15	4962-4963	:	_	_	
28-16	4963-4964	/	_	_	
28-17	4964-4965	/	_	_	
28-18	4965-4975	github.com	_	_	
28-19	4975-4976	/	_	_	
28-20	4976-4991	google-research	_	_	
28-21	4991-4992	/	_	_	
28-22	4992-5007	google-research	_	_	
28-23	5007-5008	/	_	_	
28-24	5008-5012	tree	_	_	
28-25	5012-5013	/	_	_	
28-26	5013-5019	master	_	_	
28-27	5019-5020	/	_	_	
28-28	5020-5033	kws_streaming	*	SOFTWARE	
28-29	5033-5034	)	_	_	
28-30	5035-5037	by	_	_	
28-31	5038-5044	Google	_	_	
28-32	5045-5053	Research	_	_	
28-33	5053-5054	.	_	_	

#Text=These are also released under the Apache 2.0 license, the text of which can be seen in the LICENSE file on their repository.
29-1	5055-5060	These	_	_	
29-2	5061-5064	are	_	_	
29-3	5065-5069	also	_	_	
29-4	5070-5078	released	_	_	
29-5	5079-5084	under	_	_	
29-6	5085-5088	the	_	_	
29-7	5089-5095	Apache	*[14]	LICENSE[14]	
29-8	5096-5099	2.0	*[14]	LICENSE[14]	
29-9	5100-5107	license	_	_	
29-10	5107-5108	,	_	_	
29-11	5109-5112	the	_	_	
29-12	5113-5117	text	_	_	
29-13	5118-5120	of	_	_	
29-14	5121-5126	which	_	_	
29-15	5127-5130	can	_	_	
29-16	5131-5133	be	_	_	
29-17	5134-5138	seen	_	_	
29-18	5139-5141	in	_	_	
29-19	5142-5145	the	_	_	
29-20	5146-5153	LICENSE	_	_	
29-21	5154-5158	file	_	_	
29-22	5159-5161	on	_	_	
29-23	5162-5167	their	_	_	
29-24	5168-5178	repository	_	_	
29-25	5178-5179	.	_	_	
