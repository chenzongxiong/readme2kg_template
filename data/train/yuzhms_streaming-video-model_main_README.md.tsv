#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Streaming Video Model
#Text=
#Text=> **Streaming Video Model** <br>
#Text=> Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, Zheng-Jun Zha <br>
#Text=> *CVPR 2023* <br>
#Text=
#Text=[[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Streaming_Video_Model_CVPR_2023_paper.html)] 
#Text=[[arXiv](https://arxiv.org/abs/2303.17228)] 
#Text=
#Text=## Description   
#Text=
#Text=**Streaming video model** is a general video model, which is applicable to general video understanding tasks.
1-1	0-1	#	_	_	
1-2	2-11	Streaming	*[1]	PUBLICATION[1]	
1-3	12-17	Video	*[1]	PUBLICATION[1]	
1-4	18-23	Model	*[1]	PUBLICATION[1]	
1-5	25-26	>	_	_	
1-6	27-28	*	_	_	
1-7	28-29	*	_	_	
1-8	29-38	Streaming	*[2]	PUBLICATION[2]	
1-9	39-44	Video	*[2]	PUBLICATION[2]	
1-10	45-50	Model	*[2]	PUBLICATION[2]	
1-11	50-51	*	_	_	
1-12	51-52	*	_	_	
1-13	53-54	<	_	_	
1-14	54-56	br	_	_	
1-15	56-57	>	_	_	
1-16	58-59	>	_	_	
1-17	60-67	Yucheng	_	_	
1-18	68-72	Zhao	_	_	
1-19	72-73	,	_	_	
1-20	74-79	Chong	_	_	
1-21	80-83	Luo	_	_	
1-22	83-84	,	_	_	
1-23	85-93	Chuanxin	_	_	
1-24	94-98	Tang	_	_	
1-25	98-99	,	_	_	
1-26	100-108	Dongdong	_	_	
1-27	109-113	Chen	_	_	
1-28	113-114	,	_	_	
1-29	115-119	Noel	_	_	
1-30	120-127	Codella	_	_	
1-31	127-128	,	_	_	
1-32	129-138	Zheng-Jun	_	_	
1-33	139-142	Zha	_	_	
1-34	143-144	<	_	_	
1-35	144-146	br	_	_	
1-36	146-147	>	_	_	
1-37	148-149	>	_	_	
1-38	150-151	*	_	_	
1-39	151-155	CVPR	*[3]	CONFERENCE[3]	
1-40	156-160	2023	*[3]	CONFERENCE[3]	
1-41	160-161	*	_	_	
1-42	162-163	<	_	_	
1-43	163-165	br	_	_	
1-44	165-166	>	_	_	
1-45	168-169	[	_	_	
1-46	169-170	[	_	_	
1-47	170-175	Paper	_	_	
1-48	175-176	]	_	_	
1-49	176-177	(	_	_	
1-50	177-182	https	_	_	
1-51	182-183	:	_	_	
1-52	183-184	/	_	_	
1-53	184-185	/	_	_	
1-54	185-206	openaccess.thecvf.com	_	_	
1-55	206-207	/	_	_	
1-56	207-214	content	_	_	
1-57	214-215	/	_	_	
1-58	215-223	CVPR2023	*	CONFERENCE	
1-59	223-224	/	_	_	
1-60	224-228	html	_	_	
1-61	228-229	/	_	_	
1-62	229-260	Zhao_Streaming_Video_Model_CVPR	_	_	
1-62.1	256-260	CVPR	*[4]	CONFERENCE[4]	
1-63	260-261	_	*[4]	CONFERENCE[4]	
1-64	261-265	2023	*[4]	CONFERENCE[4]	
1-65	265-266	_	_	_	
1-66	266-276	paper.html	_	_	
1-67	276-277	)	_	_	
1-68	277-278	]	_	_	
1-69	280-281	[	_	_	
1-70	281-282	[	_	_	
1-71	282-287	arXiv	_	_	
1-72	287-288	]	_	_	
1-73	288-289	(	_	_	
1-74	289-294	https	_	_	
1-75	294-295	:	_	_	
1-76	295-296	/	_	_	
1-77	296-297	/	_	_	
1-78	297-306	arxiv.org	_	_	
1-79	306-307	/	_	_	
1-80	307-310	abs	_	_	
1-81	310-311	/	_	_	
1-82	311-321	2303.17228	_	_	
1-83	321-322	)	_	_	
1-84	322-323	]	_	_	
1-85	326-327	#	_	_	
1-86	327-328	#	_	_	
1-87	329-340	Description	_	_	
1-88	345-346	*	_	_	
1-89	346-347	*	_	_	
1-90	347-356	Streaming	*[5]	SOFTWARE[5]	
1-91	357-362	video	*[5]	SOFTWARE[5]	
1-92	363-368	model	*[5]	SOFTWARE[5]	
1-93	368-369	*	_	_	
1-94	369-370	*	_	_	
1-95	371-373	is	_	_	
1-96	374-375	a	_	_	
1-97	376-383	general	_	_	
1-98	384-389	video	_	_	
1-99	390-395	model	_	_	
1-100	395-396	,	_	_	
1-101	397-402	which	_	_	
1-102	403-405	is	_	_	
1-103	406-416	applicable	_	_	
1-104	417-419	to	_	_	
1-105	420-427	general	_	_	
1-106	428-433	video	_	_	
1-107	434-447	understanding	_	_	
1-108	448-453	tasks	_	_	
1-109	453-454	.	_	_	

#Text=Traditionally, video understanding tasks have been modeled by two separate architectures, specially tailored for two distinct tasks.
2-1	455-468	Traditionally	_	_	
2-2	468-469	,	_	_	
2-3	470-475	video	_	_	
2-4	476-489	understanding	_	_	
2-5	490-495	tasks	_	_	
2-6	496-500	have	_	_	
2-7	501-505	been	_	_	
2-8	506-513	modeled	_	_	
2-9	514-516	by	_	_	
2-10	517-520	two	_	_	
2-11	521-529	separate	_	_	
2-12	530-543	architectures	_	_	
2-13	543-544	,	_	_	
2-14	545-554	specially	_	_	
2-15	555-563	tailored	_	_	
2-16	564-567	for	_	_	
2-17	568-571	two	_	_	
2-18	572-580	distinct	_	_	
2-19	581-586	tasks	_	_	
2-20	586-587	.	_	_	

#Text=Streaming video model is the first deep learning architecture that unifies video understanding tasks.
3-1	588-597	Streaming	_	_	
3-2	598-603	video	_	_	
3-3	604-609	model	_	_	
3-4	610-612	is	_	_	
3-5	613-616	the	_	_	
3-6	617-622	first	_	_	
3-7	623-627	deep	_	_	
3-8	628-636	learning	_	_	
3-9	637-649	architecture	_	_	
3-10	650-654	that	_	_	
3-11	655-662	unifies	_	_	
3-12	663-668	video	_	_	
3-13	669-682	understanding	_	_	
3-14	683-688	tasks	_	_	
3-15	688-689	.	_	_	

#Text=We build an instance of streaming video model, namely the streaming video Transformer (S-ViT).S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks.
4-1	690-692	We	_	_	
4-2	693-698	build	_	_	
4-3	699-701	an	_	_	
4-4	702-710	instance	_	_	
4-5	711-713	of	_	_	
4-6	714-723	streaming	_	_	
4-7	724-729	video	_	_	
4-8	730-735	model	_	_	
4-9	735-736	,	_	_	
4-10	737-743	namely	_	_	
4-11	744-747	the	_	_	
4-12	748-757	streaming	*[6]	SOFTWARE[6]	
4-13	758-763	video	*[6]	SOFTWARE[6]	
4-14	764-775	Transformer	*[6]	SOFTWARE[6]	
4-15	776-777	(	_	_	
4-16	777-782	S-ViT	*	SOFTWARE	
4-17	782-783	)	_	_	
4-18	783-784	.	_	_	
4-19	784-789	S-ViT	*	SOFTWARE	
4-20	790-795	first	_	_	
4-21	796-804	produces	_	_	
4-22	805-816	frame-level	_	_	
4-23	817-825	features	_	_	
4-24	826-830	with	_	_	
4-25	831-832	a	_	_	
4-26	833-847	memory-enabled	_	_	
4-27	848-864	temporally-aware	_	_	
4-28	865-872	spatial	_	_	
4-29	873-880	encoder	_	_	
4-30	881-883	to	_	_	
4-31	884-889	serve	_	_	
4-32	890-893	the	_	_	
4-33	894-905	frame-based	_	_	
4-34	906-911	video	_	_	
4-35	912-917	tasks	_	_	
4-36	917-918	.	_	_	

#Text=Then the
#Text=frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks.
5-1	919-923	Then	_	_	
5-2	924-927	the	_	_	
5-3	928-933	frame	_	_	
5-4	934-942	features	_	_	
5-5	943-946	are	_	_	
5-6	947-952	input	_	_	
5-7	953-957	into	_	_	
5-8	958-959	a	_	_	
5-9	960-972	task-related	_	_	
5-10	973-981	temporal	_	_	
5-11	982-989	decoder	_	_	
5-12	990-992	to	_	_	
5-13	993-999	obtain	_	_	
5-14	1000-1014	spatiotemporal	_	_	
5-15	1015-1023	features	_	_	
5-16	1024-1027	for	_	_	
5-17	1028-1042	sequence-based	_	_	
5-18	1043-1048	tasks	_	_	
5-19	1048-1049	.	_	_	

#Text=The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. 
#Text=
#Text=## Usage
#Text=### Installation
#Text=
#Text=Clone the repo and install requirements:
#Text=
#Text=```shell
#Text=conda create -n svm python=3.7 -y
#Text=conda activate svm
#Text=conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch
#Text=pip install git+https://github.com/JonathonLuiten/TrackEval.git
#Text=pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html
#Text=pip install mmdet==2.26.0
#Text=pip install -r requirements/build.txt
#Text=pip install --user -v -e .
#Text=pip install einops
#Text=pip install future tensorboard
#Text=pip install -U fvcore
#Text=pip install click imageio[ffmpeg] path
#Text=```
#Text=
#Text=### Dataset preparation
#Text=
#Text=Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.
6-1	1050-1053	The	_	_	
6-2	1054-1064	efficiency	_	_	
6-3	1065-1068	and	_	_	
6-4	1069-1077	efficacy	_	_	
6-5	1078-1080	of	_	_	
6-6	1081-1086	S-ViT	_	_	
6-7	1087-1089	is	_	_	
6-8	1090-1102	demonstrated	_	_	
6-9	1103-1105	by	_	_	
6-10	1106-1109	the	_	_	
6-11	1110-1126	state-of-the-art	_	_	
6-12	1127-1135	accuracy	_	_	
6-13	1136-1138	in	_	_	
6-14	1139-1142	the	_	_	
6-15	1143-1157	sequence-based	_	_	
6-16	1158-1164	action	_	_	
6-17	1165-1176	recognition	_	_	
6-18	1177-1181	task	_	_	
6-19	1182-1185	and	_	_	
6-20	1186-1189	the	_	_	
6-21	1190-1201	competitive	_	_	
6-22	1202-1211	advantage	_	_	
6-23	1212-1216	over	_	_	
6-24	1217-1229	conventional	_	_	
6-25	1230-1242	architecture	_	_	
6-26	1243-1245	in	_	_	
6-27	1246-1249	the	_	_	
6-28	1250-1261	frame-based	_	_	
6-29	1262-1265	MOT	_	_	
6-30	1266-1270	task	_	_	
6-31	1270-1271	.	_	_	
6-32	1274-1275	#	_	_	
6-33	1275-1276	#	_	_	
6-34	1277-1282	Usage	_	_	
6-35	1283-1284	#	_	_	
6-36	1284-1285	#	_	_	
6-37	1285-1286	#	_	_	
6-38	1287-1299	Installation	_	_	
6-39	1301-1306	Clone	_	_	
6-40	1307-1310	the	_	_	
6-41	1311-1315	repo	_	_	
6-42	1316-1319	and	_	_	
6-43	1320-1327	install	_	_	
6-44	1328-1340	requirements	_	_	
6-45	1340-1341	:	_	_	
6-46	1343-1344	`	_	_	
6-47	1344-1345	`	_	_	
6-48	1345-1346	`	_	_	
6-49	1346-1351	shell	*	PROGLANG	
6-50	1352-1357	conda	*	SOFTWARE	
6-51	1358-1364	create	_	_	
6-52	1365-1366	-	_	_	
6-53	1366-1367	n	_	_	
6-54	1368-1371	svm	_	_	
6-55	1372-1378	python	*[7]	PROGLANG[7]	
6-56	1378-1379	=	*[7]	PROGLANG[7]	
6-57	1379-1382	3.7	*[7]	PROGLANG[7]	
6-58	1383-1384	-	_	_	
6-59	1384-1385	y	_	_	
6-60	1386-1391	conda	*	SOFTWARE	
6-61	1392-1400	activate	_	_	
6-62	1401-1404	svm	_	_	
6-63	1405-1410	conda	*	SOFTWARE	
6-64	1411-1418	install	_	_	
6-65	1419-1426	pytorch	*[8]	SOFTWARE[8]	
6-66	1426-1427	=	*[8]	SOFTWARE[8]	
6-67	1427-1428	=	*[8]	SOFTWARE[8]	
6-68	1428-1434	1.12.0	*[8]	SOFTWARE[8]	
6-69	1435-1446	torchvision	*[9]	SOFTWARE[9]	
6-70	1446-1447	=	*[9]	SOFTWARE[9]	
6-71	1447-1448	=	*[9]	SOFTWARE[9]	
6-72	1448-1454	0.13.0	*[9]	SOFTWARE[9]	
6-73	1455-1466	cudatoolkit	*[10]	SOFTWARE[10]	
6-74	1466-1467	=	*[10]	SOFTWARE[10]	
6-75	1467-1471	11.3	*[10]	SOFTWARE[10]	
6-76	1472-1473	-	_	_	
6-77	1473-1474	c	_	_	
6-78	1475-1482	pytorch	*	SOFTWARE	
6-79	1483-1486	pip	*	SOFTWARE	
6-80	1487-1494	install	_	_	
6-81	1495-1498	git	*	SOFTWARE	
6-82	1498-1499	+	_	_	
6-83	1499-1504	https	_	_	
6-84	1504-1505	:	_	_	
6-85	1505-1506	/	_	_	
6-86	1506-1507	/	_	_	
6-87	1507-1517	github.com	_	_	
6-88	1517-1518	/	_	_	
6-89	1518-1532	JonathonLuiten	_	_	
6-90	1532-1533	/	_	_	
6-91	1533-1546	TrackEval.git	_	_	
6-92	1547-1550	pip	*	SOFTWARE	
6-93	1551-1558	install	_	_	
6-94	1559-1568	mmcv-full	*[11]	SOFTWARE[11]	
6-95	1568-1569	=	*[11]	SOFTWARE[11]	
6-96	1569-1570	=	*[11]	SOFTWARE[11]	
6-97	1570-1575	1.7.0	*[11]	SOFTWARE[11]	
6-98	1576-1577	-	_	_	
6-99	1577-1578	f	_	_	
6-100	1579-1584	https	_	_	
6-101	1584-1585	:	_	_	
6-102	1585-1586	/	_	_	
6-103	1586-1587	/	_	_	
6-104	1587-1609	download.openmmlab.com	_	_	
6-105	1609-1610	/	_	_	
6-106	1610-1614	mmcv	_	_	
6-107	1614-1615	/	_	_	
6-108	1615-1619	dist	_	_	
6-109	1619-1620	/	_	_	
6-110	1620-1625	cu113	_	_	
6-111	1625-1626	/	_	_	
6-112	1626-1635	torch1.12	_	_	
6-113	1635-1636	/	_	_	
6-114	1636-1646	index.html	_	_	
6-115	1647-1650	pip	*	SOFTWARE	
6-116	1651-1658	install	_	_	
6-117	1659-1664	mmdet	*[12]	SOFTWARE[12]	
6-118	1664-1665	=	*[12]	SOFTWARE[12]	
6-119	1665-1666	=	*[12]	SOFTWARE[12]	
6-120	1666-1672	2.26.0	*[12]	SOFTWARE[12]	
6-121	1673-1676	pip	*	SOFTWARE	
6-122	1677-1684	install	_	_	
6-123	1685-1686	-	_	_	
6-124	1686-1687	r	_	_	
6-125	1688-1700	requirements	_	_	
6-126	1700-1701	/	_	_	
6-127	1701-1710	build.txt	_	_	
6-128	1711-1714	pip	*	SOFTWARE	
6-129	1715-1722	install	_	_	
6-130	1723-1724	-	_	_	
6-131	1724-1725	-	_	_	
6-132	1725-1729	user	_	_	
6-133	1730-1731	-	_	_	
6-134	1731-1732	v	_	_	
6-135	1733-1734	-	_	_	
6-136	1734-1735	e	_	_	
6-137	1736-1737	.	_	_	
6-138	1738-1741	pip	*	SOFTWARE	
6-139	1742-1749	install	_	_	
6-140	1750-1756	einops	*	SOFTWARE	
6-141	1757-1760	pip	*	SOFTWARE	
6-142	1761-1768	install	_	_	
6-143	1769-1775	future	*	SOFTWARE	
6-144	1776-1787	tensorboard	*	SOFTWARE	
6-145	1788-1791	pip	*	SOFTWARE	
6-146	1792-1799	install	_	_	
6-147	1800-1801	-	_	_	
6-148	1801-1802	U	_	_	
6-149	1803-1809	fvcore	*	SOFTWARE	
6-150	1810-1813	pip	*	SOFTWARE	
6-151	1814-1821	install	_	_	
6-152	1822-1827	click	*	SOFTWARE	
6-153	1828-1835	imageio	*[13]	SOFTWARE[13]	
6-154	1835-1836	[	*[13]	SOFTWARE[13]	
6-155	1836-1842	ffmpeg	*[13]	SOFTWARE[13]	
6-156	1842-1843	]	*[13]	SOFTWARE[13]	
6-157	1844-1848	path	_	_	
6-158	1849-1850	`	_	_	
6-159	1850-1851	`	_	_	
6-160	1851-1852	`	_	_	
6-161	1854-1855	#	_	_	
6-162	1855-1856	#	_	_	
6-163	1856-1857	#	_	_	
6-164	1858-1865	Dataset	_	_	
6-165	1866-1877	preparation	_	_	
6-166	1879-1887	Download	_	_	
6-167	1888-1889	[	_	_	
6-168	1889-1894	MOT17	*	DATASET	
6-169	1894-1895	]	_	_	
6-170	1895-1896	(	_	_	
6-171	1896-1901	https	_	_	
6-172	1901-1902	:	_	_	
6-173	1902-1903	/	_	_	
6-174	1903-1904	/	_	_	
6-175	1904-1920	motchallenge.net	_	_	
6-176	1920-1921	/	_	_	
6-177	1921-1925	data	_	_	
6-178	1925-1926	/	_	_	
6-179	1926-1931	MOT17	_	_	
6-180	1931-1932	/	_	_	
6-181	1932-1933	)	_	_	
6-182	1933-1934	,	_	_	
6-183	1935-1936	[	_	_	
6-184	1936-1946	crowdhuman	*	DATASET	
6-185	1946-1947	]	_	_	
6-186	1947-1948	(	_	_	
6-187	1948-1953	https	_	_	
6-188	1953-1954	:	_	_	
6-189	1954-1955	/	_	_	
6-190	1955-1956	/	_	_	
6-191	1956-1974	www.crowdhuman.org	_	_	
6-192	1974-1975	/	_	_	
6-193	1975-1976	)	_	_	
6-194	1976-1977	,	_	_	
6-195	1978-1981	and	_	_	
6-196	1982-1983	[	_	_	
6-197	1983-1991	MOTSynth	*	DATASET	
6-198	1991-1992	]	_	_	
6-199	1992-1993	(	_	_	
6-200	1993-1998	https	_	_	
6-201	1998-1999	:	_	_	
6-202	1999-2000	/	_	_	
6-203	2000-2001	/	_	_	
6-204	2001-2017	motchallenge.net	_	_	
6-205	2017-2018	/	_	_	
6-206	2018-2019	)	_	_	
6-207	2020-2028	datasets	_	_	
6-208	2029-2032	and	_	_	
6-209	2033-2036	put	_	_	
6-210	2037-2041	them	_	_	
6-211	2042-2047	under	_	_	
6-212	2048-2051	the	_	_	
6-213	2052-2056	data	_	_	
6-214	2057-2066	directory	_	_	
6-215	2066-2067	.	_	_	

#Text=The data directory is structured as follows:
#Text=
#Text=```
#Text=data
#Text=|-- crowdhuman
#Text=│   ├── annotation_train.odgt
#Text=│   ├── annotation_val.odgt
#Text=│   ├── train
#Text=│   │   ├── Images
#Text=│   │   ├── CrowdHuman_train01.zip
#Text=│   │   ├── CrowdHuman_train02.zip
#Text=│   │   ├── CrowdHuman_train03.zip
#Text=│   ├── val
#Text=│   │   ├── Images
#Text=│   │   ├── CrowdHuman_val.zip
#Text=|-- MOT17
#Text=│   ├── train
#Text=│   ├── test
#Text=|-- MOTSynth
#Text=|   ├── videos
#Text=│   ├── annotations
#Text=```
#Text=
#Text=Then, we need to convert the all dataset to COCO format.
7-1	2068-2071	The	_	_	
7-2	2072-2076	data	_	_	
7-3	2077-2086	directory	_	_	
7-4	2087-2089	is	_	_	
7-5	2090-2100	structured	_	_	
7-6	2101-2103	as	_	_	
7-7	2104-2111	follows	_	_	
7-8	2111-2112	:	_	_	
7-9	2114-2115	`	_	_	
7-10	2115-2116	`	_	_	
7-11	2116-2117	`	_	_	
7-12	2118-2122	data	_	_	
7-13	2123-2124	|	_	_	
7-14	2124-2125	-	_	_	
7-15	2125-2126	-	_	_	
7-16	2127-2137	crowdhuman	*	DATASET	
7-17	2138-2139	│	_	_	
7-18	2142-2143	├	_	_	
7-19	2143-2144	─	_	_	
7-20	2144-2145	─	_	_	
7-21	2146-2167	annotation_train.odgt	_	_	
7-22	2168-2169	│	_	_	
7-23	2172-2173	├	_	_	
7-24	2173-2174	─	_	_	
7-25	2174-2175	─	_	_	
7-26	2176-2195	annotation_val.odgt	_	_	
7-27	2196-2197	│	_	_	
7-28	2200-2201	├	_	_	
7-29	2201-2202	─	_	_	
7-30	2202-2203	─	_	_	
7-31	2204-2209	train	_	_	
7-32	2210-2211	│	_	_	
7-33	2214-2215	│	_	_	
7-34	2218-2219	├	_	_	
7-35	2219-2220	─	_	_	
7-36	2220-2221	─	_	_	
7-37	2222-2228	Images	_	_	
7-38	2229-2230	│	_	_	
7-39	2233-2234	│	_	_	
7-40	2237-2238	├	_	_	
7-41	2238-2239	─	_	_	
7-42	2239-2240	─	_	_	
7-43	2241-2259	CrowdHuman_train01	_	_	
7-44	2259-2260	.	_	_	
7-45	2260-2263	zip	_	_	
7-46	2264-2265	│	_	_	
7-47	2268-2269	│	_	_	
7-48	2272-2273	├	_	_	
7-49	2273-2274	─	_	_	
7-50	2274-2275	─	_	_	
7-51	2276-2294	CrowdHuman_train02	_	_	
7-52	2294-2295	.	_	_	
7-53	2295-2298	zip	_	_	
7-54	2299-2300	│	_	_	
7-55	2303-2304	│	_	_	
7-56	2307-2308	├	_	_	
7-57	2308-2309	─	_	_	
7-58	2309-2310	─	_	_	
7-59	2311-2329	CrowdHuman_train03	_	_	
7-60	2329-2330	.	_	_	
7-61	2330-2333	zip	_	_	
7-62	2334-2335	│	_	_	
7-63	2338-2339	├	_	_	
7-64	2339-2340	─	_	_	
7-65	2340-2341	─	_	_	
7-66	2342-2345	val	_	_	
7-67	2346-2347	│	_	_	
7-68	2350-2351	│	_	_	
7-69	2354-2355	├	_	_	
7-70	2355-2356	─	_	_	
7-71	2356-2357	─	_	_	
7-72	2358-2364	Images	_	_	
7-73	2365-2366	│	_	_	
7-74	2369-2370	│	_	_	
7-75	2373-2374	├	_	_	
7-76	2374-2375	─	_	_	
7-77	2375-2376	─	_	_	
7-78	2377-2395	CrowdHuman_val.zip	_	_	
7-79	2396-2397	|	_	_	
7-80	2397-2398	-	_	_	
7-81	2398-2399	-	_	_	
7-82	2400-2405	MOT17	*	DATASET	
7-83	2406-2407	│	_	_	
7-84	2410-2411	├	_	_	
7-85	2411-2412	─	_	_	
7-86	2412-2413	─	_	_	
7-87	2414-2419	train	_	_	
7-88	2420-2421	│	_	_	
7-89	2424-2425	├	_	_	
7-90	2425-2426	─	_	_	
7-91	2426-2427	─	_	_	
7-92	2428-2432	test	_	_	
7-93	2433-2434	|	_	_	
7-94	2434-2435	-	_	_	
7-95	2435-2436	-	_	_	
7-96	2437-2445	MOTSynth	*	DATASET	
7-97	2446-2447	|	_	_	
7-98	2450-2451	├	_	_	
7-99	2451-2452	─	_	_	
7-100	2452-2453	─	_	_	
7-101	2454-2460	videos	_	_	
7-102	2461-2462	│	_	_	
7-103	2465-2466	├	_	_	
7-104	2466-2467	─	_	_	
7-105	2467-2468	─	_	_	
7-106	2469-2480	annotations	_	_	
7-107	2481-2482	`	_	_	
7-108	2482-2483	`	_	_	
7-109	2483-2484	`	_	_	
7-110	2486-2490	Then	_	_	
7-111	2490-2491	,	_	_	
7-112	2492-2494	we	_	_	
7-113	2495-2499	need	_	_	
7-114	2500-2502	to	_	_	
7-115	2503-2510	convert	_	_	
7-116	2511-2514	the	_	_	
7-117	2515-2518	all	_	_	
7-118	2519-2526	dataset	_	_	
7-119	2527-2529	to	_	_	
7-120	2530-2534	COCO	*	SOFTWARE	
7-121	2535-2541	format	_	_	
7-122	2541-2542	.	_	_	

#Text=We provide scripts to do this:
#Text=
#Text=```shell
#Text=# crowdhuman
#Text=python .
8-1	2543-2545	We	_	_	
8-2	2546-2553	provide	_	_	
8-3	2554-2561	scripts	_	_	
8-4	2562-2564	to	_	_	
8-5	2565-2567	do	_	_	
8-6	2568-2572	this	_	_	
8-7	2572-2573	:	_	_	
8-8	2575-2576	`	_	_	
8-9	2576-2577	`	_	_	
8-10	2577-2578	`	_	_	
8-11	2578-2583	shell	*	PROGLANG	
8-12	2584-2585	#	_	_	
8-13	2586-2596	crowdhuman	*	DATASET	
8-14	2597-2603	python	*	SOFTWARE	
8-15	2604-2605	.	_	_	

#Text=/tools/convert_datasets/crowdhuman2coco.py -i .
9-1	2605-2606	/	_	_	
9-2	2606-2611	tools	_	_	
9-3	2611-2612	/	_	_	
9-4	2612-2628	convert_datasets	_	_	
9-5	2628-2629	/	_	_	
9-6	2629-2647	crowdhuman2coco.py	_	_	
9-7	2648-2649	-	_	_	
9-8	2649-2650	i	_	_	
9-9	2651-2652	.	_	_	

#Text=/data/crowdhuman -o .
10-1	2652-2653	/	_	_	
10-2	2653-2657	data	_	_	
10-3	2657-2658	/	_	_	
10-4	2658-2668	crowdhuman	_	_	
10-5	2669-2670	-	_	_	
10-6	2670-2671	o	_	_	
10-7	2672-2673	.	_	_	

#Text=/data/crowdhuman/annotations
#Text=
#Text=# MOT17
#Text=python .
11-1	2673-2674	/	_	_	
11-2	2674-2678	data	_	_	
11-3	2678-2679	/	_	_	
11-4	2679-2689	crowdhuman	_	_	
11-5	2689-2690	/	_	_	
11-6	2690-2701	annotations	_	_	
11-7	2703-2704	#	_	_	
11-8	2705-2710	MOT17	*	DATASET	
11-9	2711-2717	python	*	SOFTWARE	
11-10	2718-2719	.	_	_	

#Text=/tools/convert_datasets/mot2coco.py -i .
12-1	2719-2720	/	_	_	
12-2	2720-2725	tools	_	_	
12-3	2725-2726	/	_	_	
12-4	2726-2742	convert_datasets	_	_	
12-5	2742-2743	/	_	_	
12-6	2743-2754	mot2coco.py	_	_	
12-7	2755-2756	-	_	_	
12-8	2756-2757	i	_	_	
12-9	2758-2759	.	_	_	

#Text=/data/MOT17/ -o .
13-1	2759-2760	/	_	_	
13-2	2760-2764	data	_	_	
13-3	2764-2765	/	_	_	
13-4	2765-2770	MOT17	*	DATASET	
13-5	2770-2771	/	_	_	
13-6	2772-2773	-	_	_	
13-7	2773-2774	o	_	_	
13-8	2775-2776	.	_	_	

#Text=/data/MOT17/annotations --split-train --convert-det
#Text=
#Text=# MOTSynth
#Text=python .
14-1	2776-2777	/	_	_	
14-2	2777-2781	data	_	_	
14-3	2781-2782	/	_	_	
14-4	2782-2787	MOT17	*	DATASET	
14-5	2787-2788	/	_	_	
14-6	2788-2799	annotations	_	_	
14-7	2800-2801	-	_	_	
14-8	2801-2802	-	_	_	
14-9	2802-2813	split-train	_	_	
14-10	2814-2815	-	_	_	
14-11	2815-2816	-	_	_	
14-12	2816-2827	convert-det	_	_	
14-13	2829-2830	#	_	_	
14-14	2831-2839	MOTSynth	*	DATASET	
14-15	2840-2846	python	*	SOFTWARE	
14-16	2847-2848	.	_	_	

#Text=/tools/convert_datasets/extract_motsynth.py --input_dir_path .
15-1	2848-2849	/	_	_	
15-2	2849-2854	tools	_	_	
15-3	2854-2855	/	_	_	
15-4	2855-2871	convert_datasets	_	_	
15-5	2871-2872	/	_	_	
15-6	2872-2891	extract_motsynth.py	_	_	
15-7	2892-2893	-	_	_	
15-8	2893-2894	-	_	_	
15-9	2894-2908	input_dir_path	_	_	
15-10	2909-2910	.	_	_	

#Text=/data/MOTSynth/video --out_dir_path .
16-1	2910-2911	/	_	_	
16-2	2911-2915	data	_	_	
16-3	2915-2916	/	_	_	
16-4	2916-2924	MOTSynth	*	DATASET	
16-5	2924-2925	/	_	_	
16-6	2925-2930	video	_	_	
16-7	2931-2932	-	_	_	
16-8	2932-2933	-	_	_	
16-9	2933-2945	out_dir_path	_	_	
16-10	2946-2947	.	_	_	

#Text=/data/MOTSynth/train/
#Text=python .
17-1	2947-2948	/	_	_	
17-2	2948-2952	data	_	_	
17-3	2952-2953	/	_	_	
17-4	2953-2961	MOTSynth	*	DATASET	
17-5	2961-2962	/	_	_	
17-6	2962-2967	train	_	_	
17-7	2967-2968	/	_	_	
17-8	2969-2975	python	*	SOFTWARE	
17-9	2976-2977	.	_	_	

#Text=/tools/convert_datasets/motsynth2coco.py --anns .
18-1	2977-2978	/	_	_	
18-2	2978-2983	tools	_	_	
18-3	2983-2984	/	_	_	
18-4	2984-3000	convert_datasets	_	_	
18-5	3000-3001	/	_	_	
18-6	3001-3017	motsynth2coco.py	_	_	
18-7	3018-3019	-	_	_	
18-8	3019-3020	-	_	_	
18-9	3020-3024	anns	_	_	
18-10	3025-3026	.	_	_	

#Text=/data/MOTSynth/annotations --out .
19-1	3026-3027	/	_	_	
19-2	3027-3031	data	_	_	
19-3	3031-3032	/	_	_	
19-4	3032-3040	MOTSynth	_	_	
19-5	3040-3041	/	_	_	
19-6	3041-3052	annotations	_	_	
19-7	3053-3054	-	_	_	
19-8	3054-3055	-	_	_	
19-9	3055-3058	out	_	_	
19-10	3059-3060	.	_	_	

#Text=/data/MOTSynth/all_cocoformat.json
#Text=```
#Text=
#Text=The processed dataset will be structured as follows:
#Text=
#Text=```
#Text=data
#Text=|-- crowdhuman
#Text=│   ├── annotation_train.odgt
#Text=│   ├── annotation_val.odgt
#Text=│   ├── train
#Text=│   │   ├── Images
#Text=│   │   ├── CrowdHuman_train01.zip
#Text=│   │   ├── CrowdHuman_train02.zip
#Text=│   │   ├── CrowdHuman_train03.zip
#Text=│   ├── val
#Text=│   │   ├── Images
#Text=│   │   ├── CrowdHuman_val.zip
#Text=|   ├── annotations
#Text=│   │   ├── crowdhuman_train.json
#Text=│   │   ├── crowdhuman_val.json
#Text=|-- MOT17
#Text=│   ├── train
#Text=│   │   ├── MOT17-02-DPM
#Text=│   │   ├── ...
#Text=│   ├── test
#Text=│   ├── annotations
#Text=│   │   ├── half-train_cocoformat.json
#Text=│   │   ├── ...
#Text=|-- MOTSynth
#Text=|   ├── videos
#Text=│   ├── annotations
#Text=│   ├── train
#Text=│   │   ├── 000
#Text=│   │   │   ├── img1
#Text=│   │   │   │   ├── 000001.jpg
#Text=│   │   │   │   ├── ...
#Text=│   │   ├── ...
#Text=│   ├── all_cocoformat.json
#Text=```
#Text=### Pretrained models
#Text=
#Text=We use CLIP pretrained ViT models.
20-1	3060-3061	/	_	_	
20-2	3061-3065	data	_	_	
20-3	3065-3066	/	_	_	
20-4	3066-3074	MOTSynth	_	_	
20-5	3074-3075	/	_	_	
20-6	3075-3094	all_cocoformat.json	_	_	
20-7	3095-3096	`	_	_	
20-8	3096-3097	`	_	_	
20-9	3097-3098	`	_	_	
20-10	3100-3103	The	_	_	
20-11	3104-3113	processed	_	_	
20-12	3114-3121	dataset	_	_	
20-13	3122-3126	will	_	_	
20-14	3127-3129	be	_	_	
20-15	3130-3140	structured	_	_	
20-16	3141-3143	as	_	_	
20-17	3144-3151	follows	_	_	
20-18	3151-3152	:	_	_	
20-19	3154-3155	`	_	_	
20-20	3155-3156	`	_	_	
20-21	3156-3157	`	_	_	
20-22	3158-3162	data	_	_	
20-23	3163-3164	|	_	_	
20-24	3164-3165	-	_	_	
20-25	3165-3166	-	_	_	
20-26	3167-3177	crowdhuman	*	DATASET	
20-27	3178-3179	│	_	_	
20-28	3182-3183	├	_	_	
20-29	3183-3184	─	_	_	
20-30	3184-3185	─	_	_	
20-31	3186-3207	annotation_train.odgt	_	_	
20-32	3208-3209	│	_	_	
20-33	3212-3213	├	_	_	
20-34	3213-3214	─	_	_	
20-35	3214-3215	─	_	_	
20-36	3216-3235	annotation_val.odgt	_	_	
20-37	3236-3237	│	_	_	
20-38	3240-3241	├	_	_	
20-39	3241-3242	─	_	_	
20-40	3242-3243	─	_	_	
20-41	3244-3249	train	_	_	
20-42	3250-3251	│	_	_	
20-43	3254-3255	│	_	_	
20-44	3258-3259	├	_	_	
20-45	3259-3260	─	_	_	
20-46	3260-3261	─	_	_	
20-47	3262-3268	Images	_	_	
20-48	3269-3270	│	_	_	
20-49	3273-3274	│	_	_	
20-50	3277-3278	├	_	_	
20-51	3278-3279	─	_	_	
20-52	3279-3280	─	_	_	
20-53	3281-3299	CrowdHuman_train01	_	_	
20-54	3299-3300	.	_	_	
20-55	3300-3303	zip	_	_	
20-56	3304-3305	│	_	_	
20-57	3308-3309	│	_	_	
20-58	3312-3313	├	_	_	
20-59	3313-3314	─	_	_	
20-60	3314-3315	─	_	_	
20-61	3316-3334	CrowdHuman_train02	_	_	
20-62	3334-3335	.	_	_	
20-63	3335-3338	zip	_	_	
20-64	3339-3340	│	_	_	
20-65	3343-3344	│	_	_	
20-66	3347-3348	├	_	_	
20-67	3348-3349	─	_	_	
20-68	3349-3350	─	_	_	
20-69	3351-3369	CrowdHuman_train03	_	_	
20-70	3369-3370	.	_	_	
20-71	3370-3373	zip	_	_	
20-72	3374-3375	│	_	_	
20-73	3378-3379	├	_	_	
20-74	3379-3380	─	_	_	
20-75	3380-3381	─	_	_	
20-76	3382-3385	val	_	_	
20-77	3386-3387	│	_	_	
20-78	3390-3391	│	_	_	
20-79	3394-3395	├	_	_	
20-80	3395-3396	─	_	_	
20-81	3396-3397	─	_	_	
20-82	3398-3404	Images	_	_	
20-83	3405-3406	│	_	_	
20-84	3409-3410	│	_	_	
20-85	3413-3414	├	_	_	
20-86	3414-3415	─	_	_	
20-87	3415-3416	─	_	_	
20-88	3417-3435	CrowdHuman_val.zip	_	_	
20-89	3436-3437	|	_	_	
20-90	3440-3441	├	_	_	
20-91	3441-3442	─	_	_	
20-92	3442-3443	─	_	_	
20-93	3444-3455	annotations	_	_	
20-94	3456-3457	│	_	_	
20-95	3460-3461	│	_	_	
20-96	3464-3465	├	_	_	
20-97	3465-3466	─	_	_	
20-98	3466-3467	─	_	_	
20-99	3468-3489	crowdhuman_train.json	_	_	
20-100	3490-3491	│	_	_	
20-101	3494-3495	│	_	_	
20-102	3498-3499	├	_	_	
20-103	3499-3500	─	_	_	
20-104	3500-3501	─	_	_	
20-105	3502-3521	crowdhuman_val.json	_	_	
20-106	3522-3523	|	_	_	
20-107	3523-3524	-	_	_	
20-108	3524-3525	-	_	_	
20-109	3526-3531	MOT17	*	DATASET	
20-110	3532-3533	│	_	_	
20-111	3536-3537	├	_	_	
20-112	3537-3538	─	_	_	
20-113	3538-3539	─	_	_	
20-114	3540-3545	train	_	_	
20-115	3546-3547	│	_	_	
20-116	3550-3551	│	_	_	
20-117	3554-3555	├	_	_	
20-118	3555-3556	─	_	_	
20-119	3556-3557	─	_	_	
20-120	3558-3563	MOT17	*	DATASET	
20-121	3563-3564	-	_	_	
20-122	3564-3566	02	_	_	
20-123	3566-3567	-	_	_	
20-124	3567-3570	DPM	_	_	
20-125	3571-3572	│	_	_	
20-126	3575-3576	│	_	_	
20-127	3579-3580	├	_	_	
20-128	3580-3581	─	_	_	
20-129	3581-3582	─	_	_	
20-130	3583-3584	.	_	_	
20-131	3584-3585	.	_	_	
20-132	3585-3586	.	_	_	
20-133	3587-3588	│	_	_	
20-134	3591-3592	├	_	_	
20-135	3592-3593	─	_	_	
20-136	3593-3594	─	_	_	
20-137	3595-3599	test	_	_	
20-138	3600-3601	│	_	_	
20-139	3604-3605	├	_	_	
20-140	3605-3606	─	_	_	
20-141	3606-3607	─	_	_	
20-142	3608-3619	annotations	_	_	
20-143	3620-3621	│	_	_	
20-144	3624-3625	│	_	_	
20-145	3628-3629	├	_	_	
20-146	3629-3630	─	_	_	
20-147	3630-3631	─	_	_	
20-148	3632-3658	half-train_cocoformat.json	_	_	
20-149	3659-3660	│	_	_	
20-150	3663-3664	│	_	_	
20-151	3667-3668	├	_	_	
20-152	3668-3669	─	_	_	
20-153	3669-3670	─	_	_	
20-154	3671-3672	.	_	_	
20-155	3672-3673	.	_	_	
20-156	3673-3674	.	_	_	
20-157	3675-3676	|	_	_	
20-158	3676-3677	-	_	_	
20-159	3677-3678	-	_	_	
20-160	3679-3687	MOTSynth	*	DATASET	
20-161	3688-3689	|	_	_	
20-162	3692-3693	├	_	_	
20-163	3693-3694	─	_	_	
20-164	3694-3695	─	_	_	
20-165	3696-3702	videos	_	_	
20-166	3703-3704	│	_	_	
20-167	3707-3708	├	_	_	
20-168	3708-3709	─	_	_	
20-169	3709-3710	─	_	_	
20-170	3711-3722	annotations	_	_	
20-171	3723-3724	│	_	_	
20-172	3727-3728	├	_	_	
20-173	3728-3729	─	_	_	
20-174	3729-3730	─	_	_	
20-175	3731-3736	train	_	_	
20-176	3737-3738	│	_	_	
20-177	3741-3742	│	_	_	
20-178	3745-3746	├	_	_	
20-179	3746-3747	─	_	_	
20-180	3747-3748	─	_	_	
20-181	3749-3752	000	_	_	
20-182	3753-3754	│	_	_	
20-183	3757-3758	│	_	_	
20-184	3761-3762	│	_	_	
20-185	3765-3766	├	_	_	
20-186	3766-3767	─	_	_	
20-187	3767-3768	─	_	_	
20-188	3769-3773	img1	_	_	
20-189	3774-3775	│	_	_	
20-190	3778-3779	│	_	_	
20-191	3782-3783	│	_	_	
20-192	3786-3787	│	_	_	
20-193	3790-3791	├	_	_	
20-194	3791-3792	─	_	_	
20-195	3792-3793	─	_	_	
20-196	3794-3800	000001	_	_	
20-197	3800-3801	.	_	_	
20-198	3801-3804	jpg	_	_	
20-199	3805-3806	│	_	_	
20-200	3809-3810	│	_	_	
20-201	3813-3814	│	_	_	
20-202	3817-3818	│	_	_	
20-203	3821-3822	├	_	_	
20-204	3822-3823	─	_	_	
20-205	3823-3824	─	_	_	
20-206	3825-3826	.	_	_	
20-207	3826-3827	.	_	_	
20-208	3827-3828	.	_	_	
20-209	3829-3830	│	_	_	
20-210	3833-3834	│	_	_	
20-211	3837-3838	├	_	_	
20-212	3838-3839	─	_	_	
20-213	3839-3840	─	_	_	
20-214	3841-3842	.	_	_	
20-215	3842-3843	.	_	_	
20-216	3843-3844	.	_	_	
20-217	3845-3846	│	_	_	
20-218	3849-3850	├	_	_	
20-219	3850-3851	─	_	_	
20-220	3851-3852	─	_	_	
20-221	3853-3872	all_cocoformat.json	_	_	
20-222	3873-3874	`	_	_	
20-223	3874-3875	`	_	_	
20-224	3875-3876	`	_	_	
20-225	3877-3878	#	_	_	
20-226	3878-3879	#	_	_	
20-227	3879-3880	#	_	_	
20-228	3881-3891	Pretrained	_	_	
20-229	3892-3898	models	_	_	
20-230	3900-3902	We	_	_	
20-231	3903-3906	use	_	_	
20-232	3907-3911	CLIP	_	_	
20-233	3912-3922	pretrained	_	_	
20-234	3923-3926	ViT	_	_	
20-235	3927-3933	models	_	_	
20-236	3933-3934	.	_	_	

#Text=You can download them from [here](https://github.com/openai/CLIP/tree/main) and put them under the `pretrain` directory. 
#Text=### Training and Evaluation
#Text=
#Text=Training on single node
#Text=```shell
#Text=bash .
21-1	3935-3938	You	_	_	
21-2	3939-3942	can	_	_	
21-3	3943-3951	download	_	_	
21-4	3952-3956	them	_	_	
21-5	3957-3961	from	_	_	
21-6	3962-3963	[	_	_	
21-7	3963-3967	here	_	_	
21-8	3967-3968	]	_	_	
21-9	3968-3969	(	_	_	
21-10	3969-3974	https	_	_	
21-11	3974-3975	:	_	_	
21-12	3975-3976	/	_	_	
21-13	3976-3977	/	_	_	
21-14	3977-3987	github.com	_	_	
21-15	3987-3988	/	_	_	
21-16	3988-3994	openai	_	_	
21-17	3994-3995	/	_	_	
21-18	3995-3999	CLIP	_	_	
21-19	3999-4000	/	_	_	
21-20	4000-4004	tree	_	_	
21-21	4004-4005	/	_	_	
21-22	4005-4009	main	_	_	
21-23	4009-4010	)	_	_	
21-24	4011-4014	and	_	_	
21-25	4015-4018	put	_	_	
21-26	4019-4023	them	_	_	
21-27	4024-4029	under	_	_	
21-28	4030-4033	the	_	_	
21-29	4034-4035	`	_	_	
21-30	4035-4043	pretrain	_	_	
21-31	4043-4044	`	_	_	
21-32	4045-4054	directory	_	_	
21-33	4054-4055	.	_	_	
21-34	4057-4058	#	_	_	
21-35	4058-4059	#	_	_	
21-36	4059-4060	#	_	_	
21-37	4061-4069	Training	_	_	
21-38	4070-4073	and	_	_	
21-39	4074-4084	Evaluation	_	_	
21-40	4086-4094	Training	_	_	
21-41	4095-4097	on	_	_	
21-42	4098-4104	single	_	_	
21-43	4105-4109	node	_	_	
21-44	4110-4111	`	_	_	
21-45	4111-4112	`	_	_	
21-46	4112-4113	`	_	_	
21-47	4113-4118	shell	*	PROGLANG	
21-48	4119-4123	bash	*	SOFTWARE	
21-49	4124-4125	.	_	_	

#Text=/tools/dist_train.sh configs/mot/svm/svm_base.py 8 --cfg-options \\
#Text=   model.detector.backbone.pretrain=.
22-1	4125-4126	/	_	_	
22-2	4126-4131	tools	_	_	
22-3	4131-4132	/	_	_	
22-4	4132-4145	dist_train.sh	_	_	
22-5	4146-4153	configs	_	_	
22-6	4153-4154	/	_	_	
22-7	4154-4157	mot	_	_	
22-8	4157-4158	/	_	_	
22-9	4158-4161	svm	_	_	
22-10	4161-4162	/	_	_	
22-11	4162-4173	svm_base.py	_	_	
22-12	4174-4175	8	_	_	
22-13	4176-4177	-	_	_	
22-14	4177-4178	-	_	_	
22-15	4178-4189	cfg-options	_	_	
22-16	4190-4191	\	_	_	
22-17	4195-4227	model.detector.backbone.pretrain	_	_	
22-18	4227-4228	=	_	_	
22-19	4228-4229	.	_	_	

#Text=/pretrain/ViT-B-16.pt
#Text=```
#Text=Evaluation on MOT17 half validation set
#Text=```shell
#Text=bash .
23-1	4229-4230	/	_	_	
23-2	4230-4238	pretrain	_	_	
23-3	4238-4239	/	_	_	
23-4	4239-4244	ViT-B	_	_	
23-5	4244-4245	-	_	_	
23-6	4245-4247	16	_	_	
23-7	4247-4248	.	_	_	
23-8	4248-4250	pt	_	_	
23-9	4251-4252	`	_	_	
23-10	4252-4253	`	_	_	
23-11	4253-4254	`	_	_	
23-12	4255-4265	Evaluation	_	_	
23-13	4266-4268	on	_	_	
23-14	4269-4274	MOT17	*	DATASET	
23-15	4275-4279	half	_	_	
23-16	4280-4290	validation	_	_	
23-17	4291-4294	set	_	_	
23-18	4295-4296	`	_	_	
23-19	4296-4297	`	_	_	
23-20	4297-4298	`	_	_	
23-21	4298-4303	shell	*	PROGLANG	
23-22	4304-4308	bash	*	SOFTWARE	
23-23	4309-4310	.	_	_	

#Text=/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \\
#Text=   --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth
#Text=```
#Text=
#Text=## Main Results
#Text=### MOT17
#Text=| Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    |
#Text=| :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:|
#Text=| SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |
#Text=
#Text=## Citation
#Text=If you find this work useful in your research, please consider citing:
#Text=```
#Text=@InProceedings{Zhao_2023_CVPR,
#Text=    author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},
#Text=    title     = {Streaming Video Model},
#Text=    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
#Text=    month     = {June},
#Text=    year      = {2023},
#Text=    pages     = {14602-14612}
#Text=}
#Text=```
#Text=## Acknowledgement
#Text=Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).
24-1	4310-4311	/	_	_	
24-2	4311-4316	tools	_	_	
24-3	4316-4317	/	_	_	
24-4	4317-4329	dist_test.sh	_	_	
24-5	4330-4337	configs	_	_	
24-6	4337-4338	/	_	_	
24-7	4338-4341	mot	_	_	
24-8	4341-4342	/	_	_	
24-9	4342-4345	svm	_	_	
24-10	4345-4346	/	_	_	
24-11	4346-4357	svm_test.py	_	_	
24-12	4358-4359	8	_	_	
24-13	4360-4361	\	_	_	
24-14	4365-4366	-	_	_	
24-15	4366-4367	-	_	_	
24-16	4367-4371	eval	_	_	
24-17	4372-4376	bbox	_	_	
24-18	4377-4382	track	_	_	
24-19	4383-4384	-	_	_	
24-20	4384-4385	-	_	_	
24-21	4385-4395	checkpoint	_	_	
24-22	4396-4424	svm_motsync_ch_mot17half.pth	_	_	
24-23	4425-4426	`	_	_	
24-24	4426-4427	`	_	_	
24-25	4427-4428	`	_	_	
24-26	4430-4431	#	_	_	
24-27	4431-4432	#	_	_	
24-28	4433-4437	Main	_	_	
24-29	4438-4445	Results	_	_	
24-30	4446-4447	#	_	_	
24-31	4447-4448	#	_	_	
24-32	4448-4449	#	_	_	
24-33	4450-4455	MOT17	*	DATASET	
24-34	4456-4457	|	_	_	
24-35	4458-4464	Method	_	_	
24-36	4465-4466	|	_	_	
24-37	4467-4474	Dataset	_	_	
24-38	4475-4476	|	_	_	
24-39	4492-4497	Train	_	_	
24-40	4498-4502	Data	_	_	
24-41	4518-4519	|	_	_	
24-42	4520-4524	MOTA	*	EVALMETRIC	
24-43	4525-4526	|	_	_	
24-44	4527-4531	HOTA	*	EVALMETRIC	
24-45	4532-4533	|	_	_	
24-46	4534-4538	IDF1	*	EVALMETRIC	
24-47	4539-4540	|	_	_	
24-48	4544-4547	URL	_	_	
24-49	4551-4552	|	_	_	
24-50	4553-4554	|	_	_	
24-51	4555-4556	:	_	_	
24-52	4556-4557	-	_	_	
24-53	4557-4558	-	_	_	
24-54	4558-4559	-	_	_	
24-55	4559-4560	:	_	_	
24-56	4561-4562	|	_	_	
24-57	4563-4564	:	_	_	
24-58	4564-4565	-	_	_	
24-59	4565-4566	-	_	_	
24-60	4566-4567	-	_	_	
24-61	4567-4568	:	_	_	
24-62	4569-4570	|	_	_	
24-63	4570-4571	:	_	_	
24-64	4571-4572	-	_	_	
24-65	4572-4573	-	_	_	
24-66	4573-4574	-	_	_	
24-67	4574-4575	-	_	_	
24-68	4575-4576	-	_	_	
24-69	4576-4577	-	_	_	
24-70	4577-4578	-	_	_	
24-71	4578-4579	-	_	_	
24-72	4579-4580	-	_	_	
24-73	4580-4581	-	_	_	
24-74	4581-4582	-	_	_	
24-75	4582-4583	-	_	_	
24-76	4583-4584	-	_	_	
24-77	4584-4585	-	_	_	
24-78	4585-4586	-	_	_	
24-79	4586-4587	-	_	_	
24-80	4587-4588	-	_	_	
24-81	4588-4589	-	_	_	
24-82	4589-4590	-	_	_	
24-83	4590-4591	-	_	_	
24-84	4591-4592	-	_	_	
24-85	4592-4593	-	_	_	
24-86	4593-4594	-	_	_	
24-87	4594-4595	-	_	_	
24-88	4595-4596	-	_	_	
24-89	4596-4597	-	_	_	
24-90	4597-4598	-	_	_	
24-91	4598-4599	-	_	_	
24-92	4599-4600	-	_	_	
24-93	4600-4601	-	_	_	
24-94	4601-4602	-	_	_	
24-95	4602-4603	-	_	_	
24-96	4603-4604	-	_	_	
24-97	4604-4605	-	_	_	
24-98	4605-4606	-	_	_	
24-99	4606-4607	-	_	_	
24-100	4607-4608	-	_	_	
24-101	4608-4609	-	_	_	
24-102	4609-4610	-	_	_	
24-103	4610-4611	-	_	_	
24-104	4611-4612	:	_	_	
24-105	4612-4613	|	_	_	
24-106	4613-4614	:	_	_	
24-107	4614-4615	-	_	_	
24-108	4615-4616	-	_	_	
24-109	4616-4617	-	_	_	
24-110	4617-4618	-	_	_	
24-111	4618-4619	:	_	_	
24-112	4619-4620	|	_	_	
24-113	4620-4621	:	_	_	
24-114	4621-4622	-	_	_	
24-115	4622-4623	-	_	_	
24-116	4623-4624	-	_	_	
24-117	4624-4625	-	_	_	
24-118	4625-4626	:	_	_	
24-119	4626-4627	|	_	_	
24-120	4627-4628	:	_	_	
24-121	4628-4629	-	_	_	
24-122	4629-4630	-	_	_	
24-123	4630-4631	-	_	_	
24-124	4631-4632	-	_	_	
24-125	4632-4633	:	_	_	
24-126	4633-4634	|	_	_	
24-127	4634-4635	:	_	_	
24-128	4635-4636	-	_	_	
24-129	4636-4637	-	_	_	
24-130	4637-4638	-	_	_	
24-131	4638-4639	-	_	_	
24-132	4639-4640	-	_	_	
24-133	4640-4641	-	_	_	
24-134	4641-4642	-	_	_	
24-135	4642-4643	-	_	_	
24-136	4643-4644	-	_	_	
24-137	4644-4645	:	_	_	
24-138	4645-4646	|	_	_	
24-139	4647-4648	|	_	_	
24-140	4649-4652	SVM	_	_	
24-141	4653-4654	|	_	_	
24-142	4655-4660	MOT17	*	DATASET	
24-143	4661-4662	|	_	_	
24-144	4663-4668	MOT17	*	DATASET	
24-145	4669-4679	half-train	_	_	
24-146	4680-4681	+	_	_	
24-147	4682-4692	crowdhuman	*	DATASET	
24-148	4693-4694	+	_	_	
24-149	4695-4703	MOTSynth	*	DATASET	
24-150	4704-4705	|	_	_	
24-151	4706-4710	79.7	_	_	
24-152	4711-4712	|	_	_	
24-153	4713-4717	68.1	_	_	
24-154	4718-4719	|	_	_	
24-155	4720-4724	80.9	_	_	
24-156	4725-4726	|	_	_	
24-157	4727-4728	[	_	_	
24-158	4728-4733	model	_	_	
24-159	4733-4734	]	_	_	
24-160	4734-4735	(	_	_	
24-161	4735-4740	https	_	_	
24-162	4740-4741	:	_	_	
24-163	4741-4742	/	_	_	
24-164	4742-4743	/	_	_	
24-165	4743-4753	github.com	_	_	
24-166	4753-4754	/	_	_	
24-167	4754-4760	yuzhms	_	_	
24-168	4760-4761	/	_	_	
24-169	4761-4782	Streaming-Video-Model	_	_	
24-170	4782-4783	/	_	_	
24-171	4783-4791	releases	_	_	
24-172	4791-4792	/	_	_	
24-173	4792-4800	download	_	_	
24-174	4800-4801	/	_	_	
24-175	4801-4805	v1.0	_	_	
24-176	4805-4806	/	_	_	
24-177	4806-4815	svm_mot17	_	_	
24-178	4815-4816	-	_	_	
24-179	4816-4828	half-val.pth	_	_	
24-180	4828-4829	)	_	_	
24-181	4830-4831	|	_	_	
24-182	4833-4834	#	_	_	
24-183	4834-4835	#	_	_	
24-184	4836-4844	Citation	_	_	
24-185	4845-4847	If	_	_	
24-186	4848-4851	you	_	_	
24-187	4852-4856	find	_	_	
24-188	4857-4861	this	_	_	
24-189	4862-4866	work	_	_	
24-190	4867-4873	useful	_	_	
24-191	4874-4876	in	_	_	
24-192	4877-4881	your	_	_	
24-193	4882-4890	research	_	_	
24-194	4890-4891	,	_	_	
24-195	4892-4898	please	_	_	
24-196	4899-4907	consider	_	_	
24-197	4908-4914	citing	_	_	
24-198	4914-4915	:	_	_	
24-199	4916-4917	`	_	_	
24-200	4917-4918	`	_	_	
24-201	4918-4919	`	_	_	
24-202	4920-4921	@	_	_	
24-203	4921-4934	InProceedings	_	_	
24-204	4934-4935	{	_	_	
24-205	4935-4939	Zhao	_	_	
24-206	4939-4940	_	_	_	
24-207	4940-4944	2023	_	_	
24-208	4944-4945	_	_	_	
24-209	4945-4949	CVPR	*[14]	CONFERENCE[14]	
24-210	4949-4950	,	*[14]	CONFERENCE[14]	
24-211	4955-4961	author	_	_	
24-212	4965-4966	=	_	_	
24-213	4967-4968	{	_	_	
24-214	4968-4972	Zhao	_	_	
24-215	4972-4973	,	_	_	
24-216	4974-4981	Yucheng	_	_	
24-217	4982-4985	and	_	_	
24-218	4986-4989	Luo	_	_	
24-219	4989-4990	,	_	_	
24-220	4991-4996	Chong	_	_	
24-221	4997-5000	and	_	_	
24-222	5001-5005	Tang	_	_	
24-223	5005-5006	,	_	_	
24-224	5007-5015	Chuanxin	_	_	
24-225	5016-5019	and	_	_	
24-226	5020-5024	Chen	_	_	
24-227	5024-5025	,	_	_	
24-228	5026-5034	Dongdong	_	_	
24-229	5035-5038	and	_	_	
24-230	5039-5046	Codella	_	_	
24-231	5046-5047	,	_	_	
24-232	5048-5052	Noel	_	_	
24-233	5053-5056	and	_	_	
24-234	5057-5060	Zha	_	_	
24-235	5060-5061	,	_	_	
24-236	5062-5071	Zheng-Jun	_	_	
24-237	5071-5072	}	_	_	
24-238	5072-5073	,	_	_	
24-239	5078-5083	title	_	_	
24-240	5088-5089	=	_	_	
24-241	5090-5091	{	_	_	
24-242	5091-5100	Streaming	*[15]	PUBLICATION[15]	
24-243	5101-5106	Video	*[15]	PUBLICATION[15]	
24-244	5107-5112	Model	*[15]	PUBLICATION[15]	
24-245	5112-5113	}	_	_	
24-246	5113-5114	,	_	_	
24-247	5119-5128	booktitle	_	_	
24-248	5129-5130	=	_	_	
24-249	5131-5132	{	_	_	
24-250	5132-5143	Proceedings	*[16]	PUBLICATION[16]	
24-251	5144-5146	of	*[16]	PUBLICATION[16]	
24-252	5147-5150	the	*[16]	PUBLICATION[16]	
24-253	5151-5155	IEEE	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-254	5155-5156	/	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-255	5156-5159	CVF	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-256	5160-5170	Conference	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-257	5171-5173	on	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-258	5174-5182	Computer	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-259	5183-5189	Vision	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-260	5190-5193	and	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-261	5194-5201	Pattern	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-262	5202-5213	Recognition	*[16]|*[17]	PUBLICATION[16]|CONFERENCE[17]	
24-263	5214-5215	(	*[16]	PUBLICATION[16]	
24-264	5215-5219	CVPR	*[16]|*[18]	PUBLICATION[16]|CONFERENCE[18]	
24-265	5219-5220	)	*[16]	PUBLICATION[16]	
24-266	5220-5221	}	_	_	
24-267	5221-5222	,	_	_	
24-268	5227-5232	month	_	_	
24-269	5237-5238	=	_	_	
24-270	5239-5240	{	_	_	
24-271	5240-5244	June	_	_	
24-272	5244-5245	}	_	_	
24-273	5245-5246	,	_	_	
24-274	5251-5255	year	_	_	
24-275	5261-5262	=	_	_	
24-276	5263-5264	{	_	_	
24-277	5264-5268	2023	_	_	
24-278	5268-5269	}	_	_	
24-279	5269-5270	,	_	_	
24-280	5275-5280	pages	_	_	
24-281	5285-5286	=	_	_	
24-282	5287-5288	{	_	_	
24-283	5288-5293	14602	_	_	
24-284	5293-5294	-	_	_	
24-285	5294-5299	14612	_	_	
24-286	5299-5300	}	_	_	
24-287	5301-5302	}	_	_	
24-288	5303-5304	`	_	_	
24-289	5304-5305	`	_	_	
24-290	5305-5306	`	_	_	
24-291	5307-5308	#	_	_	
24-292	5308-5309	#	_	_	
24-293	5310-5325	Acknowledgement	_	_	
24-294	5326-5329	Our	_	_	
24-295	5330-5334	code	_	_	
24-296	5335-5338	are	_	_	
24-297	5339-5344	built	_	_	
24-298	5345-5347	on	_	_	
24-299	5348-5351	top	_	_	
24-300	5352-5354	of	_	_	
24-301	5355-5356	[	_	_	
24-302	5356-5366	MMTracking	_	_	
24-303	5366-5367	]	_	_	
24-304	5367-5368	(	_	_	
24-305	5368-5373	https	_	_	
24-306	5373-5374	:	_	_	
24-307	5374-5375	/	_	_	
24-308	5375-5376	/	_	_	
24-309	5376-5386	github.com	_	_	
24-310	5386-5387	/	_	_	
24-311	5387-5397	open-mmlab	_	_	
24-312	5397-5398	/	_	_	
24-313	5398-5408	mmtracking	_	_	
24-314	5408-5409	/	_	_	
24-315	5409-5410	)	_	_	
24-316	5411-5414	and	_	_	
24-317	5415-5416	[	_	_	
24-318	5416-5420	CLIP	_	_	
24-319	5420-5421	]	_	_	
24-320	5421-5422	(	_	_	
24-321	5422-5427	https	_	_	
24-322	5427-5428	:	_	_	
24-323	5428-5429	/	_	_	
24-324	5429-5430	/	_	_	
24-325	5430-5440	github.com	_	_	
24-326	5440-5441	/	_	_	
24-327	5441-5447	openai	_	_	
24-328	5447-5448	/	_	_	
24-329	5448-5452	CLIP	_	_	
24-330	5452-5453	)	_	_	
24-331	5453-5454	.	_	_	

#Text=Many thanks for their wonderful works.
25-1	5455-5459	Many	_	_	
25-2	5460-5466	thanks	_	_	
25-3	5467-5470	for	_	_	
25-4	5471-5476	their	_	_	
25-5	5477-5486	wonderful	_	_	
25-6	5487-5492	works	_	_	
25-7	5492-5493	.	_	_	
