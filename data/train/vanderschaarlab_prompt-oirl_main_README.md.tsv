#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# ü™Ñ Prompt-OIRL: Learning to Prompt LLMs with Expert Knowledge (Known Magic Words üßô)
#Text=
#Text=### üíª Implementation and üìí tutorial for ICLR 2024 paper
#Text=
#Text= !
1-1	0-1	#	_	_	
1-2	2-4	ü™Ñ	_	_	
1-3	5-16	Prompt-OIRL	*	SOFTWARE	
1-4	16-17	:	_	_	
1-5	18-26	Learning	_	_	
1-6	27-29	to	_	_	
1-7	30-36	Prompt	_	_	
1-8	37-41	LLMs	_	_	
1-9	42-46	with	_	_	
1-10	47-53	Expert	_	_	
1-11	54-63	Knowledge	_	_	
1-12	64-65	(	_	_	
1-13	65-70	Known	_	_	
1-14	71-76	Magic	_	_	
1-15	77-82	Words	_	_	
1-16	83-85	üßô	_	_	
1-17	85-86	)	_	_	
1-18	88-89	#	_	_	
1-19	89-90	#	_	_	
1-20	90-91	#	_	_	
1-21	92-94	üíª	_	_	
1-22	95-109	Implementation	_	_	
1-23	110-113	and	_	_	
1-24	114-116	üìí	_	_	
1-25	117-125	tutorial	_	_	
1-26	126-129	for	_	_	
1-27	130-134	ICLR	*[1]	CONFERENCE[1]	
1-28	135-139	2024	*[1]	CONFERENCE[1]	
1-29	140-145	paper	_	_	
1-30	148-149	!	_	_	

#Text=[Image](prompt-oirl-title.png)
#Text=
#Text=- [Paper Link](https://arxiv.org/pdf/2309.06553.pdf)
#Text=- [Open Review Link](https://openreview.net/forum?
2-1	149-150	[	_	_	
2-2	150-155	Image	_	_	
2-3	155-156	]	_	_	
2-4	156-157	(	_	_	
2-5	157-178	prompt-oirl-title.png	_	_	
2-6	178-179	)	_	_	
2-7	181-182	-	_	_	
2-8	183-184	[	_	_	
2-9	184-189	Paper	_	_	
2-10	190-194	Link	_	_	
2-11	194-195	]	_	_	
2-12	195-196	(	_	_	
2-13	196-201	https	_	_	
2-14	201-202	:	_	_	
2-15	202-203	/	_	_	
2-16	203-204	/	_	_	
2-17	204-213	arxiv.org	_	_	
2-18	213-214	/	_	_	
2-19	214-217	pdf	_	_	
2-20	217-218	/	_	_	
2-21	218-228	2309.06553	_	_	
2-22	228-229	.	_	_	
2-23	229-232	pdf	_	_	
2-24	232-233	)	_	_	
2-25	234-235	-	_	_	
2-26	236-237	[	_	_	
2-27	237-241	Open	_	_	
2-28	242-248	Review	_	_	
2-29	249-253	Link	_	_	
2-30	253-254	]	_	_	
2-31	254-255	(	_	_	
2-32	255-260	https	_	_	
2-33	260-261	:	_	_	
2-34	261-262	/	_	_	
2-35	262-263	/	_	_	
2-36	263-277	openreview.net	_	_	
2-37	277-278	/	_	_	
2-38	278-283	forum	_	_	
2-39	283-284	?	_	_	

#Text=id=N6o0ZtPzTg)
#Text=
#Text=
#Text=## üî• News
#Text=- (2024.2) (Internal Code-Reviewing) Code with GPT3.5 and TigerBot to be released.
#Text=- (2024.1) Prompt-OIRL has been accepted by ICLR'2024.
3-1	284-286	id	_	_	
3-2	286-287	=	_	_	
3-3	287-297	N6o0ZtPzTg	_	_	
3-4	297-298	)	_	_	
3-5	301-302	#	_	_	
3-6	302-303	#	_	_	
3-7	304-306	üî•	_	_	
3-8	307-311	News	_	_	
3-9	312-313	-	_	_	
3-10	314-315	(	_	_	
3-11	315-321	2024.2	_	_	
3-12	321-322	)	_	_	
3-13	323-324	(	_	_	
3-14	324-332	Internal	_	_	
3-15	333-347	Code-Reviewing	_	_	
3-16	347-348	)	_	_	
3-17	349-353	Code	_	_	
3-18	354-358	with	_	_	
3-19	359-365	GPT3.5	*	SOFTWARE	
3-20	366-369	and	_	_	
3-21	370-378	TigerBot	*	SOFTWARE	
3-22	379-381	to	_	_	
3-23	382-384	be	_	_	
3-24	385-393	released	_	_	
3-25	393-394	.	_	_	
3-26	395-396	-	_	_	
3-27	397-398	(	_	_	
3-28	398-404	2024.1	_	_	
3-29	404-405	)	_	_	
3-30	406-417	Prompt-OIRL	*	SOFTWARE	
3-31	418-421	has	_	_	
3-32	422-426	been	_	_	
3-33	427-435	accepted	_	_	
3-34	436-438	by	_	_	
3-35	439-443	ICLR	*[2]	CONFERENCE[2]	
3-36	443-444	'	*[2]	CONFERENCE[2]	
3-37	444-448	2024	*[2]	CONFERENCE[2]	
3-38	448-449	.	_	_	

#Text=We look forward to talking with you in Vienna!
4-1	450-452	We	_	_	
4-2	453-457	look	_	_	
4-3	458-465	forward	_	_	
4-4	466-468	to	_	_	
4-5	469-476	talking	_	_	
4-6	477-481	with	_	_	
4-7	482-485	you	_	_	
4-8	486-488	in	_	_	
4-9	489-495	Vienna	_	_	
4-10	495-496	!	_	_	

#Text=- (2024.12) Prompt-OIRL has been presented at the NeurIPS conference.
5-1	497-498	-	_	_	
5-2	499-500	(	_	_	
5-3	500-507	2024.12	_	_	
5-4	507-508	)	_	_	
5-5	509-520	Prompt-OIRL	*	SOFTWARE	
5-6	521-524	has	_	_	
5-7	525-529	been	_	_	
5-8	530-539	presented	_	_	
5-9	540-542	at	_	_	
5-10	543-546	the	_	_	
5-11	547-554	NeurIPS	*	CONFERENCE	
5-12	555-565	conference	_	_	
5-13	565-566	.	_	_	

#Text=Thanks for all the invaluable feedback!
6-1	567-573	Thanks	_	_	
6-2	574-577	for	_	_	
6-3	578-581	all	_	_	
6-4	582-585	the	_	_	
6-5	586-596	invaluable	_	_	
6-6	597-605	feedback	_	_	
6-7	605-606	!	_	_	

#Text=- (2023.10) Code with LLaMA2 has been released.
#Text=- (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**.
#Text=- (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.
#Text=
#Text=## üìñ Abstract
#Text=
#Text=> In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.
7-1	607-608	-	_	_	
7-2	609-610	(	_	_	
7-3	610-617	2023.10	_	_	
7-4	617-618	)	_	_	
7-5	619-623	Code	_	_	
7-6	624-628	with	_	_	
7-7	629-635	LLaMA2	*	SOFTWARE	
7-8	636-639	has	_	_	
7-9	640-644	been	_	_	
7-10	645-653	released	_	_	
7-11	653-654	.	_	_	
7-12	655-656	-	_	_	
7-13	657-658	(	_	_	
7-14	658-665	2023.10	_	_	
7-15	665-666	)	_	_	
7-16	667-678	Prompt-OIRL	*	SOFTWARE	
7-17	679-682	has	_	_	
7-18	683-687	been	_	_	
7-19	688-696	featured	_	_	
7-20	697-699	in	_	_	
7-21	700-701	a	_	_	
7-22	702-713	positioning	_	_	
7-23	714-715	[	_	_	
7-24	715-720	paper	_	_	
7-25	720-721	]	_	_	
7-26	721-722	(	_	_	
7-27	722-727	https	_	_	
7-28	727-728	:	_	_	
7-29	728-729	/	_	_	
7-30	729-730	/	_	_	
7-31	730-739	arxiv.org	_	_	
7-32	739-740	/	_	_	
7-33	740-743	pdf	_	_	
7-34	743-744	/	_	_	
7-35	744-754	2310.06147	_	_	
7-36	754-755	.	_	_	
7-37	755-758	pdf	_	_	
7-38	758-759	)	_	_	
7-39	760-762	as	_	_	
7-40	763-765	an	_	_	
7-41	766-773	example	_	_	
7-42	774-776	of	_	_	
7-43	777-778	*	_	_	
7-44	778-779	*	_	_	
7-45	779-786	inverse	_	_	
7-46	787-796	alignment	_	_	
7-47	796-797	*	_	_	
7-48	797-798	*	_	_	
7-49	798-799	.	_	_	
7-50	800-801	-	_	_	
7-51	802-803	(	_	_	
7-52	803-809	2023.9	_	_	
7-53	809-810	)	_	_	
7-54	811-822	Prompt-OIRL	*	SOFTWARE	
7-55	823-826	has	_	_	
7-56	827-831	been	_	_	
7-57	832-840	selected	_	_	
7-58	841-843	as	_	_	
7-59	844-846	an	_	_	
7-60	847-848	*	_	_	
7-61	848-849	*	_	_	
7-62	849-853	oral	_	_	
7-63	854-866	presentation	_	_	
7-64	866-867	*	_	_	
7-65	867-868	*	_	_	
7-66	869-871	at	_	_	
7-67	872-875	the	_	_	
7-68	876-881	ENLSP	*	WORKSHOP	
7-69	882-890	workshop	_	_	
7-70	891-893	at	_	_	
7-71	894-901	NeurIPS	*[3]	CONFERENCE[3]	
7-72	901-902	'	*[3]	CONFERENCE[3]	
7-73	902-906	2023	*[3]	CONFERENCE[3]	
7-74	906-907	.	_	_	
7-75	909-910	#	_	_	
7-76	910-911	#	_	_	
7-77	912-914	üìñ	_	_	
7-78	915-923	Abstract	_	_	
7-79	925-926	>	_	_	
7-80	927-929	In	_	_	
7-81	930-934	this	_	_	
7-82	935-940	study	_	_	
7-83	940-941	,	_	_	
7-84	942-944	we	_	_	
7-85	945-948	aim	_	_	
7-86	949-951	to	_	_	
7-87	952-959	enhance	_	_	
7-88	960-963	the	_	_	
7-89	964-974	arithmetic	_	_	
7-90	975-984	reasoning	_	_	
7-91	985-992	ability	_	_	
7-92	993-995	of	_	_	
7-93	996-1001	Large	_	_	
7-94	1002-1010	Language	_	_	
7-95	1011-1017	Models	_	_	
7-96	1018-1019	(	_	_	
7-97	1019-1023	LLMs	_	_	
7-98	1023-1024	)	_	_	
7-99	1025-1032	through	_	_	
7-100	1033-1042	zero-shot	_	_	
7-101	1043-1049	prompt	_	_	
7-102	1050-1062	optimization	_	_	
7-103	1062-1063	.	_	_	

#Text=We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques.
8-1	1064-1066	We	_	_	
8-2	1067-1075	identify	_	_	
8-3	1076-1077	a	_	_	
8-4	1078-1088	previously	_	_	
8-5	1089-1099	overlooked	_	_	
8-6	1100-1109	objective	_	_	
8-7	1110-1112	of	_	_	
8-8	1113-1118	query	_	_	
8-9	1119-1129	dependency	_	_	
8-10	1130-1132	in	_	_	
8-11	1133-1137	such	_	_	
8-12	1138-1150	optimization	_	_	
8-13	1151-1154	and	_	_	
8-14	1155-1164	elucidate	_	_	
8-15	1165-1168	two	_	_	
8-16	1169-1176	ensuing	_	_	
8-17	1177-1187	challenges	_	_	
8-18	1188-1192	that	_	_	
8-19	1193-1199	impede	_	_	
8-20	1200-1203	the	_	_	
8-21	1204-1214	successful	_	_	
8-22	1215-1218	and	_	_	
8-23	1219-1229	economical	_	_	
8-24	1230-1236	design	_	_	
8-25	1237-1239	of	_	_	
8-26	1240-1246	prompt	_	_	
8-27	1247-1259	optimization	_	_	
8-28	1260-1270	techniques	_	_	
8-29	1270-1271	.	_	_	

#Text=One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable.
9-1	1272-1275	One	_	_	
9-2	1276-1283	primary	_	_	
9-3	1284-1289	issue	_	_	
9-4	1290-1292	is	_	_	
9-5	1293-1296	the	_	_	
9-6	1297-1304	absence	_	_	
9-7	1305-1307	of	_	_	
9-8	1308-1310	an	_	_	
9-9	1311-1320	effective	_	_	
9-10	1321-1327	method	_	_	
9-11	1328-1330	to	_	_	
9-12	1331-1339	evaluate	_	_	
9-13	1340-1347	prompts	_	_	
9-14	1348-1354	during	_	_	
9-15	1355-1364	inference	_	_	
9-16	1365-1369	when	_	_	
9-17	1370-1373	the	_	_	
9-18	1374-1380	golden	_	_	
9-19	1381-1387	answer	_	_	
9-20	1388-1390	is	_	_	
9-21	1391-1402	unavailable	_	_	
9-22	1402-1403	.	_	_	

#Text=Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive.
10-1	1404-1416	Concurrently	_	_	
10-2	1416-1417	,	_	_	
10-3	1418-1426	learning	_	_	
10-4	1427-1430	via	_	_	
10-5	1431-1443	interactions	_	_	
10-6	1444-1448	with	_	_	
10-7	1449-1452	the	_	_	
10-8	1453-1457	LLMs	_	_	
10-9	1458-1460	to	_	_	
10-10	1461-1469	navigate	_	_	
10-11	1470-1473	the	_	_	
10-12	1474-1483	expansive	_	_	
10-13	1484-1491	natural	_	_	
10-14	1492-1500	language	_	_	
10-15	1501-1510	prompting	_	_	
10-16	1511-1516	space	_	_	
10-17	1517-1523	proves	_	_	
10-18	1524-1526	to	_	_	
10-19	1527-1529	be	_	_	
10-20	1530-1548	resource-intensive	_	_	
10-21	1548-1549	.	_	_	

#Text=To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data.
11-1	1550-1552	To	_	_	
11-2	1553-1560	address	_	_	
11-3	1561-1565	this	_	_	
11-4	1565-1566	,	_	_	
11-5	1567-1569	we	_	_	
11-6	1570-1579	introduce	_	_	
11-7	1580-1591	Prompt-OIRL	*	SOFTWARE	
11-8	1591-1592	,	_	_	
11-9	1593-1598	which	_	_	
11-10	1599-1608	harnesses	_	_	
11-11	1609-1616	offline	_	_	
11-12	1617-1624	inverse	_	_	
11-13	1625-1638	reinforcement	_	_	
11-14	1639-1647	learning	_	_	
11-15	1648-1650	to	_	_	
11-16	1651-1655	draw	_	_	
11-17	1656-1664	insights	_	_	
11-18	1665-1669	from	_	_	
11-19	1670-1677	offline	_	_	
11-20	1678-1687	prompting	_	_	
11-21	1688-1701	demonstration	_	_	
11-22	1702-1706	data	_	_	
11-23	1706-1707	.	_	_	

#Text=Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets.
12-1	1708-1712	Such	_	_	
12-2	1713-1717	data	_	_	
12-3	1718-1724	exists	_	_	
12-4	1725-1727	as	_	_	
12-5	1728-1739	by-products	_	_	
12-6	1740-1744	when	_	_	
12-7	1745-1752	diverse	_	_	
12-8	1753-1760	prompts	_	_	
12-9	1761-1764	are	_	_	
12-10	1765-1776	benchmarked	_	_	
12-11	1777-1779	on	_	_	
12-12	1780-1795	open-accessible	_	_	
12-13	1796-1804	datasets	_	_	
12-14	1804-1805	.	_	_	

#Text=With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model.
13-1	1806-1810	With	_	_	
13-2	1811-1822	Prompt-OIRL	*	SOFTWARE	
13-3	1822-1823	,	_	_	
13-4	1824-1827	the	_	_	
13-5	1828-1843	query-dependent	_	_	
13-6	1844-1850	prompt	_	_	
13-7	1851-1863	optimization	_	_	
13-8	1864-1873	objective	_	_	
13-9	1874-1876	is	_	_	
13-10	1877-1885	achieved	_	_	
13-11	1886-1888	by	_	_	
13-12	1889-1894	first	_	_	
13-13	1895-1903	learning	_	_	
13-14	1904-1906	an	_	_	
13-15	1907-1914	offline	_	_	
13-16	1915-1921	reward	_	_	
13-17	1922-1927	model	_	_	
13-18	1927-1928	.	_	_	

#Text=This model can evaluate any query-prompt pairs without accessing LLMs.
14-1	1929-1933	This	_	_	
14-2	1934-1939	model	_	_	
14-3	1940-1943	can	_	_	
14-4	1944-1952	evaluate	_	_	
14-5	1953-1956	any	_	_	
14-6	1957-1969	query-prompt	_	_	
14-7	1970-1975	pairs	_	_	
14-8	1976-1983	without	_	_	
14-9	1984-1993	accessing	_	_	
14-10	1994-1998	LLMs	_	_	
14-11	1998-1999	.	_	_	

#Text=Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.
15-1	2000-2012	Subsequently	_	_	
15-2	2012-2013	,	_	_	
15-3	2014-2015	a	_	_	
15-4	2016-2025	best-of-N	_	_	
15-5	2026-2034	strategy	_	_	
15-6	2035-2037	is	_	_	
15-7	2038-2046	deployed	_	_	
15-8	2047-2049	to	_	_	
15-9	2050-2059	recommend	_	_	
15-10	2060-2063	the	_	_	
15-11	2064-2071	optimal	_	_	
15-12	2072-2078	prompt	_	_	
15-13	2078-2079	.	_	_	

#Text=Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.
#Text=
#Text=## ü§î Motivating Example
#Text=
#Text=!
16-1	2080-2083	Our	_	_	
16-2	2084-2096	experimental	_	_	
16-3	2097-2108	evaluations	_	_	
16-4	2109-2115	across	_	_	
16-5	2116-2123	various	_	_	
16-6	2124-2127	LLM	_	_	
16-7	2128-2134	scales	_	_	
16-8	2135-2138	and	_	_	
16-9	2139-2149	arithmetic	_	_	
16-10	2150-2159	reasoning	_	_	
16-11	2160-2168	datasets	_	_	
16-12	2169-2179	underscore	_	_	
16-13	2180-2184	both	_	_	
16-14	2185-2188	the	_	_	
16-15	2189-2197	efficacy	_	_	
16-16	2198-2201	and	_	_	
16-17	2202-2210	economic	_	_	
16-18	2211-2220	viability	_	_	
16-19	2221-2223	of	_	_	
16-20	2224-2227	the	_	_	
16-21	2228-2236	proposed	_	_	
16-22	2237-2245	approach	_	_	
16-23	2245-2246	.	_	_	
16-24	2248-2249	#	_	_	
16-25	2249-2250	#	_	_	
16-26	2251-2253	ü§î	_	_	
16-27	2254-2264	Motivating	_	_	
16-28	2265-2272	Example	_	_	
16-29	2274-2275	!	_	_	

#Text=[Image](motivatingexample.png)
#Text=Figure 1.
17-1	2275-2276	[	_	_	
17-2	2276-2281	Image	_	_	
17-3	2281-2282	]	_	_	
17-4	2282-2283	(	_	_	
17-5	2283-2304	motivatingexample.png	_	_	
17-6	2304-2305	)	_	_	
17-7	2306-2312	Figure	_	_	
17-8	2313-2314	1	_	_	
17-9	2314-2315	.	_	_	

#Text=**No prompt is perfect that works for all queries**.
18-1	2316-2317	*	_	_	
18-2	2317-2318	*	_	_	
18-3	2318-2320	No	_	_	
18-4	2321-2327	prompt	_	_	
18-5	2328-2330	is	_	_	
18-6	2331-2338	perfect	_	_	
18-7	2339-2343	that	_	_	
18-8	2344-2349	works	_	_	
18-9	2350-2353	for	_	_	
18-10	2354-2357	all	_	_	
18-11	2358-2365	queries	_	_	
18-12	2365-2366	*	_	_	
18-13	2366-2367	*	_	_	
18-14	2367-2368	.	_	_	

#Text=The optimal prompt is query-dependent.
19-1	2369-2372	The	_	_	
19-2	2373-2380	optimal	_	_	
19-3	2381-2387	prompt	_	_	
19-4	2388-2390	is	_	_	
19-5	2391-2406	query-dependent	_	_	
19-6	2406-2407	.	_	_	

#Text=Yet the seeking of such prompts can be costly and inefficient.
20-1	2408-2411	Yet	_	_	
20-2	2412-2415	the	_	_	
20-3	2416-2423	seeking	_	_	
20-4	2424-2426	of	_	_	
20-5	2427-2431	such	_	_	
20-6	2432-2439	prompts	_	_	
20-7	2440-2443	can	_	_	
20-8	2444-2446	be	_	_	
20-9	2447-2453	costly	_	_	
20-10	2454-2457	and	_	_	
20-11	2458-2469	inefficient	_	_	
20-12	2469-2470	.	_	_	

#Text=Prompt-OIRL optimizes prompt during inference time on a **query-dependent** level effectively and cost-efficiently.
21-1	2475-2486	Prompt-OIRL	*	SOFTWARE	
21-2	2487-2496	optimizes	_	_	
21-3	2497-2503	prompt	_	_	
21-4	2504-2510	during	_	_	
21-5	2511-2520	inference	_	_	
21-6	2521-2525	time	_	_	
21-7	2526-2528	on	_	_	
21-8	2529-2530	a	_	_	
21-9	2531-2532	*	_	_	
21-10	2532-2533	*	_	_	
21-11	2533-2548	query-dependent	_	_	
21-12	2548-2549	*	_	_	
21-13	2549-2550	*	_	_	
21-14	2551-2556	level	_	_	
21-15	2557-2568	effectively	_	_	
21-16	2569-2572	and	_	_	
21-17	2573-2589	cost-efficiently	_	_	
21-18	2589-2590	.	_	_	

#Text=(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))
#Text=
#Text=## ‚öôÔ∏è Reproduction
#Text=
#Text=### Preliminaries
#Text=
#Text=To reproduce our results (e.g., using LLaMA2)
#Text=
#Text=1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).
#Text=
#Text=2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)
#Text=
#Text=### Create a Virtual Env
#Text=1.
22-1	2591-2592	(	_	_	
22-2	2592-2600	original	_	_	
22-3	2601-2605	chat	_	_	
22-4	2606-2610	logs	_	_	
22-5	2611-2615	with	_	_	
22-6	2616-2620	GPT4	*	SOFTWARE	
22-7	2621-2624	for	_	_	
22-8	2625-2630	those	_	_	
22-9	2631-2641	motivating	_	_	
22-10	2642-2650	examples	_	_	
22-11	2651-2654	can	_	_	
22-12	2655-2657	be	_	_	
22-13	2658-2663	found	_	_	
22-14	2664-2666	at	_	_	
22-15	2667-2668	[	_	_	
22-16	2668-2672	Left	_	_	
22-17	2672-2673	]	_	_	
22-18	2673-2674	(	_	_	
22-19	2674-2679	https	_	_	
22-20	2679-2680	:	_	_	
22-21	2680-2681	/	_	_	
22-22	2681-2682	/	_	_	
22-23	2682-2697	chat.openai.com	_	_	
22-24	2697-2698	/	_	_	
22-25	2698-2703	share	_	_	
22-26	2703-2704	/	_	_	
22-27	2704-2712	0f2d11b1	_	_	
22-28	2712-2713	-	_	_	
22-29	2713-2717	322a	_	_	
22-30	2717-2718	-	_	_	
22-31	2718-2722	4c47	_	_	
22-32	2722-2723	-	_	_	
22-33	2723-2727	a877	_	_	
22-34	2727-2728	-	_	_	
22-35	2728-2740	ad6fbace8179	_	_	
22-36	2740-2741	)	_	_	
22-37	2741-2742	,	_	_	
22-38	2743-2744	[	_	_	
22-39	2744-2749	Right	_	_	
22-40	2749-2750	]	_	_	
22-41	2750-2751	(	_	_	
22-42	2751-2756	https	_	_	
22-43	2756-2757	:	_	_	
22-44	2757-2758	/	_	_	
22-45	2758-2759	/	_	_	
22-46	2759-2774	chat.openai.com	_	_	
22-47	2774-2775	/	_	_	
22-48	2775-2780	share	_	_	
22-49	2780-2781	/	_	_	
22-50	2781-2789	15870a47	_	_	
22-51	2789-2790	-	_	_	
22-52	2790-2794	93c7	_	_	
22-53	2794-2795	-	_	_	
22-54	2795-2799	4b98	_	_	
22-55	2799-2800	-	_	_	
22-56	2800-2804	96c8	_	_	
22-57	2804-2805	-	_	_	
22-58	2805-2817	af0516c0c999	_	_	
22-59	2817-2818	)	_	_	
22-60	2818-2819	)	_	_	
22-61	2821-2822	#	_	_	
22-62	2822-2823	#	_	_	
22-63	2824-2826	‚öôÔ∏è	_	_	
22-64	2827-2839	Reproduction	_	_	
22-65	2841-2842	#	_	_	
22-66	2842-2843	#	_	_	
22-67	2843-2844	#	_	_	
22-68	2845-2858	Preliminaries	_	_	
22-69	2860-2862	To	_	_	
22-70	2863-2872	reproduce	_	_	
22-71	2873-2876	our	_	_	
22-72	2877-2884	results	_	_	
22-73	2885-2886	(	_	_	
22-74	2886-2889	e.g	_	_	
22-75	2889-2890	.	_	_	
22-76	2890-2891	,	_	_	
22-77	2892-2897	using	_	_	
22-78	2898-2904	LLaMA2	*	SOFTWARE	
22-79	2904-2905	)	_	_	
22-80	2907-2908	1	_	_	
22-81	2908-2909	.	_	_	
22-82	2910-2913	get	_	_	
22-83	2914-2917	the	_	_	
22-84	2918-2919	[	_	_	
22-85	2919-2926	license	_	_	
22-86	2927-2929	to	_	_	
22-87	2930-2933	use	_	_	
22-88	2934-2939	LLaMA	*[4]	SOFTWARE[4]	
22-89	2939-2940	-	*[4]	SOFTWARE[4]	
22-90	2940-2941	2	*[4]	SOFTWARE[4]	
22-91	2941-2942	]	_	_	
22-92	2942-2943	(	_	_	
22-93	2943-2948	https	_	_	
22-94	2948-2949	:	_	_	
22-95	2949-2950	/	_	_	
22-96	2950-2951	/	_	_	
22-97	2951-2962	ai.meta.com	_	_	
22-98	2962-2963	/	_	_	
22-99	2963-2968	llama	*	SOFTWARE	
22-100	2968-2969	/	_	_	
22-101	2969-2970	)	_	_	
22-102	2970-2971	.	_	_	
22-103	2973-2974	2	_	_	
22-104	2974-2975	.	_	_	
22-105	2976-2979	get	_	_	
22-106	2980-2986	access	_	_	
22-107	2987-2989	to	_	_	
22-108	2990-2993	the	_	_	
22-109	2994-3002	datasets	_	_	
22-110	3002-3003	:	_	_	
22-111	3004-3005	[	_	_	
22-112	3005-3010	SVAMP	*	DATASET	
22-113	3010-3011	]	_	_	
22-114	3011-3012	(	_	_	
22-115	3012-3017	https	_	_	
22-116	3017-3018	:	_	_	
22-117	3018-3019	/	_	_	
22-118	3019-3020	/	_	_	
22-119	3020-3030	github.com	_	_	
22-120	3030-3031	/	_	_	
22-121	3031-3041	arkilpatel	_	_	
22-122	3041-3042	/	_	_	
22-123	3042-3047	SVAMP	*	DATASET	
22-124	3047-3048	)	_	_	
22-125	3048-3049	,	_	_	
22-126	3050-3051	[	_	_	
22-127	3051-3056	GSM8K	*	DATASET	
22-128	3056-3057	]	_	_	
22-129	3057-3058	(	_	_	
22-130	3058-3063	https	_	_	
22-131	3063-3064	:	_	_	
22-132	3064-3065	/	_	_	
22-133	3065-3066	/	_	_	
22-134	3066-3080	huggingface.co	_	_	
22-135	3080-3081	/	_	_	
22-136	3081-3089	datasets	_	_	
22-137	3089-3090	/	_	_	
22-138	3090-3095	gsm8k	*	DATASET	
22-139	3095-3096	)	_	_	
22-140	3096-3097	,	_	_	
22-141	3098-3099	[	_	_	
22-142	3099-3104	MAWPS	*	DATASET	
22-143	3104-3105	]	_	_	
22-144	3105-3106	(	_	_	
22-145	3106-3111	https	_	_	
22-146	3111-3112	:	_	_	
22-147	3112-3113	/	_	_	
22-148	3113-3114	/	_	_	
22-149	3114-3124	github.com	_	_	
22-150	3124-3125	/	_	_	
22-151	3125-3130	sroy9	_	_	
22-152	3130-3131	/	_	_	
22-153	3131-3136	mawps	*	DATASET	
22-154	3136-3137	)	_	_	
22-155	3139-3140	#	_	_	
22-156	3140-3141	#	_	_	
22-157	3141-3142	#	_	_	
22-158	3143-3149	Create	_	_	
22-159	3150-3151	a	_	_	
22-160	3152-3159	Virtual	_	_	
22-161	3160-3163	Env	_	_	
22-162	3164-3165	1	_	_	
22-163	3165-3166	.	_	_	

#Text=Clone the repository
#Text=```
#Text=git clone git@github.com:holarissun/Prompt-OIRL.git
#Text=```
#Text=2.
23-1	3167-3172	Clone	_	_	
23-2	3173-3176	the	_	_	
23-3	3177-3187	repository	_	_	
23-4	3188-3189	`	_	_	
23-5	3189-3190	`	_	_	
23-6	3190-3191	`	_	_	
23-7	3192-3195	git	*	SOFTWARE	
23-8	3196-3201	clone	_	_	
23-9	3202-3205	git	*	SOFTWARE	
23-10	3205-3206	@	_	_	
23-11	3206-3216	github.com	_	_	
23-12	3216-3217	:	_	_	
23-13	3217-3227	holarissun	_	_	
23-14	3227-3228	/	_	_	
23-15	3228-3243	Prompt-OIRL.git	_	_	
23-16	3244-3245	`	_	_	
23-17	3245-3246	`	_	_	
23-18	3246-3247	`	_	_	
23-19	3248-3249	2	_	_	
23-20	3249-3250	.	_	_	

#Text=Create a new virtual environment with Python 3.10, e.g.,
#Text=```
#Text=conda create --name prompt-oirl python==3.10
#Text=conda activate prompt-oirl
#Text=cd Prompt-OIRL
#Text=```
#Text=3.
24-1	3251-3257	Create	_	_	
24-2	3258-3259	a	_	_	
24-3	3260-3263	new	_	_	
24-4	3264-3271	virtual	_	_	
24-5	3272-3283	environment	_	_	
24-6	3284-3288	with	_	_	
24-7	3289-3295	Python	*[5]	PROGLANG[5]	
24-8	3296-3300	3.10	*[5]	PROGLANG[5]	
24-9	3300-3301	,	_	_	
24-10	3302-3305	e.g	_	_	
24-11	3305-3306	.	_	_	
24-12	3306-3307	,	_	_	
24-13	3308-3309	`	_	_	
24-14	3309-3310	`	_	_	
24-15	3310-3311	`	_	_	
24-16	3312-3317	conda	*	SOFTWARE	
24-17	3318-3324	create	_	_	
24-18	3325-3326	-	_	_	
24-19	3326-3327	-	_	_	
24-20	3327-3331	name	_	_	
24-21	3332-3343	prompt-oirl	_	_	
24-22	3344-3350	python	*[6]	PROGLANG[6]	
24-23	3350-3351	=	*[6]	PROGLANG[6]	
24-24	3351-3352	=	*[6]	PROGLANG[6]	
24-25	3352-3356	3.10	*[6]	PROGLANG[6]	
24-26	3357-3362	conda	*	SOFTWARE	
24-27	3363-3371	activate	_	_	
24-28	3372-3383	prompt-oirl	_	_	
24-29	3384-3386	cd	_	_	
24-30	3387-3398	Prompt-OIRL	*	SOFTWARE	
24-31	3399-3400	`	_	_	
24-32	3400-3401	`	_	_	
24-33	3401-3402	`	_	_	
24-34	3403-3404	3	_	_	
24-35	3404-3405	.	_	_	

#Text=Install the requirements
#Text=```
#Text=pip install -r requirements.txt
#Text=```
#Text=
#Text=### Reproduce the Main Results
#Text= #### Step 1.
25-1	3406-3413	Install	_	_	
25-2	3414-3417	the	_	_	
25-3	3418-3430	requirements	_	_	
25-4	3431-3432	`	_	_	
25-5	3432-3433	`	_	_	
25-6	3433-3434	`	_	_	
25-7	3435-3438	pip	*	SOFTWARE	
25-8	3439-3446	install	_	_	
25-9	3447-3448	-	_	_	
25-10	3448-3449	r	_	_	
25-11	3450-3466	requirements.txt	_	_	
25-12	3467-3468	`	_	_	
25-13	3468-3469	`	_	_	
25-14	3469-3470	`	_	_	
25-15	3472-3473	#	_	_	
25-16	3473-3474	#	_	_	
25-17	3474-3475	#	_	_	
25-18	3476-3485	Reproduce	_	_	
25-19	3486-3489	the	_	_	
25-20	3490-3494	Main	_	_	
25-21	3495-3502	Results	_	_	
25-22	3504-3505	#	_	_	
25-23	3505-3506	#	_	_	
25-24	3506-3507	#	_	_	
25-25	3507-3508	#	_	_	
25-26	3509-3513	Step	_	_	
25-27	3514-3515	1	_	_	
25-28	3515-3516	.	_	_	

#Text=(Optional, as we also released the offline dataset) Generate an offline dataset by interacting with the LLMs.
26-1	3517-3518	(	_	_	
26-2	3518-3526	Optional	_	_	
26-3	3526-3527	,	_	_	
26-4	3528-3530	as	_	_	
26-5	3531-3533	we	_	_	
26-6	3534-3538	also	_	_	
26-7	3539-3547	released	_	_	
26-8	3548-3551	the	_	_	
26-9	3552-3559	offline	_	_	
26-10	3560-3567	dataset	_	_	
26-11	3567-3568	)	_	_	
26-12	3569-3577	Generate	_	_	
26-13	3578-3580	an	_	_	
26-14	3581-3588	offline	_	_	
26-15	3589-3596	dataset	_	_	
26-16	3597-3599	by	_	_	
26-17	3600-3611	interacting	_	_	
26-18	3612-3616	with	_	_	
26-19	3617-3620	the	_	_	
26-20	3621-3625	LLMs	_	_	
26-21	3625-3626	.	_	_	

#Text=This step will take a long time --- typically a few days.
27-1	3628-3632	This	_	_	
27-2	3633-3637	step	_	_	
27-3	3638-3642	will	_	_	
27-4	3643-3647	take	_	_	
27-5	3648-3649	a	_	_	
27-6	3650-3654	long	_	_	
27-7	3655-3659	time	_	_	
27-8	3660-3661	-	_	_	
27-9	3661-3662	-	_	_	
27-10	3662-3663	-	_	_	
27-11	3664-3673	typically	_	_	
27-12	3674-3675	a	_	_	
27-13	3676-3679	few	_	_	
27-14	3680-3684	days	_	_	
27-15	3684-3685	.	_	_	

#Text=To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like GPT3.5 or TigerBot) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.
28-1	3686-3688	To	_	_	
28-2	3689-3694	avoid	_	_	
28-3	3695-3704	repeating	_	_	
28-4	3705-3709	such	_	_	
28-5	3710-3711	a	_	_	
28-6	3712-3727	computationally	_	_	
28-7	3728-3737	expensive	_	_	
28-8	3738-3739	(	_	_	
28-9	3739-3743	when	_	_	
28-10	3744-3751	running	_	_	
28-11	3752-3756	LLMs	_	_	
28-12	3757-3759	on	_	_	
28-13	3760-3765	local	_	_	
28-14	3766-3774	machines	_	_	
28-15	3774-3775	)	_	_	
28-16	3776-3778	or	_	_	
28-17	3779-3785	costly	_	_	
28-18	3786-3787	(	_	_	
28-19	3787-3791	when	_	_	
28-20	3792-3799	calling	_	_	
28-21	3800-3803	the	_	_	
28-22	3804-3814	commercial	_	_	
28-23	3815-3819	APIs	_	_	
28-24	3820-3824	like	_	_	
28-25	3825-3831	GPT3.5	*	SOFTWARE	
28-26	3832-3834	or	_	_	
28-27	3835-3843	TigerBot	*	SOFTWARE	
28-28	3843-3844	)	_	_	
28-29	3845-3852	process	_	_	
28-30	3852-3853	,	_	_	
28-31	3854-3856	we	_	_	
28-32	3857-3861	have	_	_	
28-33	3862-3863	*	_	_	
28-34	3863-3864	*	_	_	
28-35	3864-3872	released	_	_	
28-36	3873-3876	all	_	_	
28-37	3877-3880	the	_	_	
28-38	3881-3892	interactive	_	_	
28-39	3893-3897	logs	_	_	
28-40	3898-3902	with	_	_	
28-41	3903-3908	those	_	_	
28-42	3909-3913	LLMs	_	_	
28-43	3914-3923	collected	_	_	
28-44	3924-3926	in	_	_	
28-45	3927-3930	our	_	_	
28-46	3931-3942	experiments	_	_	
28-47	3942-3943	.	_	_	
28-48	3943-3944	*	_	_	
28-49	3944-3945	*	_	_	
28-50	3945-3946	.	_	_	

#Text=Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,
#Text=
#Text= If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:
#Text=
#Text= ```
#Text= git clone git@github.com:facebookresearch/llama.git
#Text= ```
#Text=and then move
#Text=```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py```
#Text=to the ```llama``` folder
#Text=
#Text=then run the following command
#Text=
#Text= ```
#Text=torchrun --nproc_per_node 1 llama_step1_gen_offline.py \\
#Text=    --ckpt_dir llama-2-7b-chat/ \\
#Text=    --tokenizer_path tokenizer.model \\
#Text=    --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k
#Text= ```
#Text= #### Step 2.
29-1	3947-3956	Therefore	_	_	
29-2	3956-3957	,	_	_	
29-3	3958-3960	if	_	_	
29-4	3961-3964	you	_	_	
29-5	3965-3968	are	_	_	
29-6	3969-3973	just	_	_	
29-7	3974-3981	looking	_	_	
29-8	3982-3984	to	_	_	
29-9	3985-3992	explore	_	_	
29-10	3993-3996	the	_	_	
29-11	3997-4003	method	_	_	
29-12	4004-4007	and	_	_	
29-13	4008-4013	don't	_	_	
29-14	4014-4018	need	_	_	
29-15	4019-4021	to	_	_	
29-16	4022-4031	re-create	_	_	
29-17	4032-4042	everything	_	_	
29-18	4043-4047	from	_	_	
29-19	4048-4055	scratch	_	_	
29-20	4055-4056	,	_	_	
29-21	4057-4059	we	_	_	
29-22	4060-4069	recommend	_	_	
29-23	4070-4074	that	_	_	
29-24	4075-4078	you	_	_	
29-25	4079-4083	skip	_	_	
29-26	4084-4088	this	_	_	
29-27	4089-4093	step	_	_	
29-28	4093-4094	,	_	_	
29-29	4097-4099	If	_	_	
29-30	4100-4103	you	_	_	
29-31	4104-4109	would	_	_	
29-32	4110-4114	like	_	_	
29-33	4115-4117	to	_	_	
29-34	4118-4127	reproduce	_	_	
29-35	4128-4131	the	_	_	
29-36	4132-4139	offline	_	_	
29-37	4140-4147	dataset	_	_	
29-38	4148-4152	with	_	_	
29-39	4153-4156	the	_	_	
29-40	4157-4163	llama2	*	SOFTWARE	
29-41	4164-4169	model	_	_	
29-42	4169-4170	,	_	_	
29-43	4171-4174	you	_	_	
29-44	4175-4179	need	_	_	
29-45	4180-4182	to	_	_	
29-46	4183-4189	follow	_	_	
29-47	4190-4195	these	_	_	
29-48	4196-4201	steps	_	_	
29-49	4201-4202	:	_	_	
29-50	4205-4206	`	_	_	
29-51	4206-4207	`	_	_	
29-52	4207-4208	`	_	_	
29-53	4210-4213	git	*	SOFTWARE	
29-54	4214-4219	clone	_	_	
29-55	4220-4223	git	_	_	
29-56	4223-4224	@	_	_	
29-57	4224-4234	github.com	_	_	
29-58	4234-4235	:	_	_	
29-59	4235-4251	facebookresearch	_	_	
29-60	4251-4252	/	_	_	
29-61	4252-4261	llama.git	_	_	
29-62	4263-4264	`	_	_	
29-63	4264-4265	`	_	_	
29-64	4265-4266	`	_	_	
29-65	4267-4270	and	_	_	
29-66	4271-4275	then	_	_	
29-67	4276-4280	move	_	_	
29-68	4281-4282	`	_	_	
29-69	4282-4283	`	_	_	
29-70	4283-4284	`	_	_	
29-71	4284-4295	Prompt-OIRL	_	_	
29-72	4295-4296	/	_	_	
29-73	4296-4306	llama_exps	_	_	
29-74	4306-4307	/	_	_	
29-75	4307-4318	llama_step1	_	_	
29-76	4318-4319	_	_	_	
29-77	4319-4333	gen_offline.py	_	_	
29-78	4333-4334	`	_	_	
29-79	4334-4335	`	_	_	
29-80	4335-4336	`	_	_	
29-81	4337-4339	to	_	_	
29-82	4340-4343	the	_	_	
29-83	4344-4345	`	_	_	
29-84	4345-4346	`	_	_	
29-85	4346-4347	`	_	_	
29-86	4347-4352	llama	_	_	
29-87	4352-4353	`	_	_	
29-88	4353-4354	`	_	_	
29-89	4354-4355	`	_	_	
29-90	4356-4362	folder	_	_	
29-91	4364-4368	then	_	_	
29-92	4369-4372	run	_	_	
29-93	4373-4376	the	_	_	
29-94	4377-4386	following	_	_	
29-95	4387-4394	command	_	_	
29-96	4397-4398	`	_	_	
29-97	4398-4399	`	_	_	
29-98	4399-4400	`	_	_	
29-99	4401-4409	torchrun	*	SOFTWARE	
29-100	4410-4411	-	_	_	
29-101	4411-4412	-	_	_	
29-102	4412-4426	nproc_per_node	_	_	
29-103	4427-4428	1	_	_	
29-104	4429-4440	llama_step1	_	_	
29-105	4440-4441	_	_	_	
29-106	4441-4455	gen_offline.py	_	_	
29-107	4456-4457	\	_	_	
29-108	4462-4463	-	_	_	
29-109	4463-4464	-	_	_	
29-110	4464-4472	ckpt_dir	_	_	
29-111	4473-4478	llama	_	_	
29-112	4478-4479	-	_	_	
29-113	4479-4480	2	_	_	
29-114	4480-4481	-	_	_	
29-115	4481-4488	7b-chat	_	_	
29-116	4488-4489	/	_	_	
29-117	4490-4491	\	_	_	
29-118	4496-4497	-	_	_	
29-119	4497-4498	-	_	_	
29-120	4498-4512	tokenizer_path	_	_	
29-121	4513-4528	tokenizer.model	_	_	
29-122	4529-4530	\	_	_	
29-123	4535-4536	-	_	_	
29-124	4536-4537	-	_	_	
29-125	4537-4548	max_seq_len	_	_	
29-126	4549-4552	512	_	_	
29-127	4553-4554	-	_	_	
29-128	4554-4555	-	_	_	
29-129	4555-4569	max_batch_size	_	_	
29-130	4570-4571	8	_	_	
29-131	4572-4573	-	_	_	
29-132	4573-4574	-	_	_	
29-133	4574-4584	prompt_idx	_	_	
29-134	4585-4586	0	_	_	
29-135	4587-4588	-	_	_	
29-136	4588-4589	-	_	_	
29-137	4589-4601	dataset_eval	_	_	
29-138	4602-4607	gsm8k	*	DATASET	
29-139	4609-4610	`	_	_	
29-140	4610-4611	`	_	_	
29-141	4611-4612	`	_	_	
29-142	4614-4615	#	_	_	
29-143	4615-4616	#	_	_	
29-144	4616-4617	#	_	_	
29-145	4617-4618	#	_	_	
29-146	4619-4623	Step	_	_	
29-147	4624-4625	2	_	_	
29-148	4625-4626	.	_	_	

#Text=Reorganize the collected offline data
#Text= For the easiest way to run the experiment it is recommended that you start with this step.
30-1	4627-4637	Reorganize	_	_	
30-2	4638-4641	the	_	_	
30-3	4642-4651	collected	_	_	
30-4	4652-4659	offline	_	_	
30-5	4660-4664	data	_	_	
30-6	4666-4669	For	_	_	
30-7	4670-4673	the	_	_	
30-8	4674-4681	easiest	_	_	
30-9	4682-4685	way	_	_	
30-10	4686-4688	to	_	_	
30-11	4689-4692	run	_	_	
30-12	4693-4696	the	_	_	
30-13	4697-4707	experiment	_	_	
30-14	4708-4710	it	_	_	
30-15	4711-4713	is	_	_	
30-16	4714-4725	recommended	_	_	
30-17	4726-4730	that	_	_	
30-18	4731-4734	you	_	_	
30-19	4735-4740	start	_	_	
30-20	4741-4745	with	_	_	
30-21	4746-4750	this	_	_	
30-22	4751-4755	step	_	_	
30-23	4755-4756	.	_	_	

#Text=This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```LMllama2```
#Text=
#Text= ```
#Text= python3 llama_step2_reorg_data.py
#Text= ```
#Text=
#Text= #### Step 3.
31-1	4758-4762	This	_	_	
31-2	4763-4767	step	_	_	
31-3	4768-4772	will	_	_	
31-4	4773-4777	take	_	_	
31-5	4778-4779	a	_	_	
31-6	4780-4783	few	_	_	
31-7	4784-4791	seconds	_	_	
31-8	4792-4794	to	_	_	
31-9	4795-4801	finish	_	_	
31-10	4801-4802	,	_	_	
31-11	4803-4805	it	_	_	
31-12	4806-4810	will	_	_	
31-13	4811-4813	do	_	_	
31-14	4814-4818	some	_	_	
31-15	4819-4823	file	_	_	
31-16	4824-4832	renaming	_	_	
31-17	4833-4836	and	_	_	
31-18	4837-4850	training-test	_	_	
31-19	4851-4856	split	_	_	
31-20	4857-4860	and	_	_	
31-21	4861-4865	save	_	_	
31-22	4866-4879	corresponding	_	_	
31-23	4880-4885	files	_	_	
31-24	4886-4888	to	_	_	
31-25	4889-4890	a	_	_	
31-26	4891-4894	new	_	_	
31-27	4895-4901	folder	_	_	
31-28	4902-4903	`	_	_	
31-29	4903-4904	`	_	_	
31-30	4904-4905	`	_	_	
31-31	4905-4913	LMllama2	*	SOFTWARE	
31-32	4913-4914	`	_	_	
31-33	4914-4915	`	_	_	
31-34	4915-4916	`	_	_	
31-35	4919-4920	`	_	_	
31-36	4920-4921	`	_	_	
31-37	4921-4922	`	_	_	
31-38	4924-4931	python3	*	SOFTWARE	
31-39	4932-4943	llama_step2	_	_	
31-40	4943-4944	_	_	_	
31-41	4944-4957	reorg_data.py	_	_	
31-42	4959-4960	`	_	_	
31-43	4960-4961	`	_	_	
31-44	4961-4962	`	_	_	
31-45	4965-4966	#	_	_	
31-46	4966-4967	#	_	_	
31-47	4967-4968	#	_	_	
31-48	4968-4969	#	_	_	
31-49	4970-4974	Step	_	_	
31-50	4975-4976	3	_	_	
31-51	4976-4977	.	_	_	

#Text=Pre-process the offline data
#Text= This step will take a few seconds to finish, it will process the data and store embeddings and labels for different experiment settings (i.e., with different availability of training prompts) with ```.npy``` format files.
32-1	4978-4989	Pre-process	_	_	
32-2	4990-4993	the	_	_	
32-3	4994-5001	offline	_	_	
32-4	5002-5006	data	_	_	
32-5	5008-5012	This	_	_	
32-6	5013-5017	step	_	_	
32-7	5018-5022	will	_	_	
32-8	5023-5027	take	_	_	
32-9	5028-5029	a	_	_	
32-10	5030-5033	few	_	_	
32-11	5034-5041	seconds	_	_	
32-12	5042-5044	to	_	_	
32-13	5045-5051	finish	_	_	
32-14	5051-5052	,	_	_	
32-15	5053-5055	it	_	_	
32-16	5056-5060	will	_	_	
32-17	5061-5068	process	_	_	
32-18	5069-5072	the	_	_	
32-19	5073-5077	data	_	_	
32-20	5078-5081	and	_	_	
32-21	5082-5087	store	_	_	
32-22	5088-5098	embeddings	_	_	
32-23	5099-5102	and	_	_	
32-24	5103-5109	labels	_	_	
32-25	5110-5113	for	_	_	
32-26	5114-5123	different	_	_	
32-27	5124-5134	experiment	_	_	
32-28	5135-5143	settings	_	_	
32-29	5144-5145	(	_	_	
32-30	5145-5148	i.e	_	_	
32-31	5148-5149	.	_	_	
32-32	5149-5150	,	_	_	
32-33	5151-5155	with	_	_	
32-34	5156-5165	different	_	_	
32-35	5166-5178	availability	_	_	
32-36	5179-5181	of	_	_	
32-37	5182-5190	training	_	_	
32-38	5191-5198	prompts	_	_	
32-39	5198-5199	)	_	_	
32-40	5200-5204	with	_	_	
32-41	5205-5206	`	_	_	
32-42	5206-5207	`	_	_	
32-43	5207-5208	`	_	_	
32-44	5208-5209	.	_	_	
32-45	5209-5212	npy	_	_	
32-46	5212-5213	`	_	_	
32-47	5213-5214	`	_	_	
32-48	5214-5215	`	_	_	
32-49	5216-5222	format	_	_	
32-50	5223-5228	files	_	_	
32-51	5228-5229	.	_	_	

#Text=Note: please make sure that you select the relevant task by updating the code (the line marked with `NOTE`) in this and the following 2 steps.
#Text= ```
#Text= python3 llama_step3_data_processing.py
#Text= ```
#Text= #### Step 4.
33-1	5232-5236	Note	_	_	
33-2	5236-5237	:	_	_	
33-3	5238-5244	please	_	_	
33-4	5245-5249	make	_	_	
33-5	5250-5254	sure	_	_	
33-6	5255-5259	that	_	_	
33-7	5260-5263	you	_	_	
33-8	5264-5270	select	_	_	
33-9	5271-5274	the	_	_	
33-10	5275-5283	relevant	_	_	
33-11	5284-5288	task	_	_	
33-12	5289-5291	by	_	_	
33-13	5292-5300	updating	_	_	
33-14	5301-5304	the	_	_	
33-15	5305-5309	code	_	_	
33-16	5310-5311	(	_	_	
33-17	5311-5314	the	_	_	
33-18	5315-5319	line	_	_	
33-19	5320-5326	marked	_	_	
33-20	5327-5331	with	_	_	
33-21	5332-5333	`	_	_	
33-22	5333-5337	NOTE	_	_	
33-23	5337-5338	`	_	_	
33-24	5338-5339	)	_	_	
33-25	5340-5342	in	_	_	
33-26	5343-5347	this	_	_	
33-27	5348-5351	and	_	_	
33-28	5352-5355	the	_	_	
33-29	5356-5365	following	_	_	
33-30	5366-5367	2	_	_	
33-31	5368-5373	steps	_	_	
33-32	5373-5374	.	_	_	
33-33	5376-5377	`	_	_	
33-34	5377-5378	`	_	_	
33-35	5378-5379	`	_	_	
33-36	5381-5388	python3	*	SOFTWARE	
33-37	5389-5400	llama_step3	_	_	
33-38	5400-5401	_	_	_	
33-39	5401-5419	data_processing.py	_	_	
33-40	5421-5422	`	_	_	
33-41	5422-5423	`	_	_	
33-42	5423-5424	`	_	_	
33-43	5426-5427	#	_	_	
33-44	5427-5428	#	_	_	
33-45	5428-5429	#	_	_	
33-46	5429-5430	#	_	_	
33-47	5431-5435	Step	_	_	
33-48	5436-5437	4	_	_	
33-49	5437-5438	.	_	_	

#Text=Proxy Reward Model Learning (i.e., Offline Prompt Evaluation)
#Text= This step will take a few minutes to a few hours to finish, depending on the algorithms chosen and the processor.
34-1	5439-5444	Proxy	_	_	
34-2	5445-5451	Reward	_	_	
34-3	5452-5457	Model	_	_	
34-4	5458-5466	Learning	_	_	
34-5	5467-5468	(	_	_	
34-6	5468-5471	i.e	_	_	
34-7	5471-5472	.	_	_	
34-8	5472-5473	,	_	_	
34-9	5474-5481	Offline	_	_	
34-10	5482-5488	Prompt	_	_	
34-11	5489-5499	Evaluation	_	_	
34-12	5499-5500	)	_	_	
34-13	5502-5506	This	_	_	
34-14	5507-5511	step	_	_	
34-15	5512-5516	will	_	_	
34-16	5517-5521	take	_	_	
34-17	5522-5523	a	_	_	
34-18	5524-5527	few	_	_	
34-19	5528-5535	minutes	_	_	
34-20	5536-5538	to	_	_	
34-21	5539-5540	a	_	_	
34-22	5541-5544	few	_	_	
34-23	5545-5550	hours	_	_	
34-24	5551-5553	to	_	_	
34-25	5554-5560	finish	_	_	
34-26	5560-5561	,	_	_	
34-27	5562-5571	depending	_	_	
34-28	5572-5574	on	_	_	
34-29	5575-5578	the	_	_	
34-30	5579-5589	algorithms	_	_	
34-31	5590-5596	chosen	_	_	
34-32	5597-5600	and	_	_	
34-33	5601-5604	the	_	_	
34-34	5605-5614	processor	_	_	
34-35	5614-5615	.	_	_	

#Text=In general, training an XGBoost reward model will take a bit longer time, and using a LightGBM reward model can be faster.
#Text= ```
#Text= python3 llama_step4_offline_evaluation.py
#Text= ```
#Text=- Note: you will need to download a missing embedding file from [this link](https://drive.google.com/file/d/1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1/view?
35-1	5616-5618	In	_	_	
35-2	5619-5626	general	_	_	
35-3	5626-5627	,	_	_	
35-4	5628-5636	training	_	_	
35-5	5637-5639	an	_	_	
35-6	5640-5647	XGBoost	_	_	
35-7	5648-5654	reward	_	_	
35-8	5655-5660	model	_	_	
35-9	5661-5665	will	_	_	
35-10	5666-5670	take	_	_	
35-11	5671-5672	a	_	_	
35-12	5673-5676	bit	_	_	
35-13	5677-5683	longer	_	_	
35-14	5684-5688	time	_	_	
35-15	5688-5689	,	_	_	
35-16	5690-5693	and	_	_	
35-17	5694-5699	using	_	_	
35-18	5700-5701	a	_	_	
35-19	5702-5710	LightGBM	_	_	
35-20	5711-5717	reward	_	_	
35-21	5718-5723	model	_	_	
35-22	5724-5727	can	_	_	
35-23	5728-5730	be	_	_	
35-24	5731-5737	faster	_	_	
35-25	5737-5738	.	_	_	
35-26	5740-5741	`	_	_	
35-27	5741-5742	`	_	_	
35-28	5742-5743	`	_	_	
35-29	5745-5752	python3	*	SOFTWARE	
35-30	5753-5764	llama_step4	_	_	
35-31	5764-5765	_	_	_	
35-32	5765-5786	offline_evaluation.py	_	_	
35-33	5788-5789	`	_	_	
35-34	5789-5790	`	_	_	
35-35	5790-5791	`	_	_	
35-36	5792-5793	-	_	_	
35-37	5794-5798	Note	_	_	
35-38	5798-5799	:	_	_	
35-39	5800-5803	you	_	_	
35-40	5804-5808	will	_	_	
35-41	5809-5813	need	_	_	
35-42	5814-5816	to	_	_	
35-43	5817-5825	download	_	_	
35-44	5826-5827	a	_	_	
35-45	5828-5835	missing	_	_	
35-46	5836-5845	embedding	_	_	
35-47	5846-5850	file	_	_	
35-48	5851-5855	from	_	_	
35-49	5856-5857	[	_	_	
35-50	5857-5861	this	_	_	
35-51	5862-5866	link	_	_	
35-52	5866-5867	]	_	_	
35-53	5867-5868	(	_	_	
35-54	5868-5873	https	_	_	
35-55	5873-5874	:	_	_	
35-56	5874-5875	/	_	_	
35-57	5875-5876	/	_	_	
35-58	5876-5892	drive.google.com	_	_	
35-59	5892-5893	/	_	_	
35-60	5893-5897	file	_	_	
35-61	5897-5898	/	_	_	
35-62	5898-5899	d	_	_	
35-63	5899-5900	/	_	_	
35-64	5900-5933	1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1	_	_	
35-65	5933-5934	/	_	_	
35-66	5934-5938	view	_	_	
35-67	5938-5939	?	_	_	

#Text=usp=sharing) and place it in the `embeddings` directory to run this step.
36-1	5939-5942	usp	_	_	
36-2	5942-5943	=	_	_	
36-3	5943-5950	sharing	_	_	
36-4	5950-5951	)	_	_	
36-5	5952-5955	and	_	_	
36-6	5956-5961	place	_	_	
36-7	5962-5964	it	_	_	
36-8	5965-5967	in	_	_	
36-9	5968-5971	the	_	_	
36-10	5972-5973	`	_	_	
36-11	5973-5983	embeddings	_	_	
36-12	5983-5984	`	_	_	
36-13	5985-5994	directory	_	_	
36-14	5995-5997	to	_	_	
36-15	5998-6001	run	_	_	
36-16	6002-6006	this	_	_	
36-17	6007-6011	step	_	_	
36-18	6011-6012	.	_	_	

#Text=(oversized for Github, ~ 230Mb)
#Text=
#Text= #### Step 5.
37-1	6013-6014	(	_	_	
37-2	6014-6023	oversized	_	_	
37-3	6024-6027	for	_	_	
37-4	6028-6034	Github	*	SOFTWARE	
37-5	6034-6035	,	_	_	
37-6	6036-6037	~	_	_	
37-7	6038-6043	230Mb	_	_	
37-8	6043-6044	)	_	_	
37-9	6047-6048	#	_	_	
37-10	6048-6049	#	_	_	
37-11	6049-6050	#	_	_	
37-12	6050-6051	#	_	_	
37-13	6052-6056	Step	_	_	
37-14	6057-6058	5	_	_	
37-15	6058-6059	.	_	_	

#Text=(Offline) Prompt Optimization
#Text= This step will take a few minutes to finish.
38-1	6060-6061	(	_	_	
38-2	6061-6068	Offline	_	_	
38-3	6068-6069	)	_	_	
38-4	6070-6076	Prompt	_	_	
38-5	6077-6089	Optimization	_	_	
38-6	6091-6095	This	_	_	
38-7	6096-6100	step	_	_	
38-8	6101-6105	will	_	_	
38-9	6106-6110	take	_	_	
38-10	6111-6112	a	_	_	
38-11	6113-6116	few	_	_	
38-12	6117-6124	minutes	_	_	
38-13	6125-6127	to	_	_	
38-14	6128-6134	finish	_	_	
38-15	6134-6135	.	_	_	

#Text=Evaluating the algorithms by interacting with the LLMs can also be an option but could be slower.
39-1	6136-6146	Evaluating	_	_	
39-2	6147-6150	the	_	_	
39-3	6151-6161	algorithms	_	_	
39-4	6162-6164	by	_	_	
39-5	6165-6176	interacting	_	_	
39-6	6177-6181	with	_	_	
39-7	6182-6185	the	_	_	
39-8	6186-6190	LLMs	_	_	
39-9	6191-6194	can	_	_	
39-10	6195-6199	also	_	_	
39-11	6200-6202	be	_	_	
39-12	6203-6205	an	_	_	
39-13	6206-6212	option	_	_	
39-14	6213-6216	but	_	_	
39-15	6217-6222	could	_	_	
39-16	6223-6225	be	_	_	
39-17	6226-6232	slower	_	_	
39-18	6232-6233	.	_	_	

#Text=Results under different settings will be all saved to ```.csv``` files
#Text= ```
#Text= python3 llama_step5_offline_optimization.py
#Text= ```
#Text=
#Text=
#Text=
#Text=## üöÄ A Related Discussion on RLHF:
#Text=Prompt-OIRL addresses the prompting problems in LLMs using an RLAIF approach.
40-1	6234-6241	Results	_	_	
40-2	6242-6247	under	_	_	
40-3	6248-6257	different	_	_	
40-4	6258-6266	settings	_	_	
40-5	6267-6271	will	_	_	
40-6	6272-6274	be	_	_	
40-7	6275-6278	all	_	_	
40-8	6279-6284	saved	_	_	
40-9	6285-6287	to	_	_	
40-10	6288-6289	`	_	_	
40-11	6289-6290	`	_	_	
40-12	6290-6291	`	_	_	
40-13	6291-6292	.	_	_	
40-14	6292-6295	csv	_	_	
40-15	6295-6296	`	_	_	
40-16	6296-6297	`	_	_	
40-17	6297-6298	`	_	_	
40-18	6299-6304	files	_	_	
40-19	6306-6307	`	_	_	
40-20	6307-6308	`	_	_	
40-21	6308-6309	`	_	_	
40-22	6311-6318	python3	*	SOFTWARE	
40-23	6319-6330	llama_step5	_	_	
40-24	6330-6331	_	_	_	
40-25	6331-6354	offline_optimization.py	_	_	
40-26	6356-6357	`	_	_	
40-27	6357-6358	`	_	_	
40-28	6358-6359	`	_	_	
40-29	6363-6364	#	_	_	
40-30	6364-6365	#	_	_	
40-31	6366-6368	üöÄ	_	_	
40-32	6369-6370	A	_	_	
40-33	6371-6378	Related	_	_	
40-34	6379-6389	Discussion	_	_	
40-35	6390-6392	on	_	_	
40-36	6393-6397	RLHF	_	_	
40-37	6397-6398	:	_	_	
40-38	6399-6410	Prompt-OIRL	*	SOFTWARE	
40-39	6411-6420	addresses	_	_	
40-40	6421-6424	the	_	_	
40-41	6425-6434	prompting	_	_	
40-42	6435-6443	problems	_	_	
40-43	6444-6446	in	_	_	
40-44	6447-6451	LLMs	_	_	
40-45	6452-6457	using	_	_	
40-46	6458-6460	an	_	_	
40-47	6461-6466	RLAIF	_	_	
40-48	6467-6475	approach	_	_	
40-49	6475-6476	.	_	_	

#Text=For readers who are also interested in RLHF and RLAIF, and in the intersection between RL and LLM research, we would refer to our related positioning paper discussing RL in LLM research:
#Text=[RL in the Era of LLMs: What is Essential?
41-1	6477-6480	For	_	_	
41-2	6481-6488	readers	_	_	
41-3	6489-6492	who	_	_	
41-4	6493-6496	are	_	_	
41-5	6497-6501	also	_	_	
41-6	6502-6512	interested	_	_	
41-7	6513-6515	in	_	_	
41-8	6516-6520	RLHF	_	_	
41-9	6521-6524	and	_	_	
41-10	6525-6530	RLAIF	_	_	
41-11	6530-6531	,	_	_	
41-12	6532-6535	and	_	_	
41-13	6536-6538	in	_	_	
41-14	6539-6542	the	_	_	
41-15	6543-6555	intersection	_	_	
41-16	6556-6563	between	_	_	
41-17	6564-6566	RL	_	_	
41-18	6567-6570	and	_	_	
41-19	6571-6574	LLM	_	_	
41-20	6575-6583	research	_	_	
41-21	6583-6584	,	_	_	
41-22	6585-6587	we	_	_	
41-23	6588-6593	would	_	_	
41-24	6594-6599	refer	_	_	
41-25	6600-6602	to	_	_	
41-26	6603-6606	our	_	_	
41-27	6607-6614	related	_	_	
41-28	6615-6626	positioning	_	_	
41-29	6627-6632	paper	_	_	
41-30	6633-6643	discussing	_	_	
41-31	6644-6646	RL	_	_	
41-32	6647-6649	in	_	_	
41-33	6650-6653	LLM	_	_	
41-34	6654-6662	research	_	_	
41-35	6662-6663	:	_	_	
41-36	6664-6665	[	_	_	
41-37	6665-6667	RL	*[7]	PUBLICATION[7]	
41-38	6668-6670	in	*[7]	PUBLICATION[7]	
41-39	6671-6674	the	*[7]	PUBLICATION[7]	
41-40	6675-6678	Era	*[7]	PUBLICATION[7]	
41-41	6679-6681	of	*[7]	PUBLICATION[7]	
41-42	6682-6686	LLMs	*[7]	PUBLICATION[7]	
41-43	6686-6687	:	*[7]	PUBLICATION[7]	
41-44	6688-6692	What	*[7]	PUBLICATION[7]	
41-45	6693-6695	is	*[7]	PUBLICATION[7]	
41-46	6696-6705	Essential	*[7]	PUBLICATION[7]	
41-47	6705-6706	?	*[7]	PUBLICATION[7]	

#Text=What is Needed?
42-1	6707-6711	What	*[7]	PUBLICATION[7]	
42-2	6712-6714	is	*[7]	PUBLICATION[7]	
42-3	6715-6721	Needed	*[7]	PUBLICATION[7]	
42-4	6721-6722	?	*[7]	PUBLICATION[7]	

#Text=RLHF, Prompting, and Beyond.]
43-1	6723-6727	RLHF	*[7]	PUBLICATION[7]	
43-2	6727-6728	,	*[7]	PUBLICATION[7]	
43-3	6729-6738	Prompting	*[7]	PUBLICATION[7]	
43-4	6738-6739	,	*[7]	PUBLICATION[7]	
43-5	6740-6743	and	*[7]	PUBLICATION[7]	
43-6	6744-6750	Beyond	*[7]	PUBLICATION[7]	
43-7	6750-6751	.	_	_	
43-8	6751-6752	]	_	_	

#Text=(https://arxiv.org/pdf/2310.06147.pdf)
#Text=
#Text=
#Text=
#Text=
#Text=## üìö BibTex Citation
#Text=If you would like to cite our code or paper, please use
#Text=
#Text=```
#Text=@inproceedings{sun2023query,
#Text=  title={Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL},
#Text=  author={Sun, Hao and H{\\"u}y{\\"u}k, Alihan and van der Schaar, Mihaela},
#Text=  booktitle={The Twelfth International Conference on Learning Representations},
#Text=  year={2024}
#Text=}
#Text=
#Text=
#Text=@article{sun2023reinforcement,
#Text=  title={Reinforcement Learning in the Era of LLMs: What is Essential?
44-1	6752-6753	(	_	_	
44-2	6753-6758	https	_	_	
44-3	6758-6759	:	_	_	
44-4	6759-6760	/	_	_	
44-5	6760-6761	/	_	_	
44-6	6761-6770	arxiv.org	_	_	
44-7	6770-6771	/	_	_	
44-8	6771-6774	pdf	_	_	
44-9	6774-6775	/	_	_	
44-10	6775-6785	2310.06147	_	_	
44-11	6785-6786	.	_	_	
44-12	6786-6789	pdf	_	_	
44-13	6789-6790	)	_	_	
44-14	6795-6796	#	_	_	
44-15	6796-6797	#	_	_	
44-16	6798-6800	üìö	_	_	
44-17	6801-6807	BibTex	_	_	
44-18	6808-6816	Citation	_	_	
44-19	6817-6819	If	_	_	
44-20	6820-6823	you	_	_	
44-21	6824-6829	would	_	_	
44-22	6830-6834	like	_	_	
44-23	6835-6837	to	_	_	
44-24	6838-6842	cite	_	_	
44-25	6843-6846	our	_	_	
44-26	6847-6851	code	_	_	
44-27	6852-6854	or	_	_	
44-28	6855-6860	paper	_	_	
44-29	6860-6861	,	_	_	
44-30	6862-6868	please	_	_	
44-31	6869-6872	use	_	_	
44-32	6874-6875	`	_	_	
44-33	6875-6876	`	_	_	
44-34	6876-6877	`	_	_	
44-35	6878-6879	@	_	_	
44-36	6879-6892	inproceedings	_	_	
44-37	6892-6893	{	_	_	
44-38	6893-6905	sun2023query	_	_	
44-39	6905-6906	,	_	_	
44-40	6909-6914	title	_	_	
44-41	6914-6915	=	_	_	
44-42	6915-6916	{	_	_	
44-43	6916-6931	Query-Dependent	*[8]	PUBLICATION[8]	
44-44	6932-6938	Prompt	*[8]	PUBLICATION[8]	
44-45	6939-6949	Evaluation	*[8]	PUBLICATION[8]	
44-46	6950-6953	and	*[8]	PUBLICATION[8]	
44-47	6954-6966	Optimization	*[8]	PUBLICATION[8]	
44-48	6967-6971	with	*[8]	PUBLICATION[8]	
44-49	6972-6979	Offline	*[8]	PUBLICATION[8]	
44-50	6980-6987	Inverse	*[8]	PUBLICATION[8]	
44-51	6988-6990	RL	*[8]	PUBLICATION[8]	
44-52	6990-6991	}	_	_	
44-53	6991-6992	,	_	_	
44-54	6995-7001	author	_	_	
44-55	7001-7002	=	_	_	
44-56	7002-7003	{	_	_	
44-57	7003-7006	Sun	_	_	
44-58	7006-7007	,	_	_	
44-59	7008-7011	Hao	_	_	
44-60	7012-7015	and	_	_	
44-61	7016-7017	H	_	_	
44-62	7017-7018	{	_	_	
44-63	7018-7019	\	_	_	
44-64	7019-7020	"	_	_	
44-65	7020-7021	u	_	_	
44-66	7021-7022	}	_	_	
44-67	7022-7023	y	_	_	
44-68	7023-7024	{	_	_	
44-69	7024-7025	\	_	_	
44-70	7025-7026	"	_	_	
44-71	7026-7027	u	_	_	
44-72	7027-7028	}	_	_	
44-73	7028-7029	k	_	_	
44-74	7029-7030	,	_	_	
44-75	7031-7037	Alihan	_	_	
44-76	7038-7041	and	_	_	
44-77	7042-7045	van	_	_	
44-78	7046-7049	der	_	_	
44-79	7050-7056	Schaar	_	_	
44-80	7056-7057	,	_	_	
44-81	7058-7065	Mihaela	_	_	
44-82	7065-7066	}	_	_	
44-83	7066-7067	,	_	_	
44-84	7070-7079	booktitle	_	_	
44-85	7079-7080	=	_	_	
44-86	7080-7081	{	_	_	
44-87	7081-7084	The	_	_	
44-88	7085-7092	Twelfth	*[9]	CONFERENCE[9]	
44-89	7093-7106	International	*[9]	CONFERENCE[9]	
44-90	7107-7117	Conference	*[9]	CONFERENCE[9]	
44-91	7118-7120	on	*[9]	CONFERENCE[9]	
44-92	7121-7129	Learning	*[9]	CONFERENCE[9]	
44-93	7130-7145	Representations	*[9]	CONFERENCE[9]	
44-94	7145-7146	}	_	_	
44-95	7146-7147	,	_	_	
44-96	7150-7154	year	_	_	
44-97	7154-7155	=	_	_	
44-98	7155-7156	{	_	_	
44-99	7156-7160	2024	_	_	
44-100	7160-7161	}	_	_	
44-101	7162-7163	}	_	_	
44-102	7166-7167	@	_	_	
44-103	7167-7174	article	_	_	
44-104	7174-7175	{	_	_	
44-105	7175-7195	sun2023reinforcement	_	_	
44-106	7195-7196	,	_	_	
44-107	7199-7204	title	_	_	
44-108	7204-7205	=	_	_	
44-109	7205-7206	{	_	_	
44-110	7206-7219	Reinforcement	*[10]	PUBLICATION[10]	
44-111	7220-7228	Learning	*[10]	PUBLICATION[10]	
44-112	7229-7231	in	*[10]	PUBLICATION[10]	
44-113	7232-7235	the	*[10]	PUBLICATION[10]	
44-114	7236-7239	Era	*[10]	PUBLICATION[10]	
44-115	7240-7242	of	*[10]	PUBLICATION[10]	
44-116	7243-7247	LLMs	*[10]	PUBLICATION[10]	
44-117	7247-7248	:	*[10]	PUBLICATION[10]	
44-118	7249-7253	What	*[10]	PUBLICATION[10]	
44-119	7254-7256	is	*[10]	PUBLICATION[10]	
44-120	7257-7266	Essential	*[10]	PUBLICATION[10]	
44-121	7266-7267	?	*[10]	PUBLICATION[10]	

#Text=What is needed?
45-1	7268-7272	What	*[10]	PUBLICATION[10]	
45-2	7273-7275	is	*[10]	PUBLICATION[10]	
45-3	7276-7282	needed	*[10]	PUBLICATION[10]	
45-4	7282-7283	?	*[10]	PUBLICATION[10]	

#Text=An RL Perspective on RLHF, Prompting, and Beyond},
#Text=  author={Sun, Hao},
#Text=  journal={arXiv preprint arXiv:2310.06147},
#Text=  year={2023}
#Text=}
46-1	7284-7286	An	*[10]	PUBLICATION[10]	
46-2	7287-7289	RL	*[10]	PUBLICATION[10]	
46-3	7290-7301	Perspective	*[10]	PUBLICATION[10]	
46-4	7302-7304	on	*[10]	PUBLICATION[10]	
46-5	7305-7309	RLHF	*[10]	PUBLICATION[10]	
46-6	7309-7310	,	*[10]	PUBLICATION[10]	
46-7	7311-7320	Prompting	*[10]	PUBLICATION[10]	
46-8	7320-7321	,	*[10]	PUBLICATION[10]	
46-9	7322-7325	and	*[10]	PUBLICATION[10]	
46-10	7326-7332	Beyond	*[10]	PUBLICATION[10]	
46-11	7332-7333	}	_	_	
46-12	7333-7334	,	_	_	
46-13	7337-7343	author	_	_	
46-14	7343-7344	=	_	_	
46-15	7344-7345	{	_	_	
46-16	7345-7348	Sun	_	_	
46-17	7348-7349	,	_	_	
46-18	7350-7353	Hao	_	_	
46-19	7353-7354	}	_	_	
46-20	7354-7355	,	_	_	
46-21	7358-7365	journal	_	_	
46-22	7365-7366	=	_	_	
46-23	7366-7367	{	_	_	
46-24	7367-7372	arXiv	_	_	
46-25	7373-7381	preprint	_	_	
46-26	7382-7387	arXiv	_	_	
46-27	7387-7388	:	_	_	
46-28	7388-7398	2310.06147	_	_	
46-29	7398-7399	}	_	_	
46-30	7399-7400	,	_	_	
46-31	7403-7407	year	_	_	
46-32	7407-7408	=	_	_	
46-33	7408-7409	{	_	_	
46-34	7409-7413	2023	_	_	
46-35	7413-7414	}	_	_	
46-36	7415-7416	}	_	_	
