#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=### Aspire Repository accompanying paper for modeling fine grained similarity between documents:   \*\*Title\*\*: "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity"  \*\*Authors\*\*: Sheshera Mysore, Arman Cohan, Tom Hope  \*\*Abstract\*\*: We present a new scientific document similarity model based on matching fine-grained aspects of texts.
1-1	0-1	#	_	_
1-2	1-2	#	_	_
1-3	2-3	#	_	_
1-4	4-10	Aspire	_	_
1-5	11-21	Repository	_	_
1-6	22-34	accompanying	_	_
1-7	35-40	paper	_	_
1-8	41-44	for	_	_
1-9	45-53	modeling	_	_
1-10	54-58	fine	_	_
1-11	59-66	grained	_	_
1-12	67-77	similarity	_	_
1-13	78-85	between	_	_
1-14	86-95	documents	_	_
1-15	95-96	:	_	_
1-16	99-100	\*	_	_
1-17	100-101	\*	_	_
1-18	101-106	Title	_	_
1-19	106-107	\*	_	_
1-20	107-108	\*	_	_
1-21	108-109	:	_	_
1-22	110-111	"	_	_
1-23	111-123	Multi-Vector	_	_
1-24	124-130	Models	_	_
1-25	131-135	with	_	_
1-26	136-143	Textual	_	_
1-27	144-152	Guidance	_	_
1-28	153-156	for	_	_
1-29	157-169	Fine-Grained	_	_
1-30	170-180	Scientific	_	_
1-31	181-189	Document	_	_
1-32	190-200	Similarity	_	_
1-33	200-201	"	_	_
1-34	203-204	\*	_	_
1-35	204-205	\*	_	_
1-36	205-212	Authors	_	_
1-37	212-213	\*	_	_
1-38	213-214	\*	_	_
1-39	214-215	:	_	_
1-40	216-224	Sheshera	_	_
1-41	225-231	Mysore	_	_
1-42	231-232	,	_	_
1-43	233-238	Arman	_	_
1-44	239-244	Cohan	_	_
1-45	244-245	,	_	_
1-46	246-249	Tom	_	_
1-47	250-254	Hope	_	_
1-48	256-257	\*	_	_
1-49	257-258	\*	_	_
1-50	258-266	Abstract	_	_
1-51	266-267	\*	_	_
1-52	267-268	\*	_	_
1-53	268-269	:	_	_
1-54	270-272	We	_	_
1-55	273-280	present	_	_
1-56	281-282	a	_	_
1-57	283-286	new	_	_
1-58	287-297	scientific	_	_
1-59	298-306	document	_	_
1-60	307-317	similarity	_	_
1-61	318-323	model	_	_
1-62	324-329	based	_	_
1-63	330-332	on	_	_
1-64	333-341	matching	_	_
1-65	342-354	fine-grained	_	_
1-66	355-362	aspects	_	_
1-67	363-365	of	_	_
1-68	366-371	texts	_	_
1-69	371-372	.	_	_

#Text=To train our model, we exploit a naturally-occurring source of supervision: sentences in the full-text of papers that cite multiple papers together (co-citations).
2-1	373-375	To	_	_
2-2	376-381	train	_	_
2-3	382-385	our	_	_
2-4	386-391	model	_	_
2-5	391-392	,	_	_
2-6	393-395	we	_	_
2-7	396-403	exploit	_	_
2-8	404-405	a	_	_
2-9	406-425	naturally-occurring	_	_
2-10	426-432	source	_	_
2-11	433-435	of	_	_
2-12	436-447	supervision	_	_
2-13	447-448	:	_	_
2-14	449-458	sentences	_	_
2-15	459-461	in	_	_
2-16	462-465	the	_	_
2-17	466-475	full-text	_	_
2-18	476-478	of	_	_
2-19	479-485	papers	_	_
2-20	486-490	that	_	_
2-21	491-495	cite	_	_
2-22	496-504	multiple	_	_
2-23	505-511	papers	_	_
2-24	512-520	together	_	_
2-25	521-522	(	_	_
2-26	522-534	co-citations	_	_
2-27	534-535	)	_	_
2-28	535-536	.	_	_

#Text=Such co-citations not only reflect close paper relatedness, but also provide textual descriptions of how the co-cited papers are related.
3-1	537-541	Such	_	_
3-2	542-554	co-citations	_	_
3-3	555-558	not	_	_
3-4	559-563	only	_	_
3-5	564-571	reflect	_	_
3-6	572-577	close	_	_
3-7	578-583	paper	_	_
3-8	584-595	relatedness	_	_
3-9	595-596	,	_	_
3-10	597-600	but	_	_
3-11	601-605	also	_	_
3-12	606-613	provide	_	_
3-13	614-621	textual	_	_
3-14	622-634	descriptions	_	_
3-15	635-637	of	_	_
3-16	638-641	how	_	_
3-17	642-645	the	_	_
3-18	646-654	co-cited	_	_
3-19	655-661	papers	_	_
3-20	662-665	are	_	_
3-21	666-673	related	_	_
3-22	673-674	.	_	_

#Text=This novel form of textual supervision is used for learning to match aspects across papers.
4-1	675-679	This	_	_
4-2	680-685	novel	_	_
4-3	686-690	form	_	_
4-4	691-693	of	_	_
4-5	694-701	textual	_	_
4-6	702-713	supervision	_	_
4-7	714-716	is	_	_
4-8	717-721	used	_	_
4-9	722-725	for	_	_
4-10	726-734	learning	_	_
4-11	735-737	to	_	_
4-12	738-743	match	_	_
4-13	744-751	aspects	_	_
4-14	752-758	across	_	_
4-15	759-765	papers	_	_
4-16	765-766	.	_	_

#Text=We develop multi-vector representations where vectors correspond to sentence-level aspects of documents, and present two methods for aspect matching: (1) A fast method that only matches single aspects, and (2) a method that makes sparse multiple matches with an Optimal Transport mechanism that computes an Earth Mover's Distance between aspects.
5-1	767-769	We	_	_
5-2	770-777	develop	_	_
5-3	778-790	multi-vector	_	_
5-4	791-806	representations	_	_
5-5	807-812	where	_	_
5-6	813-820	vectors	_	_
5-7	821-831	correspond	_	_
5-8	832-834	to	_	_
5-9	835-849	sentence-level	_	_
5-10	850-857	aspects	_	_
5-11	858-860	of	_	_
5-12	861-870	documents	_	_
5-13	870-871	,	_	_
5-14	872-875	and	_	_
5-15	876-883	present	_	_
5-16	884-887	two	_	_
5-17	888-895	methods	_	_
5-18	896-899	for	_	_
5-19	900-906	aspect	_	_
5-20	907-915	matching	_	_
5-21	915-916	:	_	_
5-22	917-918	(	_	_
5-23	918-919	1	_	_
5-24	919-920	)	_	_
5-25	921-922	A	_	_
5-26	923-927	fast	_	_
5-27	928-934	method	_	_
5-28	935-939	that	_	_
5-29	940-944	only	_	_
5-30	945-952	matches	_	_
5-31	953-959	single	_	_
5-32	960-967	aspects	_	_
5-33	967-968	,	_	_
5-34	969-972	and	_	_
5-35	973-974	(	_	_
5-36	974-975	2	_	_
5-37	975-976	)	_	_
5-38	977-978	a	_	_
5-39	979-985	method	_	_
5-40	986-990	that	_	_
5-41	991-996	makes	_	_
5-42	997-1003	sparse	_	_
5-43	1004-1012	multiple	_	_
5-44	1013-1020	matches	_	_
5-45	1021-1025	with	_	_
5-46	1026-1028	an	_	_
5-47	1029-1036	Optimal	_	_
5-48	1037-1046	Transport	_	_
5-49	1047-1056	mechanism	_	_
5-50	1057-1061	that	_	_
5-51	1062-1070	computes	_	_
5-52	1071-1073	an	_	_
5-53	1074-1079	Earth	_	_
5-54	1080-1087	Mover's	_	_
5-55	1088-1096	Distance	_	_
5-56	1097-1104	between	_	_
5-57	1105-1112	aspects	_	_
5-58	1112-1113	.	_	_

#Text=Our approach improves performance on document similarity tasks in four datasets.
6-1	1114-1117	Our	_	_
6-2	1118-1126	approach	_	_
6-3	1127-1135	improves	_	_
6-4	1136-1147	performance	_	_
6-5	1148-1150	on	_	_
6-6	1151-1159	document	_	_
6-7	1160-1170	similarity	_	_
6-8	1171-1176	tasks	_	_
6-9	1177-1179	in	_	_
6-10	1180-1184	four	_	_
6-11	1185-1193	datasets	_	_
6-12	1193-1194	.	_	_

#Text=Further, our fast single-match method achieves competitive results, paving the way for applying fine-grained similarity to large scientific corpora.
7-1	1195-1202	Further	_	_
7-2	1202-1203	,	_	_
7-3	1204-1207	our	_	_
7-4	1208-1212	fast	_	_
7-5	1213-1225	single-match	_	_
7-6	1226-1232	method	_	_
7-7	1233-1241	achieves	_	_
7-8	1242-1253	competitive	_	_
7-9	1254-1261	results	_	_
7-10	1261-1262	,	_	_
7-11	1263-1269	paving	_	_
7-12	1270-1273	the	_	_
7-13	1274-1277	way	_	_
7-14	1278-1281	for	_	_
7-15	1282-1290	applying	_	_
7-16	1291-1303	fine-grained	_	_
7-17	1304-1314	similarity	_	_
7-18	1315-1317	to	_	_
7-19	1318-1323	large	_	_
7-20	1324-1334	scientific	_	_
7-21	1335-1342	corpora	_	_
7-22	1342-1343	.	_	_

#Text=The pre-print can be accessed here: https://arxiv.org/abs/2111.08366  \*\*NEWS:\*\* This work has been accepted to NAACL 2022, stay tuned for the camera-ready paper and additional artifacts.  ### Contents 1.
8-1	1346-1349	The	_	_
8-2	1350-1359	pre-print	_	_
8-3	1360-1363	can	_	_
8-4	1364-1366	be	_	_
8-5	1367-1375	accessed	_	_
8-6	1376-1380	here	_	_
8-7	1380-1381	:	_	_
8-8	1382-1387	https	_	_
8-9	1387-1388	:	_	_
8-10	1388-1389	/	_	_
8-11	1389-1390	/	_	_
8-12	1390-1399	arxiv.org	_	_
8-13	1399-1400	/	_	_
8-14	1400-1403	abs	_	_
8-15	1403-1404	/	_	_
8-16	1404-1414	2111.08366	_	_
8-17	1416-1417	\*	_	_
8-18	1417-1418	\*	_	_
8-19	1418-1422	NEWS	_	_
8-20	1422-1423	:	_	_
8-21	1423-1424	\*	_	_
8-22	1424-1425	\*	_	_
8-23	1426-1430	This	_	_
8-24	1431-1435	work	_	_
8-25	1436-1439	has	_	_
8-26	1440-1444	been	_	_
8-27	1445-1453	accepted	_	_
8-28	1454-1456	to	_	_
8-29	1457-1462	NAACL	_	_
8-30	1463-1467	2022	_	_
8-31	1467-1468	,	_	_
8-32	1469-1473	stay	_	_
8-33	1474-1479	tuned	_	_
8-34	1480-1483	for	_	_
8-35	1484-1487	the	_	_
8-36	1488-1500	camera-ready	_	_
8-37	1501-1506	paper	_	_
8-38	1507-1510	and	_	_
8-39	1511-1521	additional	_	_
8-40	1522-1531	artifacts	_	_
8-41	1531-1532	.	_	_
8-42	1534-1535	#	_	_
8-43	1535-1536	#	_	_
8-44	1536-1537	#	_	_
8-45	1538-1546	Contents	_	_
8-46	1547-1548	1	_	_
8-47	1548-1549	.	_	_

#Text=\[Artifacts\](#artifacts)     1.
9-1	1550-1551	\[	_	_
9-2	1551-1560	Artifacts	_	_
9-3	1560-1561	\]	_	_
9-4	1561-1562	(	_	_
9-5	1562-1563	#	_	_
9-6	1563-1572	artifacts	_	_
9-7	1572-1573	)	_	_
9-8	1578-1579	1	_	_
9-9	1579-1580	.	_	_

#Text=\[HF Models\](#models)     1.
10-1	1581-1582	\[	_	_
10-2	1582-1584	HF	_	_
10-3	1585-1591	Models	_	_
10-4	1591-1592	\]	_	_
10-5	1592-1593	(	_	_
10-6	1593-1594	#	_	_
10-7	1594-1600	models	_	_
10-8	1600-1601	)	_	_
10-9	1606-1607	1	_	_
10-10	1607-1608	.	_	_

#Text=\[Evaluation Datasets\](#evaldata) 1.
11-1	1609-1610	\[	_	_
11-2	1610-1620	Evaluation	_	_
11-3	1621-1629	Datasets	_	_
11-4	1629-1630	\]	_	_
11-5	1630-1631	(	_	_
11-6	1631-1632	#	_	_
11-7	1632-1640	evaldata	_	_
11-8	1640-1641	)	_	_
11-9	1642-1643	1	_	_
11-10	1643-1644	.	_	_

#Text=\[Model Usage Instructions\](#modelusage) 1.
12-1	1645-1646	\[	_	_
12-2	1646-1651	Model	_	_
12-3	1652-1657	Usage	_	_
12-4	1658-1670	Instructions	_	_
12-5	1670-1671	\]	_	_
12-6	1671-1672	(	_	_
12-7	1672-1673	#	_	_
12-8	1673-1683	modelusage	_	_
12-9	1683-1684	)	_	_
12-10	1685-1686	1	_	_
12-11	1686-1687	.	_	_

#Text=\[Repository Contents\](#repocontents) 1.
13-1	1688-1689	\[	_	_
13-2	1689-1699	Repository	_	_
13-3	1700-1708	Contents	_	_
13-4	1708-1709	\]	_	_
13-5	1709-1710	(	_	_
13-6	1710-1711	#	_	_
13-7	1711-1723	repocontents	_	_
13-8	1723-1724	)	_	_
13-9	1725-1726	1	_	_
13-10	1726-1727	.	_	_

#Text=\[Acknowledgements\](#acks) 1.
14-1	1728-1729	\[	_	_
14-2	1729-1745	Acknowledgements	_	_
14-3	1745-1746	\]	_	_
14-4	1746-1747	(	_	_
14-5	1747-1748	#	_	_
14-6	1748-1752	acks	_	_
14-7	1752-1753	)	_	_
14-8	1754-1755	1	_	_
14-9	1755-1756	.	_	_

#Text=\[Citation\](#citation) 1.
15-1	1757-1758	\[	_	_
15-2	1758-1766	Citation	_	_
15-3	1766-1767	\]	_	_
15-4	1767-1768	(	_	_
15-5	1768-1769	#	_	_
15-6	1769-1777	citation	_	_
15-7	1777-1778	)	_	_
15-8	1779-1780	1	_	_
15-9	1780-1781	.	_	_

#Text=\[TODOs\](#todos)   ### Artifacts <a name="artifacts"></a>  #### Models <a name="models"></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - \[`allenai/aspire-contextualsentence-multim-compsci`\](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - \[`allenai/aspire-contextualsentence-multim-biomed`\](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - \[`allenai/aspire-contextualsentence-singlem-compsci`\](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - \[`allenai/aspire-contextualsentence-singlem-biomed`\](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - \[`allenai/aspire-biencoder-compsci-spec`\](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - \[`allenai/aspire-biencoder-biomed-scib`\](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - \[`allenai/aspire-biencoder-biomed-spec`\](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - \[`allenai/aspire-sentence-embedder`\](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name="modelusage"></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex\_aspire\_consent import AspireConSent, prepare\_abstracts  # Initialize the tokenizer and model. hf\_model\_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire\_tok = AutoTokenizer.from\_pretrained(hf\_model\_name) aspire\_mv\_model = AspireConSent(hf\_model\_name)   # Example input. ex\_abstracts = \[     {'TITLE': "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific"               " Document Similarity",      'ABSTRACT': \["We present a new scientific document similarity model based on "                   "matching fine-grained aspects of texts.",                   "To train our model, we exploit a naturally-occurring source of "                   "supervision: sentences in the full-text of papers that cite multiple "                   "papers together (co-citations)."\]},     {'TITLE': "CSFCube -- A Test Collection of Computer Science Research Articles for "               "Faceted Query by Example",      'ABSTRACT': \["Query by Example is a well-known information retrieval task in which"                   " a document is chosen by the user as the search query and the goal is "                   "to retrieve relevant documents from a large collection.",                   "However, a document often covers multiple aspects of a topic.",                   "To address this scenario we introduce the task of faceted Query by "                   "Example in which users can also specify a finer grained aspect in "                   "addition to the input query document. "\]} \]  bert\_batch, abs\_lens, sent\_token\_idxs = prepare\_abstracts(batch\_abs=ex\_abstracts,                                                           pt\_lm\_tokenizer=aspire\_tok) clsreps, contextual\_sent\_reps = aspire\_mv\_model.forward(bert\_batch=bert\_batch,                                                         abs\_lens=abs\_lens,                                                         sent\_tok\_idxs=sent\_token\_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for \_multiple\_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.
16-1	1782-1783	\[	_	_
16-2	1783-1788	TODOs	_	_
16-3	1788-1789	\]	_	_
16-4	1789-1790	(	_	_
16-5	1790-1791	#	_	_
16-6	1791-1796	todos	_	_
16-7	1796-1797	)	_	_
16-8	1800-1801	#	_	_
16-9	1801-1802	#	_	_
16-10	1802-1803	#	_	_
16-11	1804-1813	Artifacts	_	_
16-12	1814-1815	<	_	_
16-13	1815-1816	a	_	_
16-14	1817-1821	name	_	_
16-15	1821-1822	=	_	_
16-16	1822-1823	"	_	_
16-17	1823-1832	artifacts	_	_
16-18	1832-1833	"	_	_
16-19	1833-1834	>	_	_
16-20	1834-1835	<	_	_
16-21	1835-1836	/	_	_
16-22	1836-1837	a	_	_
16-23	1837-1838	>	_	_
16-24	1840-1841	#	_	_
16-25	1841-1842	#	_	_
16-26	1842-1843	#	_	_
16-27	1843-1844	#	_	_
16-28	1845-1851	Models	_	_
16-29	1852-1853	<	_	_
16-30	1853-1854	a	_	_
16-31	1855-1859	name	_	_
16-32	1859-1860	=	_	_
16-33	1860-1861	"	_	_
16-34	1861-1867	models	_	_
16-35	1867-1868	"	_	_
16-36	1868-1869	>	_	_
16-37	1869-1870	<	_	_
16-38	1870-1871	/	_	_
16-39	1871-1872	a	_	_
16-40	1872-1873	>	_	_
16-41	1875-1881	Models	_	_
16-42	1882-1891	described	_	_
16-43	1892-1894	in	_	_
16-44	1895-1898	the	_	_
16-45	1899-1904	paper	_	_
16-46	1905-1908	are	_	_
16-47	1909-1917	released	_	_
16-48	1918-1920	as	_	_
16-49	1921-1928	Hugging	_	_
16-50	1929-1933	Face	_	_
16-51	1934-1940	models	_	_
16-52	1940-1941	:	_	_
16-53	1943-1944	`	_	_
16-54	1944-1952	otAspire	_	_
16-55	1952-1953	`	_	_
16-56	1953-1954	:	_	_
16-57	1956-1957	-	_	_
16-58	1958-1959	\[	_	_
16-59	1959-1960	`	_	_
16-60	1960-1967	allenai	_	_
16-61	1967-1968	/	_	_
16-62	1968-2008	aspire-contextualsentence-multim-compsci	_	_
16-63	2008-2009	`	_	_
16-64	2009-2010	\]	_	_
16-65	2010-2011	(	_	_
16-66	2011-2016	https	_	_
16-67	2016-2017	:	_	_
16-68	2017-2018	/	_	_
16-69	2018-2019	/	_	_
16-70	2019-2033	huggingface.co	_	_
16-71	2033-2034	/	_	_
16-72	2034-2041	allenai	_	_
16-73	2041-2042	/	_	_
16-74	2042-2082	aspire-contextualsentence-multim-compsci	_	_
16-75	2082-2083	)	_	_
16-76	2084-2085	-	_	_
16-77	2086-2087	\[	_	_
16-78	2087-2088	`	_	_
16-79	2088-2095	allenai	_	_
16-80	2095-2096	/	_	_
16-81	2096-2135	aspire-contextualsentence-multim-biomed	_	_
16-82	2135-2136	`	_	_
16-83	2136-2137	\]	_	_
16-84	2137-2138	(	_	_
16-85	2138-2143	https	_	_
16-86	2143-2144	:	_	_
16-87	2144-2145	/	_	_
16-88	2145-2146	/	_	_
16-89	2146-2160	huggingface.co	_	_
16-90	2160-2161	/	_	_
16-91	2161-2168	allenai	_	_
16-92	2168-2169	/	_	_
16-93	2169-2208	aspire-contextualsentence-multim-biomed	_	_
16-94	2208-2209	)	_	_
16-95	2211-2212	`	_	_
16-96	2212-2220	tsAspire	_	_
16-97	2220-2221	`	_	_
16-98	2221-2222	:	_	_
16-99	2225-2226	-	_	_
16-100	2227-2228	\[	_	_
16-101	2228-2229	`	_	_
16-102	2229-2236	allenai	_	_
16-103	2236-2237	/	_	_
16-104	2237-2278	aspire-contextualsentence-singlem-compsci	_	_
16-105	2278-2279	`	_	_
16-106	2279-2280	\]	_	_
16-107	2280-2281	(	_	_
16-108	2281-2286	https	_	_
16-109	2286-2287	:	_	_
16-110	2287-2288	/	_	_
16-111	2288-2289	/	_	_
16-112	2289-2303	huggingface.co	_	_
16-113	2303-2304	/	_	_
16-114	2304-2311	allenai	_	_
16-115	2311-2312	/	_	_
16-116	2312-2353	aspire-contextualsentence-singlem-compsci	_	_
16-117	2353-2354	)	_	_
16-118	2355-2356	-	_	_
16-119	2357-2358	\[	_	_
16-120	2358-2359	`	_	_
16-121	2359-2366	allenai	_	_
16-122	2366-2367	/	_	_
16-123	2367-2407	aspire-contextualsentence-singlem-biomed	_	_
16-124	2407-2408	`	_	_
16-125	2408-2409	\]	_	_
16-126	2409-2410	(	_	_
16-127	2410-2415	https	_	_
16-128	2415-2416	:	_	_
16-129	2416-2417	/	_	_
16-130	2417-2418	/	_	_
16-131	2418-2432	huggingface.co	_	_
16-132	2432-2433	/	_	_
16-133	2433-2440	allenai	_	_
16-134	2440-2441	/	_	_
16-135	2441-2481	aspire-contextualsentence-singlem-biomed	_	_
16-136	2481-2482	)	_	_
16-137	2485-2486	`	_	_
16-138	2486-2500	SPECTER-CoCite	_	_
16-139	2500-2501	`	_	_
16-140	2501-2502	:	_	_
16-141	2505-2506	-	_	_
16-142	2507-2508	\[	_	_
16-143	2508-2509	`	_	_
16-144	2509-2516	allenai	_	_
16-145	2516-2517	/	_	_
16-146	2517-2546	aspire-biencoder-compsci-spec	_	_
16-147	2546-2547	`	_	_
16-148	2547-2548	\]	_	_
16-149	2548-2549	(	_	_
16-150	2549-2554	https	_	_
16-151	2554-2555	:	_	_
16-152	2555-2556	/	_	_
16-153	2556-2557	/	_	_
16-154	2557-2571	huggingface.co	_	_
16-155	2571-2572	/	_	_
16-156	2572-2579	allenai	_	_
16-157	2579-2580	/	_	_
16-158	2580-2609	aspire-biencoder-compsci-spec	_	_
16-159	2609-2610	)	_	_
16-160	2611-2612	-	_	_
16-161	2613-2614	\[	_	_
16-162	2614-2615	`	_	_
16-163	2615-2622	allenai	_	_
16-164	2622-2623	/	_	_
16-165	2623-2651	aspire-biencoder-biomed-scib	_	_
16-166	2651-2652	`	_	_
16-167	2652-2653	\]	_	_
16-168	2653-2654	(	_	_
16-169	2654-2659	https	_	_
16-170	2659-2660	:	_	_
16-171	2660-2661	/	_	_
16-172	2661-2662	/	_	_
16-173	2662-2676	huggingface.co	_	_
16-174	2676-2677	/	_	_
16-175	2677-2684	allenai	_	_
16-176	2684-2685	/	_	_
16-177	2685-2713	aspire-biencoder-biomed-scib	_	_
16-178	2713-2714	)	_	_
16-179	2715-2716	-	_	_
16-180	2717-2718	\[	_	_
16-181	2718-2719	`	_	_
16-182	2719-2726	allenai	_	_
16-183	2726-2727	/	_	_
16-184	2727-2755	aspire-biencoder-biomed-spec	_	_
16-185	2755-2756	`	_	_
16-186	2756-2757	\]	_	_
16-187	2757-2758	(	_	_
16-188	2758-2763	https	_	_
16-189	2763-2764	:	_	_
16-190	2764-2765	/	_	_
16-191	2765-2766	/	_	_
16-192	2766-2780	huggingface.co	_	_
16-193	2780-2781	/	_	_
16-194	2781-2788	allenai	_	_
16-195	2788-2789	/	_	_
16-196	2789-2817	aspire-biencoder-biomed-spec	_	_
16-197	2817-2818	)	_	_
16-198	2820-2821	`	_	_
16-199	2821-2831	cosentbert	_	_
16-200	2831-2832	`	_	_
16-201	2832-2833	:	_	_
16-202	2836-2837	-	_	_
16-203	2838-2839	\[	_	_
16-204	2839-2840	`	_	_
16-205	2840-2847	allenai	_	_
16-206	2847-2848	/	_	_
16-207	2848-2872	aspire-sentence-embedder	_	_
16-208	2872-2873	`	_	_
16-209	2873-2874	\]	_	_
16-210	2874-2875	(	_	_
16-211	2875-2880	https	_	_
16-212	2880-2881	:	_	_
16-213	2881-2882	/	_	_
16-214	2882-2883	/	_	_
16-215	2883-2897	huggingface.co	_	_
16-216	2897-2898	/	_	_
16-217	2898-2905	allenai	_	_
16-218	2905-2906	/	_	_
16-219	2906-2930	aspire-sentence-embedder	_	_
16-220	2930-2931	)	_	_
16-221	2933-2934	#	_	_
16-222	2934-2935	#	_	_
16-223	2935-2936	#	_	_
16-224	2936-2937	#	_	_
16-225	2938-2943	Model	_	_
16-226	2944-2949	Usage	_	_
16-227	2950-2962	Instructions	_	_
16-228	2963-2964	<	_	_
16-229	2964-2965	a	_	_
16-230	2966-2970	name	_	_
16-231	2970-2971	=	_	_
16-232	2971-2972	"	_	_
16-233	2972-2982	modelusage	_	_
16-234	2982-2983	"	_	_
16-235	2983-2984	>	_	_
16-236	2984-2985	<	_	_
16-237	2985-2986	/	_	_
16-238	2986-2987	a	_	_
16-239	2987-2988	>	_	_
16-240	2990-2991	#	_	_
16-241	2991-2992	#	_	_
16-242	2992-2993	#	_	_
16-243	2993-2994	#	_	_
16-244	2994-2995	#	_	_
16-245	2996-2997	`	_	_
16-246	2997-3005	tsAspire	_	_
16-247	3005-3006	`	_	_
16-248	3008-3011	The	_	_
16-249	3012-3013	`	_	_
16-250	3013-3021	tsAspire	_	_
16-251	3021-3022	`	_	_
16-252	3023-3035	multi-vector	_	_
16-253	3036-3041	model	_	_
16-254	3042-3049	trained	_	_
16-255	3050-3053	for	_	_
16-256	3054-3060	single	_	_
16-257	3061-3068	matches	_	_
16-258	3069-3075	across	_	_
16-259	3076-3085	documents	_	_
16-260	3086-3089	can	_	_
16-261	3090-3092	be	_	_
16-262	3093-3097	used	_	_
16-263	3098-3101	via	_	_
16-264	3102-3105	the	_	_
16-265	3106-3107	`	_	_
16-266	3107-3119	transformers	_	_
16-267	3119-3120	`	_	_
16-268	3121-3128	library	_	_
16-269	3129-3132	and	_	_
16-270	3133-3137	some	_	_
16-271	3138-3148	additional	_	_
16-272	3149-3153	code	_	_
16-273	3154-3156	to	_	_
16-274	3157-3164	compute	_	_
16-275	3165-3175	contextual	_	_
16-276	3176-3184	sentence	_	_
16-277	3185-3192	vectors	_	_
16-278	3193-3195	as	_	_
16-279	3195-3196	:	_	_
16-280	3198-3199	`	_	_
16-281	3199-3200	`	_	_
16-282	3200-3201	`	_	_
16-283	3201-3207	python	_	_
16-284	3208-3212	from	_	_
16-285	3213-3225	transformers	_	_
16-286	3226-3232	import	_	_
16-287	3233-3246	AutoTokenizer	_	_
16-288	3247-3251	from	_	_
16-289	3252-3278	examples.ex\_aspire\_consent	_	_
16-290	3279-3285	import	_	_
16-291	3286-3299	AspireConSent	_	_
16-292	3299-3300	,	_	_
16-293	3301-3318	prepare\_abstracts	_	_
16-294	3320-3321	#	_	_
16-295	3322-3332	Initialize	_	_
16-296	3333-3336	the	_	_
16-297	3337-3346	tokenizer	_	_
16-298	3347-3350	and	_	_
16-299	3351-3356	model	_	_
16-300	3356-3357	.	_	_
16-301	3358-3371	hf\_model\_name	_	_
16-302	3372-3373	=	_	_
16-303	3374-3375	'	_	_
16-304	3375-3382	allenai	_	_
16-305	3382-3383	/	_	_
16-306	3383-3424	aspire-contextualsentence-singlem-compsci	_	_
16-307	3424-3425	'	_	_
16-308	3426-3436	aspire\_tok	_	_
16-309	3437-3438	=	_	_
16-310	3439-3468	AutoTokenizer.from\_pretrained	_	_
16-311	3468-3469	(	_	_
16-312	3469-3482	hf\_model\_name	_	_
16-313	3482-3483	)	_	_
16-314	3484-3499	aspire\_mv\_model	_	_
16-315	3500-3501	=	_	_
16-316	3502-3515	AspireConSent	_	_
16-317	3515-3516	(	_	_
16-318	3516-3529	hf\_model\_name	_	_
16-319	3529-3530	)	_	_
16-320	3533-3534	#	_	_
16-321	3535-3542	Example	_	_
16-322	3543-3548	input	_	_
16-323	3548-3549	.	_	_
16-324	3550-3562	ex\_abstracts	_	_
16-325	3563-3564	=	_	_
16-326	3565-3566	\[	_	_
16-327	3571-3572	{	_	_
16-328	3572-3573	'	_	_
16-329	3573-3578	TITLE	_	_
16-330	3578-3579	'	_	_
16-331	3579-3580	:	_	_
16-332	3581-3582	"	_	_
16-333	3582-3594	Multi-Vector	_	_
16-334	3595-3601	Models	_	_
16-335	3602-3606	with	_	_
16-336	3607-3614	Textual	_	_
16-337	3615-3623	Guidance	_	_
16-338	3624-3627	for	_	_
16-339	3628-3640	Fine-Grained	_	_
16-340	3641-3651	Scientific	_	_
16-341	3651-3652	"	_	_
16-342	3667-3668	"	_	_
16-343	3669-3677	Document	_	_
16-344	3678-3688	Similarity	_	_
16-345	3688-3689	"	_	_
16-346	3689-3690	,	_	_
16-347	3696-3697	'	_	_
16-348	3697-3705	ABSTRACT	_	_
16-349	3705-3706	'	_	_
16-350	3706-3707	:	_	_
16-351	3708-3709	\[	_	_
16-352	3709-3710	"	_	_
16-353	3710-3712	We	_	_
16-354	3713-3720	present	_	_
16-355	3721-3722	a	_	_
16-356	3723-3726	new	_	_
16-357	3727-3737	scientific	_	_
16-358	3738-3746	document	_	_
16-359	3747-3757	similarity	_	_
16-360	3758-3763	model	_	_
16-361	3764-3769	based	_	_
16-362	3770-3772	on	_	_
16-363	3773-3774	"	_	_
16-364	3793-3794	"	_	_
16-365	3794-3802	matching	_	_
16-366	3803-3815	fine-grained	_	_
16-367	3816-3823	aspects	_	_
16-368	3824-3826	of	_	_
16-369	3827-3832	texts	_	_
16-370	3832-3833	.	_	_
16-371	3833-3834	"	_	_
16-372	3834-3835	,	_	_
16-373	3854-3855	"	_	_
16-374	3855-3857	To	_	_
16-375	3858-3863	train	_	_
16-376	3864-3867	our	_	_
16-377	3868-3873	model	_	_
16-378	3873-3874	,	_	_
16-379	3875-3877	we	_	_
16-380	3878-3885	exploit	_	_
16-381	3886-3887	a	_	_
16-382	3888-3907	naturally-occurring	_	_
16-383	3908-3914	source	_	_
16-384	3915-3917	of	_	_
16-385	3918-3919	"	_	_
16-386	3938-3939	"	_	_
16-387	3939-3950	supervision	_	_
16-388	3950-3951	:	_	_
16-389	3952-3961	sentences	_	_
16-390	3962-3964	in	_	_
16-391	3965-3968	the	_	_
16-392	3969-3978	full-text	_	_
16-393	3979-3981	of	_	_
16-394	3982-3988	papers	_	_
16-395	3989-3993	that	_	_
16-396	3994-3998	cite	_	_
16-397	3999-4007	multiple	_	_
16-398	4008-4009	"	_	_
16-399	4028-4029	"	_	_
16-400	4029-4035	papers	_	_
16-401	4036-4044	together	_	_
16-402	4045-4046	(	_	_
16-403	4046-4058	co-citations	_	_
16-404	4058-4059	)	_	_
16-405	4059-4060	.	_	_
16-406	4060-4061	"	_	_
16-407	4061-4062	\]	_	_
16-408	4062-4063	}	_	_
16-409	4063-4064	,	_	_
16-410	4069-4070	{	_	_
16-411	4070-4071	'	_	_
16-412	4071-4076	TITLE	_	_
16-413	4076-4077	'	_	_
16-414	4077-4078	:	_	_
16-415	4079-4080	"	_	_
16-416	4080-4087	CSFCube	_	_
16-417	4088-4089	-	_	_
16-418	4089-4090	-	_	_
16-419	4091-4092	A	_	_
16-420	4093-4097	Test	_	_
16-421	4098-4108	Collection	_	_
16-422	4109-4111	of	_	_
16-423	4112-4120	Computer	_	_
16-424	4121-4128	Science	_	_
16-425	4129-4137	Research	_	_
16-426	4138-4146	Articles	_	_
16-427	4147-4150	for	_	_
16-428	4151-4152	"	_	_
16-429	4167-4168	"	_	_
16-430	4168-4175	Faceted	_	_
16-431	4176-4181	Query	_	_
16-432	4182-4184	by	_	_
16-433	4185-4192	Example	_	_
16-434	4192-4193	"	_	_
16-435	4193-4194	,	_	_
16-436	4200-4201	'	_	_
16-437	4201-4209	ABSTRACT	_	_
16-438	4209-4210	'	_	_
16-439	4210-4211	:	_	_
16-440	4212-4213	\[	_	_
16-441	4213-4214	"	_	_
16-442	4214-4219	Query	_	_
16-443	4220-4222	by	_	_
16-444	4223-4230	Example	_	_
16-445	4231-4233	is	_	_
16-446	4234-4235	a	_	_
16-447	4236-4246	well-known	_	_
16-448	4247-4258	information	_	_
16-449	4259-4268	retrieval	_	_
16-450	4269-4273	task	_	_
16-451	4274-4276	in	_	_
16-452	4277-4282	which	_	_
16-453	4282-4283	"	_	_
16-454	4302-4303	"	_	_
16-455	4304-4305	a	_	_
16-456	4306-4314	document	_	_
16-457	4315-4317	is	_	_
16-458	4318-4324	chosen	_	_
16-459	4325-4327	by	_	_
16-460	4328-4331	the	_	_
16-461	4332-4336	user	_	_
16-462	4337-4339	as	_	_
16-463	4340-4343	the	_	_
16-464	4344-4350	search	_	_
16-465	4351-4356	query	_	_
16-466	4357-4360	and	_	_
16-467	4361-4364	the	_	_
16-468	4365-4369	goal	_	_
16-469	4370-4372	is	_	_
16-470	4373-4374	"	_	_
16-471	4393-4394	"	_	_
16-472	4394-4396	to	_	_
16-473	4397-4405	retrieve	_	_
16-474	4406-4414	relevant	_	_
16-475	4415-4424	documents	_	_
16-476	4425-4429	from	_	_
16-477	4430-4431	a	_	_
16-478	4432-4437	large	_	_
16-479	4438-4448	collection	_	_
16-480	4448-4449	.	_	_
16-481	4449-4450	"	_	_
16-482	4450-4451	,	_	_
16-483	4470-4471	"	_	_
16-484	4471-4478	However	_	_
16-485	4478-4479	,	_	_
16-486	4480-4481	a	_	_
16-487	4482-4490	document	_	_
16-488	4491-4496	often	_	_
16-489	4497-4503	covers	_	_
16-490	4504-4512	multiple	_	_
16-491	4513-4520	aspects	_	_
16-492	4521-4523	of	_	_
16-493	4524-4525	a	_	_
16-494	4526-4531	topic	_	_
16-495	4531-4532	.	_	_
16-496	4532-4533	"	_	_
16-497	4533-4534	,	_	_
16-498	4553-4554	"	_	_
16-499	4554-4556	To	_	_
16-500	4557-4564	address	_	_
16-501	4565-4569	this	_	_
16-502	4570-4578	scenario	_	_
16-503	4579-4581	we	_	_
16-504	4582-4591	introduce	_	_
16-505	4592-4595	the	_	_
16-506	4596-4600	task	_	_
16-507	4601-4603	of	_	_
16-508	4604-4611	faceted	_	_
16-509	4612-4617	Query	_	_
16-510	4618-4620	by	_	_
16-511	4621-4622	"	_	_
16-512	4641-4642	"	_	_
16-513	4642-4649	Example	_	_
16-514	4650-4652	in	_	_
16-515	4653-4658	which	_	_
16-516	4659-4664	users	_	_
16-517	4665-4668	can	_	_
16-518	4669-4673	also	_	_
16-519	4674-4681	specify	_	_
16-520	4682-4683	a	_	_
16-521	4684-4689	finer	_	_
16-522	4690-4697	grained	_	_
16-523	4698-4704	aspect	_	_
16-524	4705-4707	in	_	_
16-525	4708-4709	"	_	_
16-526	4728-4729	"	_	_
16-527	4729-4737	addition	_	_
16-528	4738-4740	to	_	_
16-529	4741-4744	the	_	_
16-530	4745-4750	input	_	_
16-531	4751-4756	query	_	_
16-532	4757-4765	document	_	_
16-533	4765-4766	.	_	_
16-534	4767-4768	"	_	_
16-535	4768-4769	\]	_	_
16-536	4769-4770	}	_	_
16-537	4771-4772	\]	_	_
16-538	4774-4784	bert\_batch	_	_
16-539	4784-4785	,	_	_
16-540	4786-4794	abs\_lens	_	_
16-541	4794-4795	,	_	_
16-542	4796-4811	sent\_token\_idxs	_	_
16-543	4812-4813	=	_	_
16-544	4814-4831	prepare\_abstracts	_	_
16-545	4831-4832	(	_	_
16-546	4832-4841	batch\_abs	_	_
16-547	4841-4842	=	_	_
16-548	4842-4854	ex\_abstracts	_	_
16-549	4854-4855	,	_	_
16-550	4914-4929	pt\_lm\_tokenizer	_	_
16-551	4929-4930	=	_	_
16-552	4930-4940	aspire\_tok	_	_
16-553	4940-4941	)	_	_
16-554	4942-4949	clsreps	_	_
16-555	4949-4950	,	_	_
16-556	4951-4971	contextual\_sent\_reps	_	_
16-557	4972-4973	=	_	_
16-558	4974-4997	aspire\_mv\_model.forward	_	_
16-559	4997-4998	(	_	_
16-560	4998-5008	bert\_batch	_	_
16-561	5008-5009	=	_	_
16-562	5009-5019	bert\_batch	_	_
16-563	5019-5020	,	_	_
16-564	5077-5085	abs\_lens	_	_
16-565	5085-5086	=	_	_
16-566	5086-5094	abs\_lens	_	_
16-567	5094-5095	,	_	_
16-568	5152-5165	sent\_tok\_idxs	_	_
16-569	5165-5166	=	_	_
16-570	5166-5181	sent\_token\_idxs	_	_
16-571	5181-5182	)	_	_
16-572	5183-5184	`	_	_
16-573	5184-5185	`	_	_
16-574	5185-5186	`	_	_
16-575	5188-5189	#	_	_
16-576	5189-5190	#	_	_
16-577	5190-5191	#	_	_
16-578	5191-5192	#	_	_
16-579	5192-5193	#	_	_
16-580	5194-5195	`	_	_
16-581	5195-5203	otAspire	_	_
16-582	5203-5204	`	_	_
16-583	5206-5209	The	_	_
16-584	5210-5211	`	_	_
16-585	5211-5219	otAspire	_	_
16-586	5219-5220	`	_	_
16-587	5221-5233	multi-vector	_	_
16-588	5234-5239	model	_	_
16-589	5240-5247	trained	_	_
16-590	5248-5251	for	_	_
16-591	5252-5253	\_	_	_
16-592	5253-5261	multiple	_	_
16-593	5261-5262	\_	_	_
16-594	5263-5271	matching	_	_
16-595	5272-5278	across	_	_
16-596	5279-5288	documents	_	_
16-597	5289-5292	can	_	_
16-598	5293-5295	be	_	_
16-599	5296-5300	used	_	_
16-600	5301-5304	via	_	_
16-601	5305-5308	the	_	_
16-602	5309-5310	`	_	_
16-603	5310-5322	transformers	_	_
16-604	5322-5323	`	_	_
16-605	5324-5331	library	_	_
16-606	5331-5332	,	_	_
16-607	5333-5336	and	_	_
16-608	5337-5341	some	_	_
16-609	5342-5352	additional	_	_
16-610	5353-5357	code	_	_
16-611	5358-5360	to	_	_
16-612	5361-5368	compute	_	_
16-613	5369-5379	contextual	_	_
16-614	5380-5388	sentence	_	_
16-615	5389-5396	vectors	_	_
16-616	5397-5400	and	_	_
16-617	5401-5403	to	_	_
16-618	5404-5408	make	_	_
16-619	5409-5417	multiple	_	_
16-620	5418-5425	matches	_	_
16-621	5426-5431	using	_	_
16-622	5432-5439	optimal	_	_
16-623	5440-5449	transport	_	_
16-624	5449-5450	.	_	_

#Text=View example usage and sample document matches here: \[`examples/demo-contextualsentence-multim.ipynb`\](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `SPECTER-CoCite`  The `SPECTER-CoCite` bi-encoder model can be used via the `transformers` library as:  ```python from transformers import AutoModel, AutoTokenizer aspire\_bienc = AutoModel.from\_pretrained('allenai/aspire-biencoder-compsci-spec') aspire\_tok = AutoTokenizer.from\_pretrained('allenai/aspire-biencoder-compsci-spec') title = "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific "         "Document Similarity" abstract = "We present a new scientific document similarity model based on matching "            "fine-grained aspects of texts." d=\[title + aspire\_tok.sep\_token + abstract\] inputs = aspire\_tok(d, padding=True, truncation=True, return\_tensors="pt", max\_length=512) result = aspire\_bienc(\*\*inputs) clsrep = result.last\_hidden\_state\[:, 0, :\] ```  However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.
17-1	5453-5457	View	_	_
17-2	5458-5465	example	_	_
17-3	5466-5471	usage	_	_
17-4	5472-5475	and	_	_
17-5	5476-5482	sample	_	_
17-6	5483-5491	document	_	_
17-7	5492-5499	matches	_	_
17-8	5500-5504	here	_	_
17-9	5504-5505	:	_	_
17-10	5506-5507	\[	_	_
17-11	5507-5508	`	_	_
17-12	5508-5516	examples	_	_
17-13	5516-5517	/	_	_
17-14	5517-5553	demo-contextualsentence-multim.ipynb	_	_
17-15	5553-5554	`	_	_
17-16	5554-5555	\]	_	_
17-17	5555-5556	(	_	_
17-18	5556-5561	https	_	_
17-19	5561-5562	:	_	_
17-20	5562-5563	/	_	_
17-21	5563-5564	/	_	_
17-22	5564-5574	github.com	_	_
17-23	5574-5575	/	_	_
17-24	5575-5582	allenai	_	_
17-25	5582-5583	/	_	_
17-26	5583-5589	aspire	_	_
17-27	5589-5590	/	_	_
17-28	5590-5594	blob	_	_
17-29	5594-5595	/	_	_
17-30	5595-5599	main	_	_
17-31	5599-5600	/	_	_
17-32	5600-5608	examples	_	_
17-33	5608-5609	/	_	_
17-34	5609-5645	demo-contextualsentence-multim.ipynb	_	_
17-35	5645-5646	)	_	_
17-36	5648-5649	#	_	_
17-37	5649-5650	#	_	_
17-38	5650-5651	#	_	_
17-39	5651-5652	#	_	_
17-40	5652-5653	#	_	_
17-41	5654-5655	`	_	_
17-42	5655-5669	SPECTER-CoCite	_	_
17-43	5669-5670	`	_	_
17-44	5672-5675	The	_	_
17-45	5676-5677	`	_	_
17-46	5677-5691	SPECTER-CoCite	_	_
17-47	5691-5692	`	_	_
17-48	5693-5703	bi-encoder	_	_
17-49	5704-5709	model	_	_
17-50	5710-5713	can	_	_
17-51	5714-5716	be	_	_
17-52	5717-5721	used	_	_
17-53	5722-5725	via	_	_
17-54	5726-5729	the	_	_
17-55	5730-5731	`	_	_
17-56	5731-5743	transformers	_	_
17-57	5743-5744	`	_	_
17-58	5745-5752	library	_	_
17-59	5753-5755	as	_	_
17-60	5755-5756	:	_	_
17-61	5758-5759	`	_	_
17-62	5759-5760	`	_	_
17-63	5760-5761	`	_	_
17-64	5761-5767	python	_	_
17-65	5768-5772	from	_	_
17-66	5773-5785	transformers	_	_
17-67	5786-5792	import	_	_
17-68	5793-5802	AutoModel	_	_
17-69	5802-5803	,	_	_
17-70	5804-5817	AutoTokenizer	_	_
17-71	5818-5830	aspire\_bienc	_	_
17-72	5831-5832	=	_	_
17-73	5833-5858	AutoModel.from\_pretrained	_	_
17-74	5858-5859	(	_	_
17-75	5859-5860	'	_	_
17-76	5860-5867	allenai	_	_
17-77	5867-5868	/	_	_
17-78	5868-5897	aspire-biencoder-compsci-spec	_	_
17-79	5897-5898	'	_	_
17-80	5898-5899	)	_	_
17-81	5900-5910	aspire\_tok	_	_
17-82	5911-5912	=	_	_
17-83	5913-5942	AutoTokenizer.from\_pretrained	_	_
17-84	5942-5943	(	_	_
17-85	5943-5944	'	_	_
17-86	5944-5951	allenai	_	_
17-87	5951-5952	/	_	_
17-88	5952-5981	aspire-biencoder-compsci-spec	_	_
17-89	5981-5982	'	_	_
17-90	5982-5983	)	_	_
17-91	5984-5989	title	_	_
17-92	5990-5991	=	_	_
17-93	5992-5993	"	_	_
17-94	5993-6005	Multi-Vector	_	_
17-95	6006-6012	Models	_	_
17-96	6013-6017	with	_	_
17-97	6018-6025	Textual	_	_
17-98	6026-6034	Guidance	_	_
17-99	6035-6038	for	_	_
17-100	6039-6051	Fine-Grained	_	_
17-101	6052-6062	Scientific	_	_
17-102	6063-6064	"	_	_
17-103	6073-6074	"	_	_
17-104	6074-6082	Document	_	_
17-105	6083-6093	Similarity	_	_
17-106	6093-6094	"	_	_
17-107	6095-6103	abstract	_	_
17-108	6104-6105	=	_	_
17-109	6106-6107	"	_	_
17-110	6107-6109	We	_	_
17-111	6110-6117	present	_	_
17-112	6118-6119	a	_	_
17-113	6120-6123	new	_	_
17-114	6124-6134	scientific	_	_
17-115	6135-6143	document	_	_
17-116	6144-6154	similarity	_	_
17-117	6155-6160	model	_	_
17-118	6161-6166	based	_	_
17-119	6167-6169	on	_	_
17-120	6170-6178	matching	_	_
17-121	6179-6180	"	_	_
17-122	6192-6193	"	_	_
17-123	6193-6205	fine-grained	_	_
17-124	6206-6213	aspects	_	_
17-125	6214-6216	of	_	_
17-126	6217-6222	texts	_	_
17-127	6222-6223	.	_	_
17-128	6223-6224	"	_	_
17-129	6225-6226	d	_	_
17-130	6226-6227	=	_	_
17-131	6227-6228	\[	_	_
17-132	6228-6233	title	_	_
17-133	6234-6235	+	_	_
17-134	6236-6256	aspire\_tok.sep\_token	_	_
17-135	6257-6258	+	_	_
17-136	6259-6267	abstract	_	_
17-137	6267-6268	\]	_	_
17-138	6269-6275	inputs	_	_
17-139	6276-6277	=	_	_
17-140	6278-6288	aspire\_tok	_	_
17-141	6288-6289	(	_	_
17-142	6289-6290	d	_	_
17-143	6290-6291	,	_	_
17-144	6292-6299	padding	_	_
17-145	6299-6300	=	_	_
17-146	6300-6304	True	_	_
17-147	6304-6305	,	_	_
17-148	6306-6316	truncation	_	_
17-149	6316-6317	=	_	_
17-150	6317-6321	True	_	_
17-151	6321-6322	,	_	_
17-152	6323-6337	return\_tensors	_	_
17-153	6337-6338	=	_	_
17-154	6338-6339	"	_	_
17-155	6339-6341	pt	_	_
17-156	6341-6342	"	_	_
17-157	6342-6343	,	_	_
17-158	6344-6354	max\_length	_	_
17-159	6354-6355	=	_	_
17-160	6355-6358	512	_	_
17-161	6358-6359	)	_	_
17-162	6360-6366	result	_	_
17-163	6367-6368	=	_	_
17-164	6369-6381	aspire\_bienc	_	_
17-165	6381-6382	(	_	_
17-166	6382-6383	\*	_	_
17-167	6383-6384	\*	_	_
17-168	6384-6390	inputs	_	_
17-169	6390-6391	)	_	_
17-170	6392-6398	clsrep	_	_
17-171	6399-6400	=	_	_
17-172	6401-6425	result.last\_hidden\_state	_	_
17-173	6425-6426	\[	_	_
17-174	6426-6427	:	_	_
17-175	6427-6428	,	_	_
17-176	6429-6430	0	_	_
17-177	6430-6431	,	_	_
17-178	6432-6433	:	_	_
17-179	6433-6434	\]	_	_
17-180	6435-6436	`	_	_
17-181	6436-6437	`	_	_
17-182	6437-6438	`	_	_
17-183	6440-6447	However	_	_
17-184	6447-6448	,	_	_
17-185	6449-6453	note	_	_
17-186	6454-6458	that	_	_
17-187	6459-6462	the	_	_
17-188	6463-6470	Hugging	_	_
17-189	6471-6475	Face	_	_
17-190	6476-6482	models	_	_
17-191	6483-6488	don't	_	_
17-192	6489-6493	have	_	_
17-193	6494-6495	a	_	_
17-194	6496-6499	set	_	_
17-195	6500-6502	of	_	_
17-196	6503-6513	additional	_	_
17-197	6514-6524	scalar-mix	_	_
17-198	6525-6535	parameters	_	_
17-199	6536-6538	to	_	_
17-200	6539-6546	compute	_	_
17-201	6547-6548	a	_	_
17-202	6549-6556	learned	_	_
17-203	6557-6565	weighted	_	_
17-204	6566-6569	sum	_	_
17-205	6570-6572	of	_	_
17-206	6573-6576	the	_	_
17-207	6577-6592	representations	_	_
17-208	6593-6597	from	_	_
17-209	6598-6607	different	_	_
17-210	6608-6614	layers	_	_
17-211	6615-6617	of	_	_
17-212	6618-6621	the	_	_
17-213	6622-6633	transformer	_	_
17-214	6634-6641	encoder	_	_
17-215	6641-6642	.	_	_

#Text=These are used in our paper and are important for performance in some datasets.
18-1	6643-6648	These	_	_
18-2	6649-6652	are	_	_
18-3	6653-6657	used	_	_
18-4	6658-6660	in	_	_
18-5	6661-6664	our	_	_
18-6	6665-6670	paper	_	_
18-7	6671-6674	and	_	_
18-8	6675-6678	are	_	_
18-9	6679-6688	important	_	_
18-10	6689-6692	for	_	_
18-11	6693-6704	performance	_	_
18-12	6705-6707	in	_	_
18-13	6708-6712	some	_	_
18-14	6713-6721	datasets	_	_
18-15	6721-6722	.	_	_

#Text=Obtain the model zip files:  - \[`aspire-biencoder-biomed-scib-full`\](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip) - \[`aspire-biencoder-biomed-spec-full`\](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip) - \[`aspire-biencoder-compsci-spec-full`\](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  ```bash wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip unzip aspire-biencoder-compsci-spec-full.zip ```  Now it may be used as:  ```python  import os, json, codecs, torch from transformers import AutoTokenizer from examples.ex\_aspire\_bienc import AspireBiEnc  # Directory where zipped model was downloaded and unzipped. model\_path = '.
19-1	6723-6729	Obtain	_	_
19-2	6730-6733	the	_	_
19-3	6734-6739	model	_	_
19-4	6740-6743	zip	_	_
19-5	6744-6749	files	_	_
19-6	6749-6750	:	_	_
19-7	6752-6753	-	_	_
19-8	6754-6755	\[	_	_
19-9	6755-6756	`	_	_
19-10	6756-6789	aspire-biencoder-biomed-scib-full	_	_
19-11	6789-6790	`	_	_
19-12	6790-6791	\]	_	_
19-13	6791-6792	(	_	_
19-14	6792-6797	https	_	_
19-15	6797-6798	:	_	_
19-16	6798-6799	/	_	_
19-17	6799-6800	/	_	_
19-18	6800-6803	ai2	_	_
19-19	6803-6804	-	_	_
19-20	6804-6806	s2	_	_
19-21	6806-6807	-	_	_
19-22	6807-6818	research.s3	_	_
19-23	6818-6819	.	_	_
19-24	6819-6826	us-west	_	_
19-25	6826-6827	-	_	_
19-26	6827-6828	2	_	_
19-27	6828-6829	.	_	_
19-28	6829-6842	amazonaws.com	_	_
19-29	6842-6843	/	_	_
19-30	6843-6849	aspire	_	_
19-31	6849-6850	/	_	_
19-32	6850-6887	aspire-biencoder-biomed-scib-full.zip	_	_
19-33	6887-6888	)	_	_
19-34	6889-6890	-	_	_
19-35	6891-6892	\[	_	_
19-36	6892-6893	`	_	_
19-37	6893-6926	aspire-biencoder-biomed-spec-full	_	_
19-38	6926-6927	`	_	_
19-39	6927-6928	\]	_	_
19-40	6928-6929	(	_	_
19-41	6929-6934	https	_	_
19-42	6934-6935	:	_	_
19-43	6935-6936	/	_	_
19-44	6936-6937	/	_	_
19-45	6937-6940	ai2	_	_
19-46	6940-6941	-	_	_
19-47	6941-6943	s2	_	_
19-48	6943-6944	-	_	_
19-49	6944-6955	research.s3	_	_
19-50	6955-6956	.	_	_
19-51	6956-6963	us-west	_	_
19-52	6963-6964	-	_	_
19-53	6964-6965	2	_	_
19-54	6965-6966	.	_	_
19-55	6966-6979	amazonaws.com	_	_
19-56	6979-6980	/	_	_
19-57	6980-6986	aspire	_	_
19-58	6986-6987	/	_	_
19-59	6987-7024	aspire-biencoder-biomed-spec-full.zip	_	_
19-60	7024-7025	)	_	_
19-61	7026-7027	-	_	_
19-62	7028-7029	\[	_	_
19-63	7029-7030	`	_	_
19-64	7030-7064	aspire-biencoder-compsci-spec-full	_	_
19-65	7064-7065	`	_	_
19-66	7065-7066	\]	_	_
19-67	7066-7067	(	_	_
19-68	7067-7072	https	_	_
19-69	7072-7073	:	_	_
19-70	7073-7074	/	_	_
19-71	7074-7075	/	_	_
19-72	7075-7078	ai2	_	_
19-73	7078-7079	-	_	_
19-74	7079-7081	s2	_	_
19-75	7081-7082	-	_	_
19-76	7082-7093	research.s3	_	_
19-77	7093-7094	.	_	_
19-78	7094-7101	us-west	_	_
19-79	7101-7102	-	_	_
19-80	7102-7103	2	_	_
19-81	7103-7104	.	_	_
19-82	7104-7117	amazonaws.com	_	_
19-83	7117-7118	/	_	_
19-84	7118-7124	aspire	_	_
19-85	7124-7125	/	_	_
19-86	7125-7163	aspire-biencoder-compsci-spec-full.zip	_	_
19-87	7163-7164	)	_	_
19-88	7166-7167	`	_	_
19-89	7167-7168	`	_	_
19-90	7168-7169	`	_	_
19-91	7169-7173	bash	_	_
19-92	7174-7178	wget	_	_
19-93	7179-7180	-	_	_
19-94	7180-7181	O	_	_
19-95	7182-7220	aspire-biencoder-compsci-spec-full.zip	_	_
19-96	7221-7226	https	_	_
19-97	7226-7227	:	_	_
19-98	7227-7228	/	_	_
19-99	7228-7229	/	_	_
19-100	7229-7232	ai2	_	_
19-101	7232-7233	-	_	_
19-102	7233-7235	s2	_	_
19-103	7235-7236	-	_	_
19-104	7236-7247	research.s3	_	_
19-105	7247-7248	.	_	_
19-106	7248-7255	us-west	_	_
19-107	7255-7256	-	_	_
19-108	7256-7257	2	_	_
19-109	7257-7258	.	_	_
19-110	7258-7271	amazonaws.com	_	_
19-111	7271-7272	/	_	_
19-112	7272-7278	aspire	_	_
19-113	7278-7279	/	_	_
19-114	7279-7317	aspire-biencoder-compsci-spec-full.zip	_	_
19-115	7318-7323	unzip	_	_
19-116	7324-7362	aspire-biencoder-compsci-spec-full.zip	_	_
19-117	7363-7364	`	_	_
19-118	7364-7365	`	_	_
19-119	7365-7366	`	_	_
19-120	7368-7371	Now	_	_
19-121	7372-7374	it	_	_
19-122	7375-7378	may	_	_
19-123	7379-7381	be	_	_
19-124	7382-7386	used	_	_
19-125	7387-7389	as	_	_
19-126	7389-7390	:	_	_
19-127	7392-7393	`	_	_
19-128	7393-7394	`	_	_
19-129	7394-7395	`	_	_
19-130	7395-7401	python	_	_
19-131	7403-7409	import	_	_
19-132	7410-7412	os	_	_
19-133	7412-7413	,	_	_
19-134	7414-7418	json	_	_
19-135	7418-7419	,	_	_
19-136	7420-7426	codecs	_	_
19-137	7426-7427	,	_	_
19-138	7428-7433	torch	_	_
19-139	7434-7438	from	_	_
19-140	7439-7451	transformers	_	_
19-141	7452-7458	import	_	_
19-142	7459-7472	AutoTokenizer	_	_
19-143	7473-7477	from	_	_
19-144	7478-7502	examples.ex\_aspire\_bienc	_	_
19-145	7503-7509	import	_	_
19-146	7510-7521	AspireBiEnc	_	_
19-147	7523-7524	#	_	_
19-148	7525-7534	Directory	_	_
19-149	7535-7540	where	_	_
19-150	7541-7547	zipped	_	_
19-151	7548-7553	model	_	_
19-152	7554-7557	was	_	_
19-153	7558-7568	downloaded	_	_
19-154	7569-7572	and	_	_
19-155	7573-7581	unzipped	_	_
19-156	7581-7582	.	_	_
19-157	7583-7593	model\_path	_	_
19-158	7594-7595	=	_	_
19-159	7596-7597	'	_	_
19-160	7597-7598	.	_	_

#Text=/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model\_path, 'run\_info.json'), 'r') as fp:     hparams = json.load(fp)     model\_hparams = hparams\['all\_hparams'\]  # Initialize the tokenizer and model. aspire\_tok = AutoTokenizer.from\_pretrained(model\_hparams\['base-pt-layer'\]) aspire\_bienc = AspireBiEnc(model\_hparams)  # Load model parameters from disk. model\_fname = os.path.join(model\_path, 'model\_cur\_best.pt') aspire\_bienc.load\_state\_dict(torch.load(model\_fname))  # Encode example input. title = "Multi-Vector Models with Textual Guidance for Fine-Grained Scientific "         "Document Similarity" abstract = "We present a new scientific document similarity model based on matching "            "fine-grained aspects of texts." d = \[title + aspire\_tok.sep\_token + abstract\]  inputs = aspire\_tok(d, padding=True, truncation=True, return\_tensors="pt", max\_length=512) clsrep = aspire\_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name="evaldata"></a>  The paper uses the following evaluation datasets:  - RELISH was created in \[Brown et al. 2019\](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?
20-1	7598-7599	/	_	_
20-2	7599-7633	aspire-biencoder-compsci-spec-full	_	_
20-3	7633-7634	'	_	_
20-4	7636-7637	#	_	_
20-5	7638-7642	Load	_	_
20-6	7643-7658	hyperparameters	_	_
20-7	7659-7663	from	_	_
20-8	7664-7668	disk	_	_
20-9	7668-7669	.	_	_
20-10	7670-7674	with	_	_
20-11	7675-7686	codecs.open	_	_
20-12	7686-7687	(	_	_
20-13	7687-7699	os.path.join	_	_
20-14	7699-7700	(	_	_
20-15	7700-7710	model\_path	_	_
20-16	7710-7711	,	_	_
20-17	7712-7713	'	_	_
20-18	7713-7726	run\_info.json	_	_
20-19	7726-7727	'	_	_
20-20	7727-7728	)	_	_
20-21	7728-7729	,	_	_
20-22	7730-7731	'	_	_
20-23	7731-7732	r	_	_
20-24	7732-7733	'	_	_
20-25	7733-7734	)	_	_
20-26	7735-7737	as	_	_
20-27	7738-7740	fp	_	_
20-28	7740-7741	:	_	_
20-29	7746-7753	hparams	_	_
20-30	7754-7755	=	_	_
20-31	7756-7765	json.load	_	_
20-32	7765-7766	(	_	_
20-33	7766-7768	fp	_	_
20-34	7768-7769	)	_	_
20-35	7774-7787	model\_hparams	_	_
20-36	7788-7789	=	_	_
20-37	7790-7797	hparams	_	_
20-38	7797-7798	\[	_	_
20-39	7798-7799	'	_	_
20-40	7799-7810	all\_hparams	_	_
20-41	7810-7811	'	_	_
20-42	7811-7812	\]	_	_
20-43	7814-7815	#	_	_
20-44	7816-7826	Initialize	_	_
20-45	7827-7830	the	_	_
20-46	7831-7840	tokenizer	_	_
20-47	7841-7844	and	_	_
20-48	7845-7850	model	_	_
20-49	7850-7851	.	_	_
20-50	7852-7862	aspire\_tok	_	_
20-51	7863-7864	=	_	_
20-52	7865-7894	AutoTokenizer.from\_pretrained	_	_
20-53	7894-7895	(	_	_
20-54	7895-7908	model\_hparams	_	_
20-55	7908-7909	\[	_	_
20-56	7909-7910	'	_	_
20-57	7910-7923	base-pt-layer	_	_
20-58	7923-7924	'	_	_
20-59	7924-7925	\]	_	_
20-60	7925-7926	)	_	_
20-61	7927-7939	aspire\_bienc	_	_
20-62	7940-7941	=	_	_
20-63	7942-7953	AspireBiEnc	_	_
20-64	7953-7954	(	_	_
20-65	7954-7967	model\_hparams	_	_
20-66	7967-7968	)	_	_
20-67	7970-7971	#	_	_
20-68	7972-7976	Load	_	_
20-69	7977-7982	model	_	_
20-70	7983-7993	parameters	_	_
20-71	7994-7998	from	_	_
20-72	7999-8003	disk	_	_
20-73	8003-8004	.	_	_
20-74	8005-8016	model\_fname	_	_
20-75	8017-8018	=	_	_
20-76	8019-8031	os.path.join	_	_
20-77	8031-8032	(	_	_
20-78	8032-8042	model\_path	_	_
20-79	8042-8043	,	_	_
20-80	8044-8045	'	_	_
20-81	8045-8062	model\_cur\_best.pt	_	_
20-82	8062-8063	'	_	_
20-83	8063-8064	)	_	_
20-84	8065-8093	aspire\_bienc.load\_state\_dict	_	_
20-85	8093-8094	(	_	_
20-86	8094-8104	torch.load	_	_
20-87	8104-8105	(	_	_
20-88	8105-8116	model\_fname	_	_
20-89	8116-8117	)	_	_
20-90	8117-8118	)	_	_
20-91	8120-8121	#	_	_
20-92	8122-8128	Encode	_	_
20-93	8129-8136	example	_	_
20-94	8137-8142	input	_	_
20-95	8142-8143	.	_	_
20-96	8144-8149	title	_	_
20-97	8150-8151	=	_	_
20-98	8152-8153	"	_	_
20-99	8153-8165	Multi-Vector	_	_
20-100	8166-8172	Models	_	_
20-101	8173-8177	with	_	_
20-102	8178-8185	Textual	_	_
20-103	8186-8194	Guidance	_	_
20-104	8195-8198	for	_	_
20-105	8199-8211	Fine-Grained	_	_
20-106	8212-8222	Scientific	_	_
20-107	8223-8224	"	_	_
20-108	8233-8234	"	_	_
20-109	8234-8242	Document	_	_
20-110	8243-8253	Similarity	_	_
20-111	8253-8254	"	_	_
20-112	8255-8263	abstract	_	_
20-113	8264-8265	=	_	_
20-114	8266-8267	"	_	_
20-115	8267-8269	We	_	_
20-116	8270-8277	present	_	_
20-117	8278-8279	a	_	_
20-118	8280-8283	new	_	_
20-119	8284-8294	scientific	_	_
20-120	8295-8303	document	_	_
20-121	8304-8314	similarity	_	_
20-122	8315-8320	model	_	_
20-123	8321-8326	based	_	_
20-124	8327-8329	on	_	_
20-125	8330-8338	matching	_	_
20-126	8339-8340	"	_	_
20-127	8352-8353	"	_	_
20-128	8353-8365	fine-grained	_	_
20-129	8366-8373	aspects	_	_
20-130	8374-8376	of	_	_
20-131	8377-8382	texts	_	_
20-132	8382-8383	.	_	_
20-133	8383-8384	"	_	_
20-134	8385-8386	d	_	_
20-135	8387-8388	=	_	_
20-136	8389-8390	\[	_	_
20-137	8390-8395	title	_	_
20-138	8396-8397	+	_	_
20-139	8398-8418	aspire\_tok.sep\_token	_	_
20-140	8419-8420	+	_	_
20-141	8421-8429	abstract	_	_
20-142	8429-8430	\]	_	_
20-143	8432-8438	inputs	_	_
20-144	8439-8440	=	_	_
20-145	8441-8451	aspire\_tok	_	_
20-146	8451-8452	(	_	_
20-147	8452-8453	d	_	_
20-148	8453-8454	,	_	_
20-149	8455-8462	padding	_	_
20-150	8462-8463	=	_	_
20-151	8463-8467	True	_	_
20-152	8467-8468	,	_	_
20-153	8469-8479	truncation	_	_
20-154	8479-8480	=	_	_
20-155	8480-8484	True	_	_
20-156	8484-8485	,	_	_
20-157	8486-8500	return\_tensors	_	_
20-158	8500-8501	=	_	_
20-159	8501-8502	"	_	_
20-160	8502-8504	pt	_	_
20-161	8504-8505	"	_	_
20-162	8505-8506	,	_	_
20-163	8507-8517	max\_length	_	_
20-164	8517-8518	=	_	_
20-165	8518-8521	512	_	_
20-166	8521-8522	)	_	_
20-167	8523-8529	clsrep	_	_
20-168	8530-8531	=	_	_
20-169	8532-8552	aspire\_bienc.forward	_	_
20-170	8552-8553	(	_	_
20-171	8553-8559	inputs	_	_
20-172	8559-8560	)	_	_
20-173	8562-8563	`	_	_
20-174	8563-8564	`	_	_
20-175	8564-8565	`	_	_
20-176	8568-8569	#	_	_
20-177	8569-8570	#	_	_
20-178	8570-8571	#	_	_
20-179	8571-8572	#	_	_
20-180	8573-8583	Evaluation	_	_
20-181	8584-8592	Datasets	_	_
20-182	8593-8594	<	_	_
20-183	8594-8595	a	_	_
20-184	8596-8600	name	_	_
20-185	8600-8601	=	_	_
20-186	8601-8602	"	_	_
20-187	8602-8610	evaldata	_	_
20-188	8610-8611	"	_	_
20-189	8611-8612	>	_	_
20-190	8612-8613	<	_	_
20-191	8613-8614	/	_	_
20-192	8614-8615	a	_	_
20-193	8615-8616	>	_	_
20-194	8618-8621	The	_	_
20-195	8622-8627	paper	_	_
20-196	8628-8632	uses	_	_
20-197	8633-8636	the	_	_
20-198	8637-8646	following	_	_
20-199	8647-8657	evaluation	_	_
20-200	8658-8666	datasets	_	_
20-201	8666-8667	:	_	_
20-202	8669-8670	-	_	_
20-203	8671-8677	RELISH	_	_
20-204	8678-8681	was	_	_
20-205	8682-8689	created	_	_
20-206	8690-8692	in	_	_
20-207	8693-8694	\[	_	_
20-208	8694-8699	Brown	_	_
20-209	8700-8702	et	_	_
20-210	8703-8705	al	_	_
20-211	8705-8706	.	_	_
20-212	8707-8711	2019	_	_
20-213	8711-8712	\]	_	_
20-214	8712-8713	(	_	_
20-215	8713-8718	https	_	_
20-216	8718-8719	:	_	_
20-217	8719-8720	/	_	_
20-218	8720-8721	/	_	_
20-219	8721-8737	academic.oup.com	_	_
20-220	8737-8738	/	_	_
20-221	8738-8746	database	_	_
20-222	8746-8747	/	_	_
20-223	8747-8754	article	_	_
20-224	8754-8755	/	_	_
20-225	8755-8758	doi	_	_
20-226	8758-8759	/	_	_
20-227	8759-8766	10.1093	_	_
20-228	8766-8767	/	_	_
20-229	8767-8775	database	_	_
20-230	8775-8776	/	_	_
20-231	8776-8782	baz085	_	_
20-232	8782-8783	/	_	_
20-233	8783-8790	5608006	_	_
20-234	8790-8791	?	_	_

#Text=login=true).
21-1	8791-8796	login	_	_
21-2	8796-8797	=	_	_
21-3	8797-8801	true	_	_
21-4	8801-8802	)	_	_
21-5	8802-8803	.	_	_

#Text=While I wasn't able to access the link in the publication.
22-1	8804-8809	While	_	_
22-2	8810-8811	I	_	_
22-3	8812-8818	wasn't	_	_
22-4	8819-8823	able	_	_
22-5	8824-8826	to	_	_
22-6	8827-8833	access	_	_
22-7	8834-8837	the	_	_
22-8	8838-8842	link	_	_
22-9	8843-8845	in	_	_
22-10	8846-8849	the	_	_
22-11	8850-8861	publication	_	_
22-12	8861-8862	.	_	_

#Text=I was able to obtain a copy of the dataset from: \[link\](http://pubannotation.org/projects/RELISH-DB).
23-1	8863-8864	I	_	_
23-2	8865-8868	was	_	_
23-3	8869-8873	able	_	_
23-4	8874-8876	to	_	_
23-5	8877-8883	obtain	_	_
23-6	8884-8885	a	_	_
23-7	8886-8890	copy	_	_
23-8	8891-8893	of	_	_
23-9	8894-8897	the	_	_
23-10	8898-8905	dataset	_	_
23-11	8906-8910	from	_	_
23-12	8910-8911	:	_	_
23-13	8912-8913	\[	_	_
23-14	8913-8917	link	_	_
23-15	8917-8918	\]	_	_
23-16	8918-8919	(	_	_
23-17	8919-8923	http	_	_
23-18	8923-8924	:	_	_
23-19	8924-8925	/	_	_
23-20	8925-8926	/	_	_
23-21	8926-8943	pubannotation.org	_	_
23-22	8943-8944	/	_	_
23-23	8944-8952	projects	_	_
23-24	8952-8953	/	_	_
23-25	8953-8962	RELISH-DB	_	_
23-26	8962-8963	)	_	_
23-27	8963-8964	.	_	_

#Text=Dataset splits are created in `pre\_proc\_relish.py`
24-1	8965-8972	Dataset	_	_
24-2	8973-8979	splits	_	_
24-3	8980-8983	are	_	_
24-4	8984-8991	created	_	_
24-5	8992-8994	in	_	_
24-6	8995-8996	`	_	_
24-7	8996-9014	pre\_proc\_relish.py	_	_
24-8	9014-9015	`	_	_

#Text=.
25-1	9015-9016	.	_	_

#Text=- TRECCOVID presents an ad-hoc search dataset.
26-1	9018-9019	-	_	_
26-2	9020-9029	TRECCOVID	_	_
26-3	9030-9038	presents	_	_
26-4	9039-9041	an	_	_
26-5	9042-9048	ad-hoc	_	_
26-6	9049-9055	search	_	_
26-7	9056-9063	dataset	_	_
26-8	9063-9064	.	_	_

#Text=The versions of the dataset used may be accessed here: \[query topics\](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), \[relevance annotations\](https://ir.nist.gov/covidSubmit/data/qrels-covid\_d5\_j0.5-5.txt), and the metadata for papers is obtained from the \[CORD-19\](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical\_releases.html) dataset in the \[2021-06-21\](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release.
27-1	9065-9068	The	_	_
27-2	9069-9077	versions	_	_
27-3	9078-9080	of	_	_
27-4	9081-9084	the	_	_
27-5	9085-9092	dataset	_	_
27-6	9093-9097	used	_	_
27-7	9098-9101	may	_	_
27-8	9102-9104	be	_	_
27-9	9105-9113	accessed	_	_
27-10	9114-9118	here	_	_
27-11	9118-9119	:	_	_
27-12	9120-9121	\[	_	_
27-13	9121-9126	query	_	_
27-14	9127-9133	topics	_	_
27-15	9133-9134	\]	_	_
27-16	9134-9135	(	_	_
27-17	9135-9140	https	_	_
27-18	9140-9141	:	_	_
27-19	9141-9142	/	_	_
27-20	9142-9143	/	_	_
27-21	9143-9154	ir.nist.gov	_	_
27-22	9154-9155	/	_	_
27-23	9155-9166	covidSubmit	_	_
27-24	9166-9167	/	_	_
27-25	9167-9171	data	_	_
27-26	9171-9172	/	_	_
27-27	9172-9183	topics-rnd5	_	_
27-28	9183-9184	.	_	_
27-29	9184-9187	xml	_	_
27-30	9187-9188	)	_	_
27-31	9188-9189	,	_	_
27-32	9190-9191	\[	_	_
27-33	9191-9200	relevance	_	_
27-34	9201-9212	annotations	_	_
27-35	9212-9213	\]	_	_
27-36	9213-9214	(	_	_
27-37	9214-9219	https	_	_
27-38	9219-9220	:	_	_
27-39	9220-9221	/	_	_
27-40	9221-9222	/	_	_
27-41	9222-9233	ir.nist.gov	_	_
27-42	9233-9234	/	_	_
27-43	9234-9245	covidSubmit	_	_
27-44	9245-9246	/	_	_
27-45	9246-9250	data	_	_
27-46	9250-9251	/	_	_
27-47	9251-9265	qrels-covid\_d5	_	_
27-48	9265-9266	\_	_	_
27-49	9266-9270	j0.5	_	_
27-50	9270-9271	-	_	_
27-51	9271-9272	5	_	_
27-52	9272-9273	.	_	_
27-53	9273-9276	txt	_	_
27-54	9276-9277	)	_	_
27-55	9277-9278	,	_	_
27-56	9279-9282	and	_	_
27-57	9283-9286	the	_	_
27-58	9287-9295	metadata	_	_
27-59	9296-9299	for	_	_
27-60	9300-9306	papers	_	_
27-61	9307-9309	is	_	_
27-62	9310-9318	obtained	_	_
27-63	9319-9323	from	_	_
27-64	9324-9327	the	_	_
27-65	9328-9329	\[	_	_
27-66	9329-9333	CORD	_	_
27-67	9333-9334	-	_	_
27-68	9334-9336	19	_	_
27-69	9336-9337	\]	_	_
27-70	9337-9338	(	_	_
27-71	9338-9343	https	_	_
27-72	9343-9344	:	_	_
27-73	9344-9345	/	_	_
27-74	9345-9346	/	_	_
27-75	9346-9349	ai2	_	_
27-76	9349-9350	-	_	_
27-77	9350-9370	semanticscholar-cord	_	_
27-78	9370-9371	-	_	_
27-79	9371-9373	19	_	_
27-80	9373-9374	.	_	_
27-81	9374-9376	s3	_	_
27-82	9376-9377	-	_	_
27-83	9377-9384	us-west	_	_
27-84	9384-9385	-	_	_
27-85	9385-9386	2	_	_
27-86	9386-9387	.	_	_
27-87	9387-9400	amazonaws.com	_	_
27-88	9400-9401	/	_	_
27-89	9401-9425	historical\_releases.html	_	_
27-90	9425-9426	)	_	_
27-91	9427-9434	dataset	_	_
27-92	9435-9437	in	_	_
27-93	9438-9441	the	_	_
27-94	9442-9443	\[	_	_
27-95	9443-9447	2021	_	_
27-96	9447-9448	-	_	_
27-97	9448-9450	06	_	_
27-98	9450-9451	-	_	_
27-99	9451-9453	21	_	_
27-100	9453-9454	\]	_	_
27-101	9454-9455	(	_	_
27-102	9455-9460	https	_	_
27-103	9460-9461	:	_	_
27-104	9461-9462	/	_	_
27-105	9462-9463	/	_	_
27-106	9463-9466	ai2	_	_
27-107	9466-9467	-	_	_
27-108	9467-9487	semanticscholar-cord	_	_
27-109	9487-9488	-	_	_
27-110	9488-9490	19	_	_
27-111	9490-9491	.	_	_
27-112	9491-9493	s3	_	_
27-113	9493-9494	-	_	_
27-114	9494-9501	us-west	_	_
27-115	9501-9502	-	_	_
27-116	9502-9503	2	_	_
27-117	9503-9504	.	_	_
27-118	9504-9517	amazonaws.com	_	_
27-119	9517-9518	/	_	_
27-120	9518-9522	2021	_	_
27-121	9522-9523	-	_	_
27-122	9523-9525	06	_	_
27-123	9525-9526	-	_	_
27-124	9526-9528	21	_	_
27-125	9528-9529	/	_	_
27-126	9529-9541	metadata.csv	_	_
27-127	9541-9542	)	_	_
27-128	9543-9550	release	_	_
27-129	9550-9551	.	_	_

#Text=The function `get\_qbe\_pools` in `pre\_proc\_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper.
28-1	9552-9555	The	_	_
28-2	9556-9564	function	_	_
28-3	9565-9566	`	_	_
28-4	9566-9579	get\_qbe\_pools	_	_
28-5	9579-9580	`	_	_
28-6	9581-9583	in	_	_
28-7	9584-9585	`	_	_
28-8	9585-9606	pre\_proc\_treccovid.py	_	_
28-8	9594-9603	treccovid	_	_
28-9	9606-9607	`	_	_
28-10	9607-9608	,	_	_
28-11	9609-9617	converts	_	_
28-12	9618-9621	the	_	_
28-13	9622-9629	dataset	_	_
28-14	9630-9632	in	_	_
28-15	9633-9636	its	_	_
28-16	9637-9645	original	_	_
28-17	9646-9650	form	_	_
28-18	9651-9653	to	_	_
28-19	9654-9657	the	_	_
28-20	9658-9670	reformulated	_	_
28-21	9671-9675	form	_	_
28-22	9675-9676	,	_	_
28-23	9677-9689	TRECCOVID-RF	_	_
28-23	9677-9686	TRECCOVID	_	_
28-24	9689-9690	,	_	_
28-25	9691-9695	used	_	_
28-26	9696-9698	in	_	_
28-27	9699-9702	the	_	_
28-28	9703-9708	paper	_	_
28-29	9708-9709	.	_	_

#Text=Dataset splits are created in `pre\_proc\_treccovid.py`
29-1	9710-9717	Dataset	_	_
29-2	9718-9724	splits	_	_
29-3	9725-9728	are	_	_
29-4	9729-9736	created	_	_
29-5	9737-9739	in	_	_
29-6	9740-9741	`	_	_
29-7	9741-9762	pre\_proc\_treccovid.py	_	_
29-7	9750-9759	treccovid	_	_
29-8	9762-9763	`	_	_

#Text=.
30-1	9763-9764	.	_	_

#Text=- SciDocs is obtained from: \[link\](https://github.com/allenai/scidocs).
31-1	9766-9767	-	_	_
31-2	9768-9775	SciDocs	_	_
31-3	9776-9778	is	_	_
31-4	9779-9787	obtained	_	_
31-5	9788-9792	from	_	_
31-6	9792-9793	:	_	_
31-7	9794-9795	\[	_	_
31-8	9795-9799	link	_	_
31-9	9799-9800	\]	_	_
31-10	9800-9801	(	_	_
31-11	9801-9806	https	_	_
31-12	9806-9807	:	_	_
31-13	9807-9808	/	_	_
31-14	9808-9809	/	_	_
31-15	9809-9819	github.com	_	_
31-16	9819-9820	/	_	_
31-17	9820-9827	allenai	_	_
31-18	9827-9828	/	_	_
31-19	9828-9835	scidocs	_	_
31-20	9835-9836	)	_	_
31-21	9836-9837	.	_	_

#Text=The dataset splits supplied alongside the original dataset are used as is
32-1	9838-9841	The	_	_
32-2	9842-9849	dataset	_	_
32-3	9850-9856	splits	_	_
32-4	9857-9865	supplied	_	_
32-5	9866-9875	alongside	_	_
32-6	9876-9879	the	_	_
32-7	9880-9888	original	_	_
32-8	9889-9896	dataset	_	_
32-9	9897-9900	are	_	_
32-10	9901-9905	used	_	_
32-11	9906-9908	as	_	_
32-12	9909-9911	is	_	_

#Text=.
33-1	9911-9912	.	_	_

#Text=- CSFCube is obtained from: \[link\](https://github.com/iesl/CSFCube).
34-1	9914-9915	-	_	_
34-2	9916-9923	CSFCube	_	_
34-3	9924-9926	is	_	_
34-4	9927-9935	obtained	_	_
34-5	9936-9940	from	_	_
34-6	9940-9941	:	_	_
34-7	9942-9943	\[	_	_
34-8	9943-9947	link	_	_
34-9	9947-9948	\]	_	_
34-10	9948-9949	(	_	_
34-11	9949-9954	https	_	_
34-12	9954-9955	:	_	_
34-13	9955-9956	/	_	_
34-14	9956-9957	/	_	_
34-15	9957-9967	github.com	_	_
34-16	9967-9968	/	_	_
34-17	9968-9972	iesl	_	_
34-18	9972-9973	/	_	_
34-19	9973-9980	CSFCube	_	_
34-20	9980-9981	)	_	_
34-21	9981-9982	.	_	_

#Text=The dataset splits supplied alongside the original dataset are used as is.
35-1	9983-9986	The	_	_
35-2	9987-9994	dataset	_	_
35-3	9995-10001	splits	_	_
35-4	10002-10010	supplied	_	_
35-5	10011-10020	alongside	_	_
35-6	10021-10024	the	_	_
35-7	10025-10033	original	_	_
35-8	10034-10041	dataset	_	_
35-9	10042-10045	are	_	_
35-10	10046-10050	used	_	_
35-11	10051-10053	as	_	_
35-12	10054-10056	is	_	_
35-13	10056-10057	.	_	_

#Text=Complete evaluation datasets used in the paper can be downloaded here: \[`datasets/datasets.md`\](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name="repocontents"></a>      ├── bin     ├── config     │             └── models\_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid\_models         │             │             ├── disent\_models.py         │             │             ├── pair\_distances.py         │             │             └── sentsim\_models.py         │             ├── main\_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre\_process             ├── extract\_entities.py             ├── pp\_settings.py             ├── pre\_proc\_cocits.py             ├── pre\_proc\_gorc.py             ├── pre\_proc\_relish.py             ├── pre\_proc\_scidocs.py             ├── pre\_proc\_treccovid.py             ├── pp\_gen\_nearest.py             └── pre\_proc\_buildreps.py   \*\*The repository is organized broadly as:\*\*  `src/pre\_process/`: Scripts to 1) generate gather and filter co-citations data from the \[S2ORC\](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets.
36-1	10059-10067	Complete	_	_
36-2	10068-10078	evaluation	_	_
36-3	10079-10087	datasets	_	_
36-4	10088-10092	used	_	_
36-5	10093-10095	in	_	_
36-6	10096-10099	the	_	_
36-7	10100-10105	paper	_	_
36-8	10106-10109	can	_	_
36-9	10110-10112	be	_	_
36-10	10113-10123	downloaded	_	_
36-11	10124-10128	here	_	_
36-12	10128-10129	:	_	_
36-13	10130-10131	\[	_	_
36-14	10131-10132	`	_	_
36-15	10132-10140	datasets	_	_
36-16	10140-10141	/	_	_
36-17	10141-10152	datasets.md	_	_
36-18	10152-10153	`	_	_
36-19	10153-10154	\]	_	_
36-20	10154-10155	(	_	_
36-21	10155-10160	https	_	_
36-22	10160-10161	:	_	_
36-23	10161-10162	/	_	_
36-24	10162-10163	/	_	_
36-25	10163-10173	github.com	_	_
36-26	10173-10174	/	_	_
36-27	10174-10181	allenai	_	_
36-28	10181-10182	/	_	_
36-29	10182-10188	aspire	_	_
36-30	10188-10189	/	_	_
36-31	10189-10193	blob	_	_
36-32	10193-10194	/	_	_
36-33	10194-10198	main	_	_
36-34	10198-10199	/	_	_
36-35	10199-10207	datasets	_	_
36-36	10207-10208	/	_	_
36-37	10208-10219	datasets.md	_	_
36-38	10219-10220	)	_	_
36-39	10223-10224	#	_	_
36-40	10224-10225	#	_	_
36-41	10225-10226	#	_	_
36-42	10227-10237	Repository	_	_
36-43	10238-10246	Contents	_	_
36-44	10247-10248	<	_	_
36-45	10248-10249	a	_	_
36-46	10250-10254	name	_	_
36-47	10254-10255	=	_	_
36-48	10255-10256	"	_	_
36-49	10256-10268	repocontents	_	_
36-50	10268-10269	"	_	_
36-51	10269-10270	>	_	_
36-52	10270-10271	<	_	_
36-53	10271-10272	/	_	_
36-54	10272-10273	a	_	_
36-55	10273-10274	>	_	_
36-56	10280-10281	├	_	_
36-57	10281-10282	─	_	_
36-58	10282-10283	─	_	_
36-59	10284-10287	bin	_	_
36-60	10292-10293	├	_	_
36-61	10293-10294	─	_	_
36-62	10294-10295	─	_	_
36-63	10296-10302	config	_	_
36-64	10307-10308	│	_	_
36-65	10321-10322	└	_	_
36-66	10322-10323	─	_	_
36-67	10323-10324	─	_	_
36-68	10325-10338	models\_config	_	_
36-69	10343-10344	│	_	_
36-70	10361-10362	├	_	_
36-71	10362-10363	─	_	_
36-72	10363-10364	─	_	_
36-73	10365-10376	s2orcbiomed	_	_
36-74	10381-10382	│	_	_
36-75	10399-10400	├	_	_
36-76	10400-10401	─	_	_
36-77	10401-10402	─	_	_
36-78	10403-10415	s2orccompsci	_	_
36-79	10420-10421	│	_	_
36-80	10438-10439	└	_	_
36-81	10439-10440	─	_	_
36-82	10440-10441	─	_	_
36-83	10442-10454	s2orcscidocs	_	_
36-84	10459-10460	├	_	_
36-85	10460-10461	─	_	_
36-86	10461-10462	─	_	_
36-87	10463-10470	scripts	_	_
36-88	10475-10476	└	_	_
36-89	10476-10477	─	_	_
36-90	10477-10478	─	_	_
36-91	10479-10482	src	_	_
36-92	10491-10492	├	_	_
36-93	10492-10493	─	_	_
36-94	10493-10494	─	_	_
36-95	10495-10505	evaluation	_	_
36-96	10514-10515	│	_	_
36-97	10528-10529	├	_	_
36-98	10529-10530	─	_	_
36-99	10530-10531	─	_	_
36-100	10532-10537	utils	_	_
36-101	10546-10547	│	_	_
36-102	10560-10561	│	_	_
36-103	10574-10575	├	_	_
36-104	10575-10576	─	_	_
36-105	10576-10577	─	_	_
36-106	10578-10589	datasets.py	_	_
36-107	10598-10599	│	_	_
36-108	10612-10613	│	_	_
36-109	10626-10627	├	_	_
36-110	10627-10628	─	_	_
36-111	10628-10629	─	_	_
36-112	10630-10640	metrics.py	_	_
36-113	10649-10650	│	_	_
36-114	10663-10664	│	_	_
36-115	10677-10678	├	_	_
36-116	10678-10679	─	_	_
36-117	10679-10680	─	_	_
36-118	10681-10690	models.py	_	_
36-119	10699-10700	│	_	_
36-120	10713-10714	│	_	_
36-121	10727-10728	└	_	_
36-122	10728-10729	─	_	_
36-123	10729-10730	─	_	_
36-124	10731-10739	utils.py	_	_
36-125	10748-10749	│	_	_
36-126	10762-10763	└	_	_
36-127	10763-10764	─	_	_
36-128	10764-10765	─	_	_
36-129	10766-10777	evaluate.py	_	_
36-130	10786-10787	├	_	_
36-131	10787-10788	─	_	_
36-132	10788-10789	─	_	_
36-133	10790-10798	learning	_	_
36-134	10807-10808	│	_	_
36-135	10821-10822	├	_	_
36-136	10822-10823	─	_	_
36-137	10823-10824	─	_	_
36-138	10825-10839	facetid\_models	_	_
36-139	10848-10849	│	_	_
36-140	10862-10863	│	_	_
36-141	10876-10877	├	_	_
36-142	10877-10878	─	_	_
36-143	10878-10879	─	_	_
36-144	10880-10896	disent\_models.py	_	_
36-145	10905-10906	│	_	_
36-146	10919-10920	│	_	_
36-147	10933-10934	├	_	_
36-148	10934-10935	─	_	_
36-149	10935-10936	─	_	_
36-150	10937-10954	pair\_distances.py	_	_
36-151	10963-10964	│	_	_
36-152	10977-10978	│	_	_
36-153	10991-10992	└	_	_
36-154	10992-10993	─	_	_
36-155	10993-10994	─	_	_
36-156	10995-11012	sentsim\_models.py	_	_
36-157	11021-11022	│	_	_
36-158	11035-11036	├	_	_
36-159	11036-11037	─	_	_
36-160	11037-11038	─	_	_
36-161	11039-11051	main\_fsim.py	_	_
36-162	11060-11061	│	_	_
36-163	11074-11075	├	_	_
36-164	11075-11076	─	_	_
36-165	11076-11077	─	_	_
36-166	11078-11089	batchers.py	_	_
36-167	11098-11099	│	_	_
36-168	11112-11113	└	_	_
36-169	11113-11114	─	_	_
36-170	11114-11115	─	_	_
36-171	11116-11126	trainer.py	_	_
36-172	11135-11136	└	_	_
36-173	11136-11137	─	_	_
36-174	11137-11138	─	_	_
36-175	11139-11150	pre\_process	_	_
36-176	11163-11164	├	_	_
36-177	11164-11165	─	_	_
36-178	11165-11166	─	_	_
36-179	11167-11186	extract\_entities.py	_	_
36-180	11199-11200	├	_	_
36-181	11200-11201	─	_	_
36-182	11201-11202	─	_	_
36-183	11203-11217	pp\_settings.py	_	_
36-184	11230-11231	├	_	_
36-185	11231-11232	─	_	_
36-186	11232-11233	─	_	_
36-187	11234-11252	pre\_proc\_cocits.py	_	_
36-188	11265-11266	├	_	_
36-189	11266-11267	─	_	_
36-190	11267-11268	─	_	_
36-191	11269-11285	pre\_proc\_gorc.py	_	_
36-192	11298-11299	├	_	_
36-193	11299-11300	─	_	_
36-194	11300-11301	─	_	_
36-195	11302-11320	pre\_proc\_relish.py	_	_
36-196	11333-11334	├	_	_
36-197	11334-11335	─	_	_
36-198	11335-11336	─	_	_
36-199	11337-11356	pre\_proc\_scidocs.py	_	_
36-200	11369-11370	├	_	_
36-201	11370-11371	─	_	_
36-202	11371-11372	─	_	_
36-203	11373-11394	pre\_proc\_treccovid.py	_	_
36-204	11407-11408	├	_	_
36-205	11408-11409	─	_	_
36-206	11409-11410	─	_	_
36-207	11411-11428	pp\_gen\_nearest.py	_	_
36-208	11441-11442	└	_	_
36-209	11442-11443	─	_	_
36-210	11443-11444	─	_	_
36-211	11445-11466	pre\_proc\_buildreps.py	_	_
36-212	11469-11470	\*	_	_
36-213	11470-11471	\*	_	_
36-214	11471-11474	The	_	_
36-215	11475-11485	repository	_	_
36-216	11486-11488	is	_	_
36-217	11489-11498	organized	_	_
36-218	11499-11506	broadly	_	_
36-219	11507-11509	as	_	_
36-220	11509-11510	:	_	_
36-221	11510-11511	\*	_	_
36-222	11511-11512	\*	_	_
36-223	11514-11515	`	_	_
36-224	11515-11518	src	_	_
36-225	11518-11519	/	_	_
36-226	11519-11530	pre\_process	_	_
36-227	11530-11531	/	_	_
36-228	11531-11532	`	_	_
36-229	11532-11533	:	_	_
36-230	11534-11541	Scripts	_	_
36-231	11542-11544	to	_	_
36-232	11545-11546	1	_	_
36-233	11546-11547	)	_	_
36-234	11548-11556	generate	_	_
36-235	11557-11563	gather	_	_
36-236	11564-11567	and	_	_
36-237	11568-11574	filter	_	_
36-238	11575-11587	co-citations	_	_
36-239	11588-11592	data	_	_
36-240	11593-11597	from	_	_
36-241	11598-11601	the	_	_
36-242	11602-11603	\[	_	_
36-243	11603-11608	S2ORC	_	_
36-244	11608-11609	\]	_	_
36-245	11609-11610	(	_	_
36-246	11610-11615	https	_	_
36-247	11615-11616	:	_	_
36-248	11616-11617	/	_	_
36-249	11617-11618	/	_	_
36-250	11618-11628	github.com	_	_
36-251	11628-11629	/	_	_
36-252	11629-11636	allenai	_	_
36-253	11636-11637	/	_	_
36-254	11637-11642	s2orc	_	_
36-255	11642-11643	)	_	_
36-256	11644-11650	corpus	_	_
36-257	11651-11652	2	_	_
36-258	11652-11653	)	_	_
36-259	11654-11662	generate	_	_
36-260	11663-11671	training	_	_
36-261	11672-11680	examples	_	_
36-262	11681-11685	with	_	_
36-263	11686-11697	co-citation	_	_
36-264	11698-11702	data	_	_
36-265	11703-11704	3	_	_
36-266	11704-11705	)	_	_
36-267	11706-11717	pre-process	_	_
36-268	11718-11721	the	_	_
36-269	11722-11732	evaluation	_	_
36-270	11733-11741	datasets	_	_
36-271	11742-11746	into	_	_
36-272	11747-11750	apt	_	_
36-273	11751-11758	formats	_	_
36-274	11759-11762	for	_	_
36-275	11763-11766	use	_	_
36-276	11767-11771	with	_	_
36-277	11772-11778	models	_	_
36-278	11779-11780	4	_	_
36-279	11780-11781	)	_	_
36-280	11782-11789	extract	_	_
36-281	11790-11793	NER	_	_
36-282	11794-11802	entities	_	_
36-283	11803-11807	from	_	_
36-284	11808-11816	datasets	_	_
36-285	11816-11817	.	_	_

#Text=`src/learning/`: Classes for implementing models, training, batching data, and a main script to train and save the model.
37-1	11819-11820	`	_	_
37-2	11820-11823	src	_	_
37-3	11823-11824	/	_	_
37-4	11824-11832	learning	_	_
37-5	11832-11833	/	_	_
37-6	11833-11834	`	_	_
37-7	11834-11835	:	_	_
37-8	11836-11843	Classes	_	_
37-9	11844-11847	for	_	_
37-10	11848-11860	implementing	_	_
37-11	11861-11867	models	_	_
37-12	11867-11868	,	_	_
37-13	11869-11877	training	_	_
37-14	11877-11878	,	_	_
37-15	11879-11887	batching	_	_
37-16	11888-11892	data	_	_
37-17	11892-11893	,	_	_
37-18	11894-11897	and	_	_
37-19	11898-11899	a	_	_
37-20	11900-11904	main	_	_
37-21	11905-11911	script	_	_
37-22	11912-11914	to	_	_
37-23	11915-11920	train	_	_
37-24	11921-11924	and	_	_
37-25	11925-11929	save	_	_
37-26	11930-11933	the	_	_
37-27	11934-11939	model	_	_
37-28	11939-11940	.	_	_

#Text=`src/evaluation/`: Scripts to evaluate model performances on various evaluation datasets.
38-1	11942-11943	`	_	_
38-2	11943-11946	src	_	_
38-3	11946-11947	/	_	_
38-4	11947-11957	evaluation	_	_
38-5	11957-11958	/	_	_
38-6	11958-11959	`	_	_
38-7	11959-11960	:	_	_
38-8	11961-11968	Scripts	_	_
38-9	11969-11971	to	_	_
38-10	11972-11980	evaluate	_	_
38-11	11981-11986	model	_	_
38-12	11987-11999	performances	_	_
38-13	12000-12002	on	_	_
38-14	12003-12010	various	_	_
38-15	12011-12021	evaluation	_	_
38-16	12022-12030	datasets	_	_
38-17	12030-12031	.	_	_

#Text=See `src/evaluation/evaluate.md` for help.
39-1	12032-12035	See	_	_
39-2	12036-12037	`	_	_
39-3	12037-12040	src	_	_
39-4	12040-12041	/	_	_
39-5	12041-12051	evaluation	_	_
39-6	12051-12052	/	_	_
39-7	12052-12063	evaluate.md	_	_
39-8	12063-12064	`	_	_
39-9	12065-12068	for	_	_
39-10	12069-12073	help	_	_
39-11	12073-12074	.	_	_

#Text=`config/models\_config`: JSON files with hyper-parameters for models in the paper consumed by code in `src/learning/`.
40-1	12076-12077	`	_	_
40-2	12077-12083	config	_	_
40-3	12083-12084	/	_	_
40-4	12084-12097	models\_config	_	_
40-5	12097-12098	`	_	_
40-6	12098-12099	:	_	_
40-7	12100-12104	JSON	_	_
40-8	12105-12110	files	_	_
40-9	12111-12115	with	_	_
40-10	12116-12132	hyper-parameters	_	_
40-11	12133-12136	for	_	_
40-12	12137-12143	models	_	_
40-13	12144-12146	in	_	_
40-14	12147-12150	the	_	_
40-15	12151-12156	paper	_	_
40-16	12157-12165	consumed	_	_
40-17	12166-12168	by	_	_
40-18	12169-12173	code	_	_
40-19	12174-12176	in	_	_
40-20	12177-12178	`	_	_
40-21	12178-12181	src	_	_
40-22	12181-12182	/	_	_
40-23	12182-12190	learning	_	_
40-24	12190-12191	/	_	_
40-25	12191-12192	`	_	_
40-26	12192-12193	.	_	_

#Text=Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain.
41-1	12194-12199	Since	_	_
41-2	12200-12202	we	_	_
41-3	12203-12211	evaluate	_	_
41-4	12212-12214	on	_	_
41-5	12215-12223	datasets	_	_
41-6	12224-12226	in	_	_
41-7	12227-12230	the	_	_
41-8	12231-12241	Biomedical	_	_
41-9	12242-12243	(	_	_
41-10	12243-12249	RELISH	_	_
41-11	12249-12250	,	_	_
41-12	12251-12263	TRECCOVID-RF	_	_
41-12	12251-12260	TRECCOVID	_	_
41-13	12263-12264	)	_	_
41-14	12264-12265	,	_	_
41-15	12266-12274	Computer	_	_
41-16	12275-12282	Science	_	_
41-17	12283-12284	(	_	_
41-18	12284-12291	CSFCube	_	_
41-19	12291-12292	)	_	_
41-20	12292-12293	,	_	_
41-21	12294-12297	and	_	_
41-22	12298-12303	mixed	_	_
41-23	12304-12311	domains	_	_
41-24	12312-12313	(	_	_
41-25	12313-12320	SciDocs	_	_
41-26	12320-12321	)	_	_
41-27	12322-12324	we	_	_
41-28	12325-12330	train	_	_
41-29	12331-12339	separate	_	_
41-30	12340-12346	models	_	_
41-31	12347-12350	for	_	_
41-32	12351-12356	these	_	_
41-33	12357-12364	domains	_	_
41-34	12364-12365	,	_	_
41-35	12366-12369	the	_	_
41-36	12370-12385	sub-directories	_	_
41-37	12386-12391	named	_	_
41-38	12392-12393	`	_	_
41-39	12393-12404	s2orcbiomed	_	_
41-40	12404-12405	`	_	_
41-41	12405-12406	,	_	_
41-42	12407-12408	`	_	_
41-43	12408-12420	s2orccompsci	_	_
41-44	12420-12421	`	_	_
41-45	12421-12422	,	_	_
41-46	12423-12426	and	_	_
41-47	12427-12428	`	_	_
41-48	12428-12440	s2orcscidocs	_	_
41-49	12440-12441	`	_	_
41-50	12442-12449	contain	_	_
41-51	12450-12456	config	_	_
41-52	12457-12462	files	_	_
41-53	12463-12466	for	_	_
41-54	12467-12470	the	_	_
41-55	12471-12477	models	_	_
41-56	12478-12485	trained	_	_
41-57	12486-12489	for	_	_
41-58	12490-12494	each	_	_
41-59	12495-12501	domain	_	_
41-60	12501-12502	.	_	_

#Text=`bin`: Shell scripts to call the scripts in all the `src` sub-directories with appropriate command line arguments.
42-1	12504-12505	`	_	_
42-2	12505-12508	bin	_	_
42-3	12508-12509	`	_	_
42-4	12509-12510	:	_	_
42-5	12511-12516	Shell	_	_
42-6	12517-12524	scripts	_	_
42-7	12525-12527	to	_	_
42-8	12528-12532	call	_	_
42-9	12533-12536	the	_	_
42-10	12537-12544	scripts	_	_
42-11	12545-12547	in	_	_
42-12	12548-12551	all	_	_
42-13	12552-12555	the	_	_
42-14	12556-12557	`	_	_
42-15	12557-12560	src	_	_
42-16	12560-12561	`	_	_
42-17	12562-12577	sub-directories	_	_
42-18	12578-12582	with	_	_
42-19	12583-12594	appropriate	_	_
42-20	12595-12602	command	_	_
42-21	12603-12607	line	_	_
42-22	12608-12617	arguments	_	_
42-23	12617-12618	.	_	_

#Text=`scripts`: Miscellaneous glue code.
43-1	12620-12621	`	_	_
43-2	12621-12628	scripts	_	_
43-3	12628-12629	`	_	_
43-4	12629-12630	:	_	_
43-5	12631-12644	Miscellaneous	_	_
43-6	12645-12649	glue	_	_
43-7	12650-12654	code	_	_
43-8	12654-12655	.	_	_

#Text=\*\*The following files are the main entry points into the repository:\*\*  `src/learning/main\_fsim.py`: The main script called from `bin/learning/run\_main\_fsim-ddp.sh` to initialize and train a model.
44-1	12657-12658	\*	_	_
44-2	12658-12659	\*	_	_
44-3	12659-12662	The	_	_
44-4	12663-12672	following	_	_
44-5	12673-12678	files	_	_
44-6	12679-12682	are	_	_
44-7	12683-12686	the	_	_
44-8	12687-12691	main	_	_
44-9	12692-12697	entry	_	_
44-10	12698-12704	points	_	_
44-11	12705-12709	into	_	_
44-12	12710-12713	the	_	_
44-13	12714-12724	repository	_	_
44-14	12724-12725	:	_	_
44-15	12725-12726	\*	_	_
44-16	12726-12727	\*	_	_
44-17	12729-12730	`	_	_
44-18	12730-12733	src	_	_
44-19	12733-12734	/	_	_
44-20	12734-12742	learning	_	_
44-21	12742-12743	/	_	_
44-22	12743-12755	main\_fsim.py	_	_
44-23	12755-12756	`	_	_
44-24	12756-12757	:	_	_
44-25	12758-12761	The	_	_
44-26	12762-12766	main	_	_
44-27	12767-12773	script	_	_
44-28	12774-12780	called	_	_
44-29	12781-12785	from	_	_
44-30	12786-12787	`	_	_
44-31	12787-12790	bin	_	_
44-32	12790-12791	/	_	_
44-33	12791-12799	learning	_	_
44-34	12799-12800	/	_	_
44-35	12800-12820	run\_main\_fsim-ddp.sh	_	_
44-36	12820-12821	`	_	_
44-37	12822-12824	to	_	_
44-38	12825-12835	initialize	_	_
44-39	12836-12839	and	_	_
44-40	12840-12845	train	_	_
44-41	12846-12847	a	_	_
44-42	12848-12853	model	_	_
44-43	12853-12854	.	_	_

#Text=The models consume json config files in `config/models\_config/{<domain>}`.
45-1	12855-12858	The	_	_
45-2	12859-12865	models	_	_
45-3	12866-12873	consume	_	_
45-4	12874-12878	json	_	_
45-5	12879-12885	config	_	_
45-6	12886-12891	files	_	_
45-7	12892-12894	in	_	_
45-8	12895-12896	`	_	_
45-9	12896-12902	config	_	_
45-10	12902-12903	/	_	_
45-11	12903-12916	models\_config	_	_
45-12	12916-12917	/	_	_
45-13	12917-12918	{	_	_
45-14	12918-12919	<	_	_
45-15	12919-12925	domain	_	_
45-16	12925-12926	>	_	_
45-17	12926-12927	}	_	_
45-18	12927-12928	`	_	_
45-19	12928-12929	.	_	_

#Text=A mapping from the model names/classes/configs in the repository to the models reported in the paper is as follows:  `src/evaluation/evaluate.py`: Contain code to generate rankings over the evaluation datasets.
46-1	12930-12931	A	_	_
46-2	12932-12939	mapping	_	_
46-3	12940-12944	from	_	_
46-4	12945-12948	the	_	_
46-5	12949-12954	model	_	_
46-6	12955-12960	names	_	_
46-7	12960-12961	/	_	_
46-8	12961-12968	classes	_	_
46-9	12968-12969	/	_	_
46-10	12969-12976	configs	_	_
46-11	12977-12979	in	_	_
46-12	12980-12983	the	_	_
46-13	12984-12994	repository	_	_
46-14	12995-12997	to	_	_
46-15	12998-13001	the	_	_
46-16	13002-13008	models	_	_
46-17	13009-13017	reported	_	_
46-18	13018-13020	in	_	_
46-19	13021-13024	the	_	_
46-20	13025-13030	paper	_	_
46-21	13031-13033	is	_	_
46-22	13034-13036	as	_	_
46-23	13037-13044	follows	_	_
46-24	13044-13045	:	_	_
46-25	13047-13048	`	_	_
46-26	13048-13051	src	_	_
46-27	13051-13052	/	_	_
46-28	13052-13062	evaluation	_	_
46-29	13062-13063	/	_	_
46-30	13063-13074	evaluate.py	_	_
46-31	13074-13075	`	_	_
46-32	13075-13076	:	_	_
46-33	13077-13084	Contain	_	_
46-34	13085-13089	code	_	_
46-35	13090-13092	to	_	_
46-36	13093-13101	generate	_	_
46-37	13102-13110	rankings	_	_
46-38	13111-13115	over	_	_
46-39	13116-13119	the	_	_
46-40	13120-13130	evaluation	_	_
46-41	13131-13139	datasets	_	_
46-42	13139-13140	.	_	_

#Text=Supports trained & downloaded models, and it is simple to add new models to the flow.
47-1	13141-13149	Supports	_	_
47-2	13150-13157	trained	_	_
47-3	13158-13159	&	_	_
47-4	13160-13170	downloaded	_	_
47-5	13171-13177	models	_	_
47-6	13177-13178	,	_	_
47-7	13179-13182	and	_	_
47-8	13183-13185	it	_	_
47-9	13186-13188	is	_	_
47-10	13189-13195	simple	_	_
47-11	13196-13198	to	_	_
47-12	13199-13202	add	_	_
47-13	13203-13206	new	_	_
47-14	13207-13213	models	_	_
47-15	13214-13216	to	_	_
47-16	13217-13220	the	_	_
47-17	13221-13225	flow	_	_
47-18	13225-13226	.	_	_

#Text=For instructions, read the detailed help snippets of argument parser.
48-1	13227-13230	For	_	_
48-2	13231-13243	instructions	_	_
48-3	13243-13244	,	_	_
48-4	13245-13249	read	_	_
48-5	13250-13253	the	_	_
48-6	13254-13262	detailed	_	_
48-7	13263-13267	help	_	_
48-8	13268-13276	snippets	_	_
48-9	13277-13279	of	_	_
48-10	13280-13288	argument	_	_
48-11	13289-13295	parser	_	_
48-12	13295-13296	.	_	_

#Text=`src/pre\_process/pre\_proc\_gorc.py:` Code to gather full text articles from the \[S2ORC\](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers).
49-1	13298-13299	`	_	_
49-2	13299-13302	src	_	_
49-3	13302-13303	/	_	_
49-4	13303-13314	pre\_process	_	_
49-5	13314-13315	/	_	_
49-6	13315-13331	pre\_proc\_gorc.py	_	_
49-7	13331-13332	:	_	_
49-8	13332-13333	`	_	_
49-9	13334-13338	Code	_	_
49-10	13339-13341	to	_	_
49-11	13342-13348	gather	_	_
49-12	13349-13353	full	_	_
49-13	13354-13358	text	_	_
49-14	13359-13367	articles	_	_
49-15	13368-13372	from	_	_
49-16	13373-13376	the	_	_
49-17	13377-13378	\[	_	_
49-18	13378-13383	S2ORC	_	_
49-19	13383-13384	\]	_	_
49-20	13384-13385	(	_	_
49-21	13385-13390	https	_	_
49-22	13390-13391	:	_	_
49-23	13391-13392	/	_	_
49-24	13392-13393	/	_	_
49-25	13393-13403	github.com	_	_
49-26	13403-13404	/	_	_
49-27	13404-13411	allenai	_	_
49-28	13411-13412	/	_	_
49-29	13412-13417	s2orc	_	_
49-30	13417-13418	)	_	_
49-31	13419-13425	corpus	_	_
49-32	13425-13426	,	_	_
49-33	13427-13434	exclude	_	_
49-34	13435-13440	noisy	_	_
49-35	13441-13445	data	_	_
49-36	13445-13446	,	_	_
49-37	13447-13450	and	_	_
49-38	13451-13457	gather	_	_
49-39	13458-13470	co-citations	_	_
49-40	13471-13474	for	_	_
49-41	13475-13484	different	_	_
49-42	13485-13492	domains	_	_
49-43	13493-13497	used	_	_
49-44	13498-13500	in	_	_
49-45	13501-13504	the	_	_
49-46	13505-13510	paper	_	_
49-47	13511-13512	(	_	_
49-48	13512-13522	biomedical	_	_
49-49	13523-13529	papers	_	_
49-50	13530-13533	and	_	_
49-51	13534-13542	computer	_	_
49-52	13543-13550	science	_	_
49-53	13551-13557	papers	_	_
49-54	13557-13558	)	_	_
49-55	13558-13559	.	_	_

#Text=This code assumes the 2019-09-28 release of S2ORC.
50-1	13560-13564	This	_	_
50-2	13565-13569	code	_	_
50-3	13570-13577	assumes	_	_
50-4	13578-13581	the	_	_
50-5	13582-13586	2019	_	_
50-6	13586-13587	-	_	_
50-7	13587-13589	09	_	_
50-8	13589-13590	-	_	_
50-9	13590-13592	28	_	_
50-10	13593-13600	release	_	_
50-11	13601-13603	of	_	_
50-12	13604-13609	S2ORC	_	_
50-13	13609-13610	.	_	_

#Text=`src/pre\_process/pre\_proc\_cocits.py:` Generate training data for the models reported in the paper.
51-1	13613-13614	`	_	_
51-2	13614-13617	src	_	_
51-3	13617-13618	/	_	_
51-4	13618-13629	pre\_process	_	_
51-5	13629-13630	/	_	_
51-6	13630-13648	pre\_proc\_cocits.py	_	_
51-7	13648-13649	:	_	_
51-8	13649-13650	`	_	_
51-9	13651-13659	Generate	_	_
51-10	13660-13668	training	_	_
51-11	13669-13673	data	_	_
51-12	13674-13677	for	_	_
51-13	13678-13681	the	_	_
51-14	13682-13688	models	_	_
51-15	13689-13697	reported	_	_
51-16	13698-13700	in	_	_
51-17	13701-13704	the	_	_
51-18	13705-13710	paper	_	_
51-19	13710-13711	.	_	_

#Text=Co-citations are used for training sentence level encoder models and whole abstract models, training data for both these model types are generated from functions in this script.
52-1	13712-13724	Co-citations	_	_
52-2	13725-13728	are	_	_
52-3	13729-13733	used	_	_
52-4	13734-13737	for	_	_
52-5	13738-13746	training	_	_
52-6	13747-13755	sentence	_	_
52-7	13756-13761	level	_	_
52-8	13762-13769	encoder	_	_
52-9	13770-13776	models	_	_
52-10	13777-13780	and	_	_
52-11	13781-13786	whole	_	_
52-12	13787-13795	abstract	_	_
52-13	13796-13802	models	_	_
52-14	13802-13803	,	_	_
52-15	13804-13812	training	_	_
52-16	13813-13817	data	_	_
52-17	13818-13821	for	_	_
52-18	13822-13826	both	_	_
52-19	13827-13832	these	_	_
52-20	13833-13838	model	_	_
52-21	13839-13844	types	_	_
52-22	13845-13848	are	_	_
52-23	13849-13858	generated	_	_
52-24	13859-13863	from	_	_
52-25	13864-13873	functions	_	_
52-26	13874-13876	in	_	_
52-27	13877-13881	this	_	_
52-28	13882-13888	script	_	_
52-29	13888-13889	.	_	_

#Text=These are the `filter\_cocitation\_sentences` and `filter\_cocitation\_papers` functions respectively.
53-1	13890-13895	These	_	_
53-2	13896-13899	are	_	_
53-3	13900-13903	the	_	_
53-4	13904-13905	`	_	_
53-5	13905-13932	filter\_cocitation\_sentences	_	_
53-6	13932-13933	`	_	_
53-7	13934-13937	and	_	_
53-8	13938-13939	`	_	_
53-9	13939-13963	filter\_cocitation\_papers	_	_
53-10	13963-13964	`	_	_
53-11	13965-13974	functions	_	_
53-12	13975-13987	respectively	_	_
53-13	13987-13988	.	_	_

#Text=Functions listed under `write\_examples` generate training positive pairs for various models (negatives are generated with in-batch negative sampling).
54-1	13989-13998	Functions	_	_
54-2	13999-14005	listed	_	_
54-3	14006-14011	under	_	_
54-4	14012-14013	`	_	_
54-5	14013-14027	write\_examples	_	_
54-6	14027-14028	`	_	_
54-7	14029-14037	generate	_	_
54-8	14038-14046	training	_	_
54-9	14047-14055	positive	_	_
54-10	14056-14061	pairs	_	_
54-11	14062-14065	for	_	_
54-12	14066-14073	various	_	_
54-13	14074-14080	models	_	_
54-14	14081-14082	(	_	_
54-15	14082-14091	negatives	_	_
54-16	14092-14095	are	_	_
54-17	14096-14105	generated	_	_
54-18	14106-14110	with	_	_
54-19	14111-14119	in-batch	_	_
54-20	14120-14128	negative	_	_
54-21	14129-14137	sampling	_	_
54-22	14137-14138	)	_	_
54-23	14138-14139	.	_	_

#Text=`src/pre\_process/pre\_proc\_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts.
55-1	14141-14142	`	_	_
55-2	14142-14145	src	_	_
55-3	14145-14146	/	_	_
55-4	14146-14157	pre\_process	_	_
55-5	14157-14158	/	_	_
55-6	14158-14166	pre\_proc	_	_
55-7	14166-14167	\_	_	_
55-8	14167-14168	{	_	_
55-9	14168-14174	relish	_	_
55-10	14174-14175	/	_	_
55-11	14175-14182	scidocs	_	_
55-12	14182-14183	/	_	_
55-13	14183-14192	treccovid	_	_
55-14	14192-14193	}	_	_
55-15	14193-14194	.	_	_
55-16	14194-14196	py	_	_
55-17	14196-14197	`	_	_
55-18	14197-14198	:	_	_
55-19	14199-14210	Pre-process	_	_
55-20	14211-14214	the	_	_
55-21	14215-14225	evaluation	_	_
55-22	14226-14234	datasets	_	_
55-23	14235-14236	(	_	_
55-24	14236-14242	RELISH	_	_
55-25	14242-14243	,	_	_
55-26	14244-14253	TRECCOVID	_	_
55-27	14253-14254	,	_	_
55-28	14255-14258	and	_	_
55-29	14259-14266	SciDocs	_	_
55-30	14266-14267	)	_	_
55-31	14268-14272	into	_	_
55-32	14273-14274	a	_	_
55-33	14275-14281	format	_	_
55-34	14282-14290	consumed	_	_
55-35	14291-14293	by	_	_
55-36	14294-14301	trained	_	_
55-37	14302-14308	models	_	_
55-38	14309-14312	and	_	_
55-39	14313-14323	evaluation	_	_
55-40	14324-14331	scripts	_	_
55-41	14331-14332	.	_	_

#Text=CSFCube data format matches the assumed format.
56-1	14333-14340	CSFCube	_	_
56-2	14341-14345	data	_	_
56-3	14346-14352	format	_	_
56-4	14353-14360	matches	_	_
56-5	14361-14364	the	_	_
56-6	14365-14372	assumed	_	_
56-7	14373-14379	format	_	_
56-8	14379-14380	.	_	_

#Text=Details about each dataset are as follows:  `src/pre\_process/extract\_entities.py`:  Use PURE's Entity Model () to extract named entities from abstracts.
57-1	14381-14388	Details	_	_
57-2	14389-14394	about	_	_
57-3	14395-14399	each	_	_
57-4	14400-14407	dataset	_	_
57-5	14408-14411	are	_	_
57-6	14412-14414	as	_	_
57-7	14415-14422	follows	_	_
57-8	14422-14423	:	_	_
57-9	14425-14426	`	_	_
57-10	14426-14429	src	_	_
57-11	14429-14430	/	_	_
57-12	14430-14441	pre\_process	_	_
57-13	14441-14442	/	_	_
57-14	14442-14461	extract\_entities.py	_	_
57-15	14461-14462	`	_	_
57-16	14462-14463	:	_	_
57-17	14465-14468	Use	_	_
57-18	14469-14475	PURE's	_	_
57-19	14476-14482	Entity	_	_
57-20	14483-14488	Model	_	_
57-21	14489-14490	(	_	_
57-22	14490-14491	)	_	_
57-23	14492-14494	to	_	_
57-24	14495-14502	extract	_	_
57-25	14503-14508	named	_	_
57-26	14509-14517	entities	_	_
57-27	14518-14522	from	_	_
57-28	14523-14532	abstracts	_	_
57-29	14532-14533	.	_	_

#Text=In some experiments, these are added to the abstract as additional sentences as an augmented input, improving results.
58-1	14534-14536	In	_	_
58-2	14537-14541	some	_	_
58-3	14542-14553	experiments	_	_
58-4	14553-14554	,	_	_
58-5	14555-14560	these	_	_
58-6	14561-14564	are	_	_
58-7	14565-14570	added	_	_
58-8	14571-14573	to	_	_
58-9	14574-14577	the	_	_
58-10	14578-14586	abstract	_	_
58-11	14587-14589	as	_	_
58-12	14590-14600	additional	_	_
58-13	14601-14610	sentences	_	_
58-14	14611-14613	as	_	_
58-15	14614-14616	an	_	_
58-16	14617-14626	augmented	_	_
58-17	14627-14632	input	_	_
58-18	14632-14633	,	_	_
58-19	14634-14643	improving	_	_
58-20	14644-14651	results	_	_
58-21	14651-14652	.	_	_

#Text=For info on how to run this file see src/pre\_process/README\_NER     <div style="margin-left: auto\;             margin-right: auto\;             width: 95%">  \| Model name in paper         \| Config under `config/models\_config/{<domain>}`  \| Model class in code   \| \|-----------------------------\|:-------------------------:\|:-----------:\| \| cosentbert              \|        `cosentbert`        \|  `facetid\_models.sentsim\_models.SentBERTWrapper` \| \| ICTSentBert              \|        `ictsentbert`        \|  `facetid\_models.sentsim\_models.ICTBERTWrapper` \| \| SPECTER-CoCite              \|        `hparam\_opt/cospecter-best`/`hparam\_opt/cospecter-specinit-best`        \|  `facetid\_models.disent\_models.MySPECTER`  \| \| tsAspire                    \|        `hparam\_opt/sbalisentbienc-sup-best`        \|        `facetid\_models.disent\_models.WordSentAbsSupAlignBiEnc`   \| \| otAspire                    \|        `hparam\_opt/miswordbienc-otstuni-best`        \|      `facetid\_models.disent\_models.WordSentAlignBiEnc`   \| \| ts+otAspire                 \|        `hparam\_opt/sbalisentbienc-otuni-best`        \|        `facetid\_models.disent\_models.WordSentAbsSupAlignBiEnc`   \| \| maxAspire                 \|          `hparam\_opt/miswordbienc-l2max-best`      \|        `facetid\_models.disent\_models.WordSentAlignBiEnc` \| \| absAspire                 \|          `hparam\_opt/sbalisentbienc-sup-absali-best`      \|        `facetid\_models.disent\_models.WordSentAbsSupAlignBiEnc`   \| \| attAspire                 \|          `hparam\_opt/miswordbienc-cdatt-best`      \|        `facetid\_models.disent\_models.WordSentAlignBiEnc`   \|  </div>   ### Acknowledgements <a name="acks"></a>  This work relies on: (1) Data from the \[Semantic Scholar Open Research Corpus\](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by \[Mariana Neves\](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of \[SPECTER\](https://github.com/allenai/specter). (3) The software packages: \[GeomLoss\](https://www.kernel-operations.io/geomloss/index.html) and \[sentence-transformers\](https://www.sbert.net/).   ### Citation <a name="citation"></a>  Please cite the \[Aspire paper\](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name="todos"></a>  1.
59-1	14653-14656	For	_	_
59-2	14657-14661	info	_	_
59-3	14662-14664	on	_	_
59-4	14665-14668	how	_	_
59-5	14669-14671	to	_	_
59-6	14672-14675	run	_	_
59-7	14676-14680	this	_	_
59-8	14681-14685	file	_	_
59-9	14686-14689	see	_	_
59-10	14690-14693	src	_	_
59-11	14693-14694	/	_	_
59-12	14694-14705	pre\_process	_	_
59-13	14705-14706	/	_	_
59-14	14706-14716	README\_NER	_	_
59-15	14721-14722	<	_	_
59-16	14722-14725	div	_	_
59-17	14726-14731	style	_	_
59-18	14731-14732	=	_	_
59-19	14732-14733	"	_	_
59-20	14733-14744	margin-left	_	_
59-21	14744-14745	:	_	_
59-22	14746-14750	auto	_	_
59-23	14750-14751	\;	_	_
59-24	14764-14776	margin-right	_	_
59-25	14776-14777	:	_	_
59-26	14778-14782	auto	_	_
59-27	14782-14783	\;	_	_
59-28	14796-14801	width	_	_
59-29	14801-14802	:	_	_
59-30	14803-14806	95%	_	_
59-31	14806-14807	"	_	_
59-32	14807-14808	>	_	_
59-33	14810-14811	\|	_	_
59-34	14812-14817	Model	_	_
59-35	14818-14822	name	_	_
59-36	14823-14825	in	_	_
59-37	14826-14831	paper	_	_
59-38	14840-14841	\|	_	_
59-39	14842-14848	Config	_	_
59-40	14849-14854	under	_	_
59-41	14855-14856	`	_	_
59-42	14856-14862	config	_	_
59-43	14862-14863	/	_	_
59-44	14863-14876	models\_config	_	_
59-45	14876-14877	/	_	_
59-46	14877-14878	{	_	_
59-47	14878-14879	<	_	_
59-48	14879-14885	domain	_	_
59-49	14885-14886	>	_	_
59-50	14886-14887	}	_	_
59-51	14887-14888	`	_	_
59-52	14890-14891	\|	_	_
59-53	14892-14897	Model	_	_
59-54	14898-14903	class	_	_
59-55	14904-14906	in	_	_
59-56	14907-14911	code	_	_
59-57	14914-14915	\|	_	_
59-58	14916-14917	\|	_	_
59-59	14917-14918	-	_	_
59-60	14918-14919	-	_	_
59-61	14919-14920	-	_	_
59-62	14920-14921	-	_	_
59-63	14921-14922	-	_	_
59-64	14922-14923	-	_	_
59-65	14923-14924	-	_	_
59-66	14924-14925	-	_	_
59-67	14925-14926	-	_	_
59-68	14926-14927	-	_	_
59-69	14927-14928	-	_	_
59-70	14928-14929	-	_	_
59-71	14929-14930	-	_	_
59-72	14930-14931	-	_	_
59-73	14931-14932	-	_	_
59-74	14932-14933	-	_	_
59-75	14933-14934	-	_	_
59-76	14934-14935	-	_	_
59-77	14935-14936	-	_	_
59-78	14936-14937	-	_	_
59-79	14937-14938	-	_	_
59-80	14938-14939	-	_	_
59-81	14939-14940	-	_	_
59-82	14940-14941	-	_	_
59-83	14941-14942	-	_	_
59-84	14942-14943	-	_	_
59-85	14943-14944	-	_	_
59-86	14944-14945	-	_	_
59-87	14945-14946	-	_	_
59-88	14946-14947	\|	_	_
59-89	14947-14948	:	_	_
59-90	14948-14949	-	_	_
59-91	14949-14950	-	_	_
59-92	14950-14951	-	_	_
59-93	14951-14952	-	_	_
59-94	14952-14953	-	_	_
59-95	14953-14954	-	_	_
59-96	14954-14955	-	_	_
59-97	14955-14956	-	_	_
59-98	14956-14957	-	_	_
59-99	14957-14958	-	_	_
59-100	14958-14959	-	_	_
59-101	14959-14960	-	_	_
59-102	14960-14961	-	_	_
59-103	14961-14962	-	_	_
59-104	14962-14963	-	_	_
59-105	14963-14964	-	_	_
59-106	14964-14965	-	_	_
59-107	14965-14966	-	_	_
59-108	14966-14967	-	_	_
59-109	14967-14968	-	_	_
59-110	14968-14969	-	_	_
59-111	14969-14970	-	_	_
59-112	14970-14971	-	_	_
59-113	14971-14972	-	_	_
59-114	14972-14973	-	_	_
59-115	14973-14974	:	_	_
59-116	14974-14975	\|	_	_
59-117	14975-14976	:	_	_
59-118	14976-14977	-	_	_
59-119	14977-14978	-	_	_
59-120	14978-14979	-	_	_
59-121	14979-14980	-	_	_
59-122	14980-14981	-	_	_
59-123	14981-14982	-	_	_
59-124	14982-14983	-	_	_
59-125	14983-14984	-	_	_
59-126	14984-14985	-	_	_
59-127	14985-14986	-	_	_
59-128	14986-14987	-	_	_
59-129	14987-14988	:	_	_
59-130	14988-14989	\|	_	_
59-131	14990-14991	\|	_	_
59-132	14992-15002	cosentbert	_	_
59-133	15016-15017	\|	_	_
59-134	15025-15026	`	_	_
59-135	15026-15036	cosentbert	_	_
59-136	15036-15037	`	_	_
59-137	15045-15046	\|	_	_
59-138	15048-15049	`	_	_
59-139	15049-15094	facetid\_models.sentsim\_models.SentBERTWrapper	_	_
59-140	15094-15095	`	_	_
59-141	15096-15097	\|	_	_
59-142	15098-15099	\|	_	_
59-143	15100-15111	ICTSentBert	_	_
59-144	15125-15126	\|	_	_
59-145	15134-15135	`	_	_
59-146	15135-15146	ictsentbert	_	_
59-147	15146-15147	`	_	_
59-148	15155-15156	\|	_	_
59-149	15158-15159	`	_	_
59-150	15159-15203	facetid\_models.sentsim\_models.ICTBERTWrapper	_	_
59-151	15203-15204	`	_	_
59-152	15205-15206	\|	_	_
59-153	15207-15208	\|	_	_
59-154	15209-15223	SPECTER-CoCite	_	_
59-155	15237-15238	\|	_	_
59-156	15246-15247	`	_	_
59-157	15247-15257	hparam\_opt	_	_
59-158	15257-15258	/	_	_
59-159	15258-15272	cospecter-best	_	_
59-160	15272-15273	`	_	_
59-161	15273-15274	/	_	_
59-162	15274-15275	`	_	_
59-163	15275-15285	hparam\_opt	_	_
59-164	15285-15286	/	_	_
59-165	15286-15309	cospecter-specinit-best	_	_
59-166	15309-15310	`	_	_
59-167	15318-15319	\|	_	_
59-168	15321-15322	`	_	_
59-169	15322-15360	facetid\_models.disent\_models.MySPECTER	_	_
59-170	15360-15361	`	_	_
59-171	15363-15364	\|	_	_
59-172	15365-15366	\|	_	_
59-173	15367-15375	tsAspire	_	_
59-174	15395-15396	\|	_	_
59-175	15404-15405	`	_	_
59-176	15405-15415	hparam\_opt	_	_
59-177	15415-15416	/	_	_
59-178	15416-15439	sbalisentbienc-sup-best	_	_
59-179	15439-15440	`	_	_
59-180	15448-15449	\|	_	_
59-181	15457-15458	`	_	_
59-182	15458-15511	facetid\_models.disent\_models.WordSentAbsSupAlignBiEnc	_	_
59-183	15511-15512	`	_	_
59-184	15515-15516	\|	_	_
59-185	15517-15518	\|	_	_
59-186	15519-15527	otAspire	_	_
59-187	15547-15548	\|	_	_
59-188	15556-15557	`	_	_
59-189	15557-15567	hparam\_opt	_	_
59-190	15567-15568	/	_	_
59-191	15568-15593	miswordbienc-otstuni-best	_	_
59-192	15593-15594	`	_	_
59-193	15602-15603	\|	_	_
59-194	15609-15610	`	_	_
59-195	15610-15657	facetid\_models.disent\_models.WordSentAlignBiEnc	_	_
59-196	15657-15658	`	_	_
59-197	15661-15662	\|	_	_
59-198	15663-15664	\|	_	_
59-199	15665-15667	ts	_	_
59-200	15667-15668	+	_	_
59-201	15668-15676	otAspire	_	_
59-202	15693-15694	\|	_	_
59-203	15702-15703	`	_	_
59-204	15703-15713	hparam\_opt	_	_
59-205	15713-15714	/	_	_
59-206	15714-15739	sbalisentbienc-otuni-best	_	_
59-207	15739-15740	`	_	_
59-208	15748-15749	\|	_	_
59-209	15757-15758	`	_	_
59-210	15758-15811	facetid\_models.disent\_models.WordSentAbsSupAlignBiEnc	_	_
59-211	15811-15812	`	_	_
59-212	15815-15816	\|	_	_
59-213	15817-15818	\|	_	_
59-214	15819-15828	maxAspire	_	_
59-215	15845-15846	\|	_	_
59-216	15856-15857	`	_	_
59-217	15857-15867	hparam\_opt	_	_
59-218	15867-15868	/	_	_
59-219	15868-15891	miswordbienc-l2max-best	_	_
59-220	15891-15892	`	_	_
59-221	15898-15899	\|	_	_
59-222	15907-15908	`	_	_
59-223	15908-15955	facetid\_models.disent\_models.WordSentAlignBiEnc	_	_
59-224	15955-15956	`	_	_
59-225	15957-15958	\|	_	_
59-226	15959-15960	\|	_	_
59-227	15961-15970	absAspire	_	_
59-228	15987-15988	\|	_	_
59-229	15998-15999	`	_	_
59-230	15999-16009	hparam\_opt	_	_
59-231	16009-16010	/	_	_
59-232	16010-16040	sbalisentbienc-sup-absali-best	_	_
59-233	16040-16041	`	_	_
59-234	16047-16048	\|	_	_
59-235	16056-16057	`	_	_
59-236	16057-16110	facetid\_models.disent\_models.WordSentAbsSupAlignBiEnc	_	_
59-237	16110-16111	`	_	_
59-238	16114-16115	\|	_	_
59-239	16116-16117	\|	_	_
59-240	16118-16127	attAspire	_	_
59-241	16144-16145	\|	_	_
59-242	16155-16156	`	_	_
59-243	16156-16166	hparam\_opt	_	_
59-244	16166-16167	/	_	_
59-245	16167-16190	miswordbienc-cdatt-best	_	_
59-246	16190-16191	`	_	_
59-247	16197-16198	\|	_	_
59-248	16206-16207	`	_	_
59-249	16207-16254	facetid\_models.disent\_models.WordSentAlignBiEnc	_	_
59-250	16254-16255	`	_	_
59-251	16258-16259	\|	_	_
59-252	16261-16262	<	_	_
59-253	16262-16263	/	_	_
59-254	16263-16266	div	_	_
59-255	16266-16267	>	_	_
59-256	16270-16271	#	_	_
59-257	16271-16272	#	_	_
59-258	16272-16273	#	_	_
59-259	16274-16290	Acknowledgements	_	_
59-260	16291-16292	<	_	_
59-261	16292-16293	a	_	_
59-262	16294-16298	name	_	_
59-263	16298-16299	=	_	_
59-264	16299-16300	"	_	_
59-265	16300-16304	acks	_	_
59-266	16304-16305	"	_	_
59-267	16305-16306	>	_	_
59-268	16306-16307	<	_	_
59-269	16307-16308	/	_	_
59-270	16308-16309	a	_	_
59-271	16309-16310	>	_	_
59-272	16312-16316	This	_	_
59-273	16317-16321	work	_	_
59-274	16322-16328	relies	_	_
59-275	16329-16331	on	_	_
59-276	16331-16332	:	_	_
59-277	16333-16334	(	_	_
59-278	16334-16335	1	_	_
59-279	16335-16336	)	_	_
59-280	16337-16341	Data	_	_
59-281	16342-16346	from	_	_
59-282	16347-16350	the	_	_
59-283	16351-16352	\[	_	_
59-284	16352-16360	Semantic	_	_
59-285	16361-16368	Scholar	_	_
59-286	16369-16373	Open	_	_
59-287	16374-16382	Research	_	_
59-288	16383-16389	Corpus	_	_
59-289	16389-16390	\]	_	_
59-290	16390-16391	(	_	_
59-291	16391-16396	https	_	_
59-292	16396-16397	:	_	_
59-293	16397-16398	/	_	_
59-294	16398-16399	/	_	_
59-295	16399-16409	github.com	_	_
59-296	16409-16410	/	_	_
59-297	16410-16417	allenai	_	_
59-298	16417-16418	/	_	_
59-299	16418-16423	s2orc	_	_
59-300	16423-16424	)	_	_
59-301	16425-16426	(	_	_
59-302	16426-16431	S2ORC	_	_
59-303	16431-16432	)	_	_
59-304	16433-16436	and	_	_
59-305	16437-16440	the	_	_
59-306	16441-16451	evaluation	_	_
59-307	16452-16460	datasets	_	_
59-308	16461-16467	RELISH	_	_
59-309	16468-16469	(	_	_
59-310	16469-16475	kindly	_	_
59-311	16476-16482	shared	_	_
59-312	16483-16485	by	_	_
59-313	16486-16487	\[	_	_
59-314	16487-16494	Mariana	_	_
59-315	16495-16500	Neves	_	_
59-316	16500-16501	\]	_	_
59-317	16501-16502	(	_	_
59-318	16502-16507	https	_	_
59-319	16507-16508	:	_	_
59-320	16508-16509	/	_	_
59-321	16509-16510	/	_	_
59-322	16510-16532	mariananeves.github.io	_	_
59-323	16532-16533	/	_	_
59-324	16533-16534	)	_	_
59-325	16534-16535	)	_	_
59-326	16535-16536	,	_	_
59-327	16537-16546	TRECCOVID	_	_
59-328	16546-16547	,	_	_
59-329	16548-16555	SciDocs	_	_
59-330	16555-16556	,	_	_
59-331	16557-16560	and	_	_
59-332	16561-16568	CSFCube	_	_
59-333	16569-16575	linked	_	_
59-334	16576-16581	above	_	_
59-335	16581-16582	.	_	_
59-336	16583-16584	(	_	_
59-337	16584-16585	2	_	_
59-338	16585-16586	)	_	_
59-339	16587-16590	The	_	_
59-340	16591-16602	pre-trained	_	_
59-341	16603-16609	models	_	_
59-342	16610-16612	of	_	_
59-343	16613-16614	\[	_	_
59-344	16614-16621	SPECTER	_	_
59-345	16621-16622	\]	_	_
59-346	16622-16623	(	_	_
59-347	16623-16628	https	_	_
59-348	16628-16629	:	_	_
59-349	16629-16630	/	_	_
59-350	16630-16631	/	_	_
59-351	16631-16641	github.com	_	_
59-352	16641-16642	/	_	_
59-353	16642-16649	allenai	_	_
59-354	16649-16650	/	_	_
59-355	16650-16657	specter	_	_
59-356	16657-16658	)	_	_
59-357	16658-16659	.	_	_
59-358	16660-16661	(	_	_
59-359	16661-16662	3	_	_
59-360	16662-16663	)	_	_
59-361	16664-16667	The	_	_
59-362	16668-16676	software	_	_
59-363	16677-16685	packages	_	_
59-364	16685-16686	:	_	_
59-365	16687-16688	\[	_	_
59-366	16688-16696	GeomLoss	_	_
59-367	16696-16697	\]	_	_
59-368	16697-16698	(	_	_
59-369	16698-16703	https	_	_
59-370	16703-16704	:	_	_
59-371	16704-16705	/	_	_
59-372	16705-16706	/	_	_
59-373	16706-16730	www.kernel-operations.io	_	_
59-374	16730-16731	/	_	_
59-375	16731-16739	geomloss	_	_
59-376	16739-16740	/	_	_
59-377	16740-16750	index.html	_	_
59-378	16750-16751	)	_	_
59-379	16752-16755	and	_	_
59-380	16756-16757	\[	_	_
59-381	16757-16778	sentence-transformers	_	_
59-382	16778-16779	\]	_	_
59-383	16779-16780	(	_	_
59-384	16780-16785	https	_	_
59-385	16785-16786	:	_	_
59-386	16786-16787	/	_	_
59-387	16787-16788	/	_	_
59-388	16788-16801	www.sbert.net	_	_
59-389	16801-16802	/	_	_
59-390	16802-16803	)	_	_
59-391	16803-16804	.	_	_
59-392	16807-16808	#	_	_
59-393	16808-16809	#	_	_
59-394	16809-16810	#	_	_
59-395	16811-16819	Citation	_	_
59-396	16820-16821	<	_	_
59-397	16821-16822	a	_	_
59-398	16823-16827	name	_	_
59-399	16827-16828	=	_	_
59-400	16828-16829	"	_	_
59-401	16829-16837	citation	_	_
59-402	16837-16838	"	_	_
59-403	16838-16839	>	_	_
59-404	16839-16840	<	_	_
59-405	16840-16841	/	_	_
59-406	16841-16842	a	_	_
59-407	16842-16843	>	_	_
59-408	16845-16851	Please	_	_
59-409	16852-16856	cite	_	_
59-410	16857-16860	the	_	_
59-411	16861-16862	\[	_	_
59-412	16862-16868	Aspire	_	_
59-413	16869-16874	paper	_	_
59-414	16874-16875	\]	_	_
59-415	16875-16876	(	_	_
59-416	16876-16881	https	_	_
59-417	16881-16882	:	_	_
59-418	16882-16883	/	_	_
59-419	16883-16884	/	_	_
59-420	16884-16893	arxiv.org	_	_
59-421	16893-16894	/	_	_
59-422	16894-16897	pdf	_	_
59-423	16897-16898	/	_	_
59-424	16898-16908	2004.07180	_	_
59-425	16908-16909	.	_	_
59-426	16909-16912	pdf	_	_
59-427	16912-16913	)	_	_
59-428	16914-16916	as	_	_
59-429	16916-16917	:	_	_
59-430	16921-16922	`	_	_
59-431	16922-16923	`	_	_
59-432	16923-16924	`	_	_
59-433	16924-16930	bibtex	_	_
59-434	16931-16932	@	_	_
59-435	16932-16936	misc	_	_
59-436	16936-16937	{	_	_
59-437	16937-16953	mysore2021aspire	_	_
59-438	16953-16954	,	_	_
59-439	16961-16966	title	_	_
59-440	16966-16967	=	_	_
59-441	16967-16968	{	_	_
59-442	16968-16980	Multi-Vector	_	_
59-443	16981-16987	Models	_	_
59-444	16988-16992	with	_	_
59-445	16993-17000	Textual	_	_
59-446	17001-17009	Guidance	_	_
59-447	17010-17013	for	_	_
59-448	17014-17026	Fine-Grained	_	_
59-449	17027-17037	Scientific	_	_
59-450	17038-17046	Document	_	_
59-451	17047-17057	Similarity	_	_
59-452	17057-17058	}	_	_
59-453	17058-17059	,	_	_
59-454	17067-17073	author	_	_
59-455	17073-17074	=	_	_
59-456	17074-17075	{	_	_
59-457	17075-17083	Sheshera	_	_
59-458	17084-17090	Mysore	_	_
59-459	17091-17094	and	_	_
59-460	17095-17100	Arman	_	_
59-461	17101-17106	Cohan	_	_
59-462	17107-17110	and	_	_
59-463	17111-17114	Tom	_	_
59-464	17115-17119	Hope	_	_
59-465	17119-17120	}	_	_
59-466	17120-17121	,	_	_
59-467	17128-17132	year	_	_
59-468	17132-17133	=	_	_
59-469	17133-17134	{	_	_
59-470	17134-17138	2021	_	_
59-471	17138-17139	}	_	_
59-472	17139-17140	,	_	_
59-473	17147-17153	eprint	_	_
59-474	17153-17154	=	_	_
59-475	17154-17155	{	_	_
59-476	17155-17165	2111.08366	_	_
59-477	17165-17166	}	_	_
59-478	17166-17167	,	_	_
59-479	17174-17187	archivePrefix	_	_
59-480	17187-17188	=	_	_
59-481	17188-17189	{	_	_
59-482	17189-17194	arXiv	_	_
59-483	17194-17195	}	_	_
59-484	17195-17196	,	_	_
59-485	17203-17215	primaryClass	_	_
59-486	17215-17216	=	_	_
59-487	17216-17217	{	_	_
59-488	17217-17222	cs.CL	_	_
59-489	17222-17223	}	_	_
59-490	17224-17225	}	_	_
59-491	17226-17227	`	_	_
59-492	17227-17228	`	_	_
59-493	17228-17229	`	_	_
59-494	17232-17233	#	_	_
59-495	17233-17234	#	_	_
59-496	17234-17235	#	_	_
59-497	17236-17241	TODOs	_	_
59-498	17242-17243	<	_	_
59-499	17243-17244	a	_	_
59-500	17245-17249	name	_	_
59-501	17249-17250	=	_	_
59-502	17250-17251	"	_	_
59-503	17251-17256	todos	_	_
59-504	17256-17257	"	_	_
59-505	17257-17258	>	_	_
59-506	17258-17259	<	_	_
59-507	17259-17260	/	_	_
59-508	17260-17261	a	_	_
59-509	17261-17262	>	_	_
59-510	17264-17265	1	_	_
59-511	17265-17266	.	_	_

#Text=Release trained model parameters.
60-1	17267-17274	Release	_	_
60-2	17275-17282	trained	_	_
60-3	17283-17288	model	_	_
60-4	17289-17299	parameters	_	_
60-5	17299-17300	.	_	_

#Text=(in-progress)     - Currently released models are \_per-domain\_ models for computer science and biomedical papers which were used in the paper.
61-1	17301-17302	(	_	_
61-2	17302-17313	in-progress	_	_
61-3	17313-17314	)	_	_
61-4	17319-17320	-	_	_
61-5	17321-17330	Currently	_	_
61-6	17331-17339	released	_	_
61-7	17340-17346	models	_	_
61-8	17347-17350	are	_	_
61-9	17351-17352	\_	_	_
61-10	17352-17362	per-domain	_	_
61-11	17362-17363	\_	_	_
61-12	17364-17370	models	_	_
61-13	17371-17374	for	_	_
61-14	17375-17383	computer	_	_
61-15	17384-17391	science	_	_
61-16	17392-17395	and	_	_
61-17	17396-17406	biomedical	_	_
61-18	17407-17413	papers	_	_
61-19	17414-17419	which	_	_
61-20	17420-17424	were	_	_
61-21	17425-17429	used	_	_
61-22	17430-17432	in	_	_
61-23	17433-17436	the	_	_
61-24	17437-17442	paper	_	_
61-25	17442-17443	.	_	_

#Text=The coming months will also see release of domain independent models trained on data across different scientific domains. 2.
62-1	17444-17447	The	_	_
62-2	17448-17454	coming	_	_
62-3	17455-17461	months	_	_
62-4	17462-17466	will	_	_
62-5	17467-17471	also	_	_
62-6	17472-17475	see	_	_
62-7	17476-17483	release	_	_
62-8	17484-17486	of	_	_
62-9	17487-17493	domain	_	_
62-10	17494-17505	independent	_	_
62-11	17506-17512	models	_	_
62-12	17513-17520	trained	_	_
62-13	17521-17523	on	_	_
62-14	17524-17528	data	_	_
62-15	17529-17535	across	_	_
62-16	17536-17545	different	_	_
62-17	17546-17556	scientific	_	_
62-18	17557-17564	domains	_	_
62-19	17564-17565	.	_	_
62-20	17566-17567	2	_	_
62-21	17567-17568	.	_	_

#Text=Release training training data
63-1	17569-17576	Release	_	_
63-2	17577-17585	training	_	_
63-3	17586-17594	training	_	_
63-4	17595-17599	data	_	_

#Text=.
64-1	17599-17600	.	_	_

#Text=- Co-citation data used to train the above model will also be released, this is co-citation pairs on the order of a few million pairs of papers. 3.
65-1	17605-17606	-	_	_
65-2	17607-17618	Co-citation	_	_
65-3	17619-17623	data	_	_
65-4	17624-17628	used	_	_
65-5	17629-17631	to	_	_
65-6	17632-17637	train	_	_
65-7	17638-17641	the	_	_
65-8	17642-17647	above	_	_
65-9	17648-17653	model	_	_
65-10	17654-17658	will	_	_
65-11	17659-17663	also	_	_
65-12	17664-17666	be	_	_
65-13	17667-17675	released	_	_
65-14	17675-17676	,	_	_
65-15	17677-17681	this	_	_
65-16	17682-17684	is	_	_
65-17	17685-17696	co-citation	_	_
65-18	17697-17702	pairs	_	_
65-19	17703-17705	on	_	_
65-20	17706-17709	the	_	_
65-21	17710-17715	order	_	_
65-22	17716-17718	of	_	_
65-23	17719-17720	a	_	_
65-24	17721-17724	few	_	_
65-25	17725-17732	million	_	_
65-26	17733-17738	pairs	_	_
65-27	17739-17741	of	_	_
65-28	17742-17748	papers	_	_
65-29	17748-17749	.	_	_
65-30	17750-17751	3	_	_
65-31	17751-17752	.	_	_

#Text=Training code usage instructions
66-1	17753-17761	Training	_	_
66-2	17762-17766	code	_	_
66-3	17767-17772	usage	_	_
66-4	17773-17785	instructions	_	_

#Text=.
67-1	17785-17786	.	_	_

#Text=- This will be released for reproducibility.
68-1	17791-17792	-	_	_
68-2	17793-17797	This	_	_
68-3	17798-17802	will	_	_
68-4	17803-17805	be	_	_
68-5	17806-17814	released	_	_
68-6	17815-17818	for	_	_
68-7	17819-17834	reproducibility	_	_
68-8	17834-17835	.	_	_