#FORMAT=WebAnno TSV 3.3
#T_SP=de.tudarmstadt.ukp.dkpro.core.api.ner.type.NamedEntity|identifier|value


#Text=# Keyword Transformer: A Self-Attention Model for Keyword Spotting  <img src="kwt.png" alt="drawing" width="200"/>  This is the official repository for the paper \[Keyword Transformer: A Self-Attention Model for Keyword Spotting\](https://arxiv.org/abs/2104.00769), presented at Interspeech 2021.
1-1	0-1	#	_	_
1-2	2-9	Keyword	_	_
1-3	10-21	Transformer	_	_
1-4	21-22	:	_	_
1-5	23-24	A	_	_
1-6	25-39	Self-Attention	_	_
1-7	40-45	Model	_	_
1-8	46-49	for	_	_
1-9	50-57	Keyword	_	_
1-10	58-66	Spotting	_	_
1-11	68-69	<	_	_
1-12	69-72	img	_	_
1-13	73-76	src	_	_
1-14	76-77	=	_	_
1-15	77-78	"	_	_
1-16	78-85	kwt.png	_	_
1-17	85-86	"	_	_
1-18	87-90	alt	_	_
1-19	90-91	=	_	_
1-20	91-92	"	_	_
1-21	92-99	drawing	_	_
1-22	99-100	"	_	_
1-23	101-106	width	_	_
1-24	106-107	=	_	_
1-25	107-108	"	_	_
1-26	108-111	200	_	_
1-27	111-112	"	_	_
1-28	112-113	/	_	_
1-29	113-114	>	_	_
1-30	116-120	This	_	_
1-31	121-123	is	_	_
1-32	124-127	the	_	_
1-33	128-136	official	_	_
1-34	137-147	repository	_	_
1-35	148-151	for	_	_
1-36	152-155	the	_	_
1-37	156-161	paper	_	_
1-38	162-163	\[	_	_
1-39	163-170	Keyword	_	_
1-40	171-182	Transformer	_	_
1-41	182-183	:	_	_
1-42	184-185	A	_	_
1-43	186-200	Self-Attention	_	_
1-44	201-206	Model	_	_
1-45	207-210	for	_	_
1-46	211-218	Keyword	_	_
1-47	219-227	Spotting	_	_
1-48	227-228	\]	_	_
1-49	228-229	(	_	_
1-50	229-234	https	_	_
1-51	234-235	:	_	_
1-52	235-236	/	_	_
1-53	236-237	/	_	_
1-54	237-246	arxiv.org	_	_
1-55	246-247	/	_	_
1-56	247-250	abs	_	_
1-57	250-251	/	_	_
1-58	251-261	2104.00769	_	_
1-59	261-262	)	_	_
1-60	262-263	,	_	_
1-61	264-273	presented	_	_
1-62	274-276	at	_	_
1-63	277-288	Interspeech	_	_
1-64	289-293	2021	_	_
1-65	293-294	.	_	_

#Text=Consider citing our paper if you find this work useful.  ``` @inproceedings{berg21\_interspeech,   author={Axel Berg and Mark O’Connor and Miguel Tairum Cruz},   title={{Keyword Transformer: A Self-Attention Model for Keyword Spotting}},   year=2021,   booktitle={Proc.
2-1	295-303	Consider	_	_
2-2	304-310	citing	_	_
2-3	311-314	our	_	_
2-4	315-320	paper	_	_
2-5	321-323	if	_	_
2-6	324-327	you	_	_
2-7	328-332	find	_	_
2-8	333-337	this	_	_
2-9	338-342	work	_	_
2-10	343-349	useful	_	_
2-11	349-350	.	_	_
2-12	352-353	`	_	_
2-13	353-354	`	_	_
2-14	354-355	`	_	_
2-15	356-357	@	_	_
2-16	357-370	inproceedings	_	_
2-17	370-371	{	_	_
2-18	371-377	berg21	_	_
2-19	377-378	\_	_	_
2-20	378-389	interspeech	_	_
2-21	389-390	,	_	_
2-22	393-399	author	_	_
2-23	399-400	=	_	_
2-24	400-401	{	_	_
2-25	401-405	Axel	_	_
2-26	406-410	Berg	_	_
2-27	411-414	and	_	_
2-28	415-419	Mark	_	_
2-29	420-421	O	_	_
2-30	421-422	’	_	_
2-31	422-428	Connor	_	_
2-32	429-432	and	_	_
2-33	433-439	Miguel	_	_
2-34	440-446	Tairum	_	_
2-35	447-451	Cruz	_	_
2-36	451-452	}	_	_
2-37	452-453	,	_	_
2-38	456-461	title	_	_
2-39	461-462	=	_	_
2-40	462-463	{	_	_
2-41	463-464	{	_	_
2-42	464-471	Keyword	_	_
2-43	472-483	Transformer	_	_
2-44	483-484	:	_	_
2-45	485-486	A	_	_
2-46	487-501	Self-Attention	_	_
2-47	502-507	Model	_	_
2-48	508-511	for	_	_
2-49	512-519	Keyword	_	_
2-50	520-528	Spotting	_	_
2-51	528-529	}	_	_
2-52	529-530	}	_	_
2-53	530-531	,	_	_
2-54	534-538	year	_	_
2-55	538-539	=	_	_
2-56	539-543	2021	_	_
2-57	543-544	,	_	_
2-58	547-556	booktitle	_	_
2-59	556-557	=	_	_
2-60	557-558	{	_	_
2-61	558-562	Proc	_	_
2-62	562-563	.	_	_

#Text=Interspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2.
3-1	564-575	Interspeech	_	_
3-2	576-580	2021	_	_
3-3	580-581	}	_	_
3-4	581-582	,	_	_
3-5	585-590	pages	_	_
3-6	590-591	=	_	_
3-7	591-592	{	_	_
3-8	592-596	4249	_	_
3-9	596-597	-	_	_
3-10	597-598	-	_	_
3-11	598-602	4253	_	_
3-12	602-603	}	_	_
3-13	603-604	,	_	_
3-14	607-610	doi	_	_
3-15	610-611	=	_	_
3-16	611-612	{	_	_
3-17	612-620	10.21437	_	_
3-18	620-621	/	_	_
3-19	621-632	Interspeech	_	_
3-20	632-637	.2021	_	_
3-21	637-638	-	_	_
3-22	638-642	1286	_	_
3-23	642-643	}	_	_
3-24	644-645	}	_	_
3-25	646-647	`	_	_
3-26	647-648	`	_	_
3-27	648-649	`	_	_
3-28	651-652	#	_	_
3-29	652-653	#	_	_
3-30	654-659	Setup	_	_
3-31	661-662	#	_	_
3-32	662-663	#	_	_
3-33	663-664	#	_	_
3-34	665-673	Download	_	_
3-35	674-680	Google	_	_
3-36	681-687	Speech	_	_
3-37	688-696	Commands	_	_
3-38	698-703	There	_	_
3-39	704-707	are	_	_
3-40	708-711	two	_	_
3-41	712-720	versions	_	_
3-42	721-723	of	_	_
3-43	724-727	the	_	_
3-44	728-735	dataset	_	_
3-45	735-736	,	_	_
3-46	737-739	V1	_	_
3-47	740-743	and	_	_
3-48	744-746	V2	_	_
3-49	746-747	.	_	_

#Text=To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech\_commands\_v0.02.tar.gz mkdir data2 mv .
4-1	748-750	To	_	_
4-2	751-759	download	_	_
4-3	760-763	and	_	_
4-4	764-771	extract	_	_
4-5	772-779	dataset	_	_
4-6	780-782	V2	_	_
4-7	782-783	,	_	_
4-8	784-787	run	_	_
4-9	787-788	:	_	_
4-10	790-791	`	_	_
4-11	791-792	`	_	_
4-12	792-793	`	_	_
4-13	793-798	shell	_	_
4-14	799-803	wget	_	_
4-15	804-809	https	_	_
4-16	809-810	:	_	_
4-17	810-811	/	_	_
4-18	811-812	/	_	_
4-19	812-834	storage.googleapis.com	_	_
4-20	834-835	/	_	_
4-21	835-858	download.tensorflow.org	_	_
4-21	844-854	tensorflow	_	_
4-22	858-859	/	_	_
4-23	859-863	data	_	_
4-24	863-864	/	_	_
4-25	864-885	speech\_commands\_v0.02	_	_
4-26	885-886	.	_	_
4-27	886-892	tar.gz	_	_
4-28	893-898	mkdir	_	_
4-29	899-904	data2	_	_
4-30	905-907	mv	_	_
4-31	908-909	.	_	_

#Text=/speech\_commands\_v0.02.tar.gz .
5-1	909-910	/	_	_
5-2	910-931	speech\_commands\_v0.02	_	_
5-3	931-932	.	_	_
5-4	932-938	tar.gz	_	_
5-5	939-940	.	_	_

#Text=/data2 cd .
6-1	940-941	/	_	_
6-2	941-946	data2	_	_
6-3	947-949	cd	_	_
6-4	950-951	.	_	_

#Text=/data2 tar -xf .
7-1	951-952	/	_	_
7-2	952-957	data2	_	_
7-3	958-961	tar	_	_
7-4	962-963	-	_	_
7-5	963-965	xf	_	_
7-6	966-967	.	_	_

#Text=/speech\_commands\_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech\_commands\_v0.01.tar.gz mkdir data1 mv .
8-1	967-968	/	_	_
8-2	968-989	speech\_commands\_v0.02	_	_
8-3	989-990	.	_	_
8-4	990-996	tar.gz	_	_
8-5	997-999	cd	_	_
8-6	1000-1001	.	_	_
8-7	1001-1002	.	_	_
8-8	1002-1003	/	_	_
8-9	1004-1005	`	_	_
8-10	1005-1006	`	_	_
8-11	1006-1007	`	_	_
8-12	1009-1012	And	_	_
8-13	1013-1022	similarly	_	_
8-14	1023-1026	for	_	_
8-15	1027-1029	V1	_	_
8-16	1029-1030	:	_	_
8-17	1032-1033	`	_	_
8-18	1033-1034	`	_	_
8-19	1034-1035	`	_	_
8-20	1035-1040	shell	_	_
8-21	1041-1045	wget	_	_
8-22	1046-1050	http	_	_
8-23	1050-1051	:	_	_
8-24	1051-1052	/	_	_
8-25	1052-1053	/	_	_
8-26	1053-1076	download.tensorflow.org	_	_
8-26	1062-1072	tensorflow	_	_
8-27	1076-1077	/	_	_
8-28	1077-1081	data	_	_
8-29	1081-1082	/	_	_
8-30	1082-1103	speech\_commands\_v0.01	_	_
8-31	1103-1104	.	_	_
8-32	1104-1110	tar.gz	_	_
8-33	1111-1116	mkdir	_	_
8-34	1117-1122	data1	_	_
8-35	1123-1125	mv	_	_
8-36	1126-1127	.	_	_

#Text=/speech\_commands\_v0.01.tar.gz .
9-1	1127-1128	/	_	_
9-2	1128-1149	speech\_commands\_v0.01	_	_
9-3	1149-1150	.	_	_
9-4	1150-1156	tar.gz	_	_
9-5	1157-1158	.	_	_

#Text=/data1 cd .
10-1	1158-1159	/	_	_
10-2	1159-1164	data1	_	_
10-3	1165-1167	cd	_	_
10-4	1168-1169	.	_	_

#Text=/data1 tar -xf .
11-1	1169-1170	/	_	_
11-2	1170-1175	data1	_	_
11-3	1176-1179	tar	_	_
11-4	1180-1181	-	_	_
11-5	1181-1183	xf	_	_
11-6	1184-1185	.	_	_

#Text=/speech\_commands\_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .
12-1	1185-1186	/	_	_
12-2	1186-1207	speech\_commands\_v0.01	_	_
12-3	1207-1208	.	_	_
12-4	1208-1214	tar.gz	_	_
12-5	1215-1217	cd	_	_
12-6	1218-1219	.	_	_
12-7	1219-1220	.	_	_
12-8	1220-1221	/	_	_
12-9	1222-1223	`	_	_
12-10	1223-1224	`	_	_
12-11	1224-1225	`	_	_
12-12	1227-1228	#	_	_
12-13	1228-1229	#	_	_
12-14	1229-1230	#	_	_
12-15	1231-1238	Install	_	_
12-16	1239-1251	dependencies	_	_
12-17	1253-1256	Set	_	_
12-18	1257-1259	up	_	_
12-19	1260-1261	a	_	_
12-20	1262-1265	new	_	_
12-21	1266-1273	virtual	_	_
12-22	1274-1285	environment	_	_
12-23	1285-1286	:	_	_
12-24	1288-1289	`	_	_
12-25	1289-1290	`	_	_
12-26	1290-1291	`	_	_
12-27	1291-1296	shell	_	_
12-28	1297-1300	pip	_	_
12-29	1301-1308	install	_	_
12-30	1309-1319	virtualenv	_	_
12-31	1320-1330	virtualenv	_	_
12-32	1331-1332	-	_	_
12-33	1332-1333	-	_	_
12-34	1333-1353	system-site-packages	_	_
12-35	1354-1355	-	_	_
12-36	1355-1356	p	_	_
12-37	1357-1364	python3	_	_
12-38	1365-1366	.	_	_

#Text=/venv3 source .
13-1	1366-1367	/	_	_
13-2	1367-1372	venv3	_	_
13-3	1373-1379	source	_	_
13-4	1380-1381	.	_	_

#Text=/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using Tensorflow 2.4.0rc1 with CUDA 11.
14-1	1381-1382	/	_	_
14-2	1382-1387	venv3	_	_
14-3	1387-1388	/	_	_
14-4	1388-1391	bin	_	_
14-5	1391-1392	/	_	_
14-6	1392-1400	activate	_	_
14-7	1401-1402	`	_	_
14-8	1402-1403	`	_	_
14-9	1403-1404	`	_	_
14-10	1406-1408	To	_	_
14-11	1409-1416	install	_	_
14-12	1417-1429	dependencies	_	_
14-13	1429-1430	,	_	_
14-14	1431-1434	run	_	_
14-15	1436-1437	`	_	_
14-16	1437-1438	`	_	_
14-17	1438-1439	`	_	_
14-18	1439-1444	shell	_	_
14-19	1445-1448	pip	_	_
14-20	1449-1456	install	_	_
14-21	1457-1458	-	_	_
14-22	1458-1459	r	_	_
14-23	1460-1476	requirements.txt	_	_
14-24	1477-1478	`	_	_
14-25	1478-1479	`	_	_
14-26	1479-1480	`	_	_
14-27	1482-1488	Tested	_	_
14-28	1489-1494	using	_	_
14-29	1495-1505	Tensorflow	_	_
14-30	1506-1514	2.4.0rc1	_	_
14-31	1515-1519	with	_	_
14-32	1520-1524	CUDA	_	_
14-33	1525-1527	11	_	_
14-34	1527-1528	.	_	_

#Text=\*\*Note\*\*: Installing the correct Tensorflow version is important for reproducibility!
15-1	1530-1531	\*	_	_
15-2	1531-1532	\*	_	_
15-3	1532-1536	Note	_	_
15-4	1536-1537	\*	_	_
15-5	1537-1538	\*	_	_
15-6	1538-1539	:	_	_
15-7	1540-1550	Installing	_	_
15-8	1551-1554	the	_	_
15-9	1555-1562	correct	_	_
15-10	1563-1573	Tensorflow	_	_
15-11	1574-1581	version	_	_
15-12	1582-1584	is	_	_
15-13	1585-1594	important	_	_
15-14	1595-1598	for	_	_
15-15	1599-1614	reproducibility	_	_
15-16	1614-1615	!	_	_

#Text=Using more recent versions of Tensorflow results in small accuracy differences each time the model is evaluated.
16-1	1616-1621	Using	_	_
16-2	1622-1626	more	_	_
16-3	1627-1633	recent	_	_
16-4	1634-1642	versions	_	_
16-5	1643-1645	of	_	_
16-6	1646-1656	Tensorflow	_	_
16-7	1657-1664	results	_	_
16-8	1665-1667	in	_	_
16-9	1668-1673	small	_	_
16-10	1674-1682	accuracy	_	_
16-11	1683-1694	differences	_	_
16-12	1695-1699	each	_	_
16-13	1700-1704	time	_	_
16-14	1705-1708	the	_	_
16-15	1709-1714	model	_	_
16-16	1715-1717	is	_	_
16-17	1718-1727	evaluated	_	_
16-18	1727-1728	.	_	_

#Text=This might be due to a change in how the random seed generator is implemented, and therefore changes the sampling of the "unknown"  keyword class.  ## Model The Keyword-Transformer model is defined \[here\](kws\_streaming/models/kws\_transformer.py).
17-1	1729-1733	This	_	_
17-2	1734-1739	might	_	_
17-3	1740-1742	be	_	_
17-4	1743-1746	due	_	_
17-5	1747-1749	to	_	_
17-6	1750-1751	a	_	_
17-7	1752-1758	change	_	_
17-8	1759-1761	in	_	_
17-9	1762-1765	how	_	_
17-10	1766-1769	the	_	_
17-11	1770-1776	random	_	_
17-12	1777-1781	seed	_	_
17-13	1782-1791	generator	_	_
17-14	1792-1794	is	_	_
17-15	1795-1806	implemented	_	_
17-16	1806-1807	,	_	_
17-17	1808-1811	and	_	_
17-18	1812-1821	therefore	_	_
17-19	1822-1829	changes	_	_
17-20	1830-1833	the	_	_
17-21	1834-1842	sampling	_	_
17-22	1843-1845	of	_	_
17-23	1846-1849	the	_	_
17-24	1850-1851	"	_	_
17-25	1851-1858	unknown	_	_
17-26	1858-1859	"	_	_
17-27	1861-1868	keyword	_	_
17-28	1869-1874	class	_	_
17-29	1874-1875	.	_	_
17-30	1877-1878	#	_	_
17-31	1878-1879	#	_	_
17-32	1880-1885	Model	_	_
17-33	1886-1889	The	_	_
17-34	1890-1909	Keyword-Transformer	_	_
17-35	1910-1915	model	_	_
17-36	1916-1918	is	_	_
17-37	1919-1926	defined	_	_
17-38	1927-1928	\[	_	_
17-39	1928-1932	here	_	_
17-40	1932-1933	\]	_	_
17-41	1933-1934	(	_	_
17-42	1934-1947	kws\_streaming	_	_
17-43	1947-1948	/	_	_
17-44	1948-1954	models	_	_
17-45	1954-1955	/	_	_
17-46	1955-1973	kws\_transformer.py	_	_
17-47	1973-1974	)	_	_
17-48	1974-1975	.	_	_

#Text=It takes the mel scale spectrogram as input, which has shape 98 x 40 using the default settings, corresponding to the 98 time windows with 40 frequency coefficients.
18-1	1976-1978	It	_	_
18-2	1979-1984	takes	_	_
18-3	1985-1988	the	_	_
18-4	1989-1992	mel	_	_
18-5	1993-1998	scale	_	_
18-6	1999-2010	spectrogram	_	_
18-7	2011-2013	as	_	_
18-8	2014-2019	input	_	_
18-9	2019-2020	,	_	_
18-10	2021-2026	which	_	_
18-11	2027-2030	has	_	_
18-12	2031-2036	shape	_	_
18-13	2037-2039	98	_	_
18-14	2040-2041	x	_	_
18-15	2042-2044	40	_	_
18-16	2045-2050	using	_	_
18-17	2051-2054	the	_	_
18-18	2055-2062	default	_	_
18-19	2063-2071	settings	_	_
18-20	2071-2072	,	_	_
18-21	2073-2086	corresponding	_	_
18-22	2087-2089	to	_	_
18-23	2090-2093	the	_	_
18-24	2094-2096	98	_	_
18-25	2097-2101	time	_	_
18-26	2102-2109	windows	_	_
18-27	2110-2114	with	_	_
18-28	2115-2117	40	_	_
18-29	2118-2127	frequency	_	_
18-30	2128-2140	coefficients	_	_
18-31	2140-2141	.	_	_

#Text=There are three variants of the Keyword-Transformer model:  \* \*\*Time-domain attention\*\*: each time-window is treated as a patch, self-attention is computed between time-windows \* \*\*Frequency-domain attention\*\*: each frequency is treated as a patch self-attention is computed between frequencies \* \*\*Combination of both\*\*: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined \* \*\*Patch-wise attention\*\*: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train\_dir`) cannot exist prior to start script.
19-1	2143-2148	There	_	_
19-2	2149-2152	are	_	_
19-3	2153-2158	three	_	_
19-4	2159-2167	variants	_	_
19-5	2168-2170	of	_	_
19-6	2171-2174	the	_	_
19-7	2175-2194	Keyword-Transformer	_	_
19-8	2195-2200	model	_	_
19-9	2200-2201	:	_	_
19-10	2203-2204	\*	_	_
19-11	2205-2206	\*	_	_
19-12	2206-2207	\*	_	_
19-13	2207-2218	Time-domain	_	_
19-14	2219-2228	attention	_	_
19-15	2228-2229	\*	_	_
19-16	2229-2230	\*	_	_
19-17	2230-2231	:	_	_
19-18	2232-2236	each	_	_
19-19	2237-2248	time-window	_	_
19-20	2249-2251	is	_	_
19-21	2252-2259	treated	_	_
19-22	2260-2262	as	_	_
19-23	2263-2264	a	_	_
19-24	2265-2270	patch	_	_
19-25	2270-2271	,	_	_
19-26	2272-2286	self-attention	_	_
19-27	2287-2289	is	_	_
19-28	2290-2298	computed	_	_
19-29	2299-2306	between	_	_
19-30	2307-2319	time-windows	_	_
19-31	2320-2321	\*	_	_
19-32	2322-2323	\*	_	_
19-33	2323-2324	\*	_	_
19-34	2324-2340	Frequency-domain	_	_
19-35	2341-2350	attention	_	_
19-36	2350-2351	\*	_	_
19-37	2351-2352	\*	_	_
19-38	2352-2353	:	_	_
19-39	2354-2358	each	_	_
19-40	2359-2368	frequency	_	_
19-41	2369-2371	is	_	_
19-42	2372-2379	treated	_	_
19-43	2380-2382	as	_	_
19-44	2383-2384	a	_	_
19-45	2385-2390	patch	_	_
19-46	2391-2405	self-attention	_	_
19-47	2406-2408	is	_	_
19-48	2409-2417	computed	_	_
19-49	2418-2425	between	_	_
19-50	2426-2437	frequencies	_	_
19-51	2438-2439	\*	_	_
19-52	2440-2441	\*	_	_
19-53	2441-2442	\*	_	_
19-54	2442-2453	Combination	_	_
19-55	2454-2456	of	_	_
19-56	2457-2461	both	_	_
19-57	2461-2462	\*	_	_
19-58	2462-2463	\*	_	_
19-59	2463-2464	:	_	_
19-60	2465-2468	The	_	_
19-61	2469-2475	signal	_	_
19-62	2476-2478	is	_	_
19-63	2479-2482	fed	_	_
19-64	2483-2487	into	_	_
19-65	2488-2492	both	_	_
19-66	2493-2494	a	_	_
19-67	2495-2499	time	_	_
19-68	2499-2500	-	_	_
19-69	2501-2504	and	_	_
19-70	2505-2506	a	_	_
19-71	2507-2523	frequency-domain	_	_
19-72	2524-2535	transformer	_	_
19-73	2536-2539	and	_	_
19-74	2540-2543	the	_	_
19-75	2544-2551	outputs	_	_
19-76	2552-2555	are	_	_
19-77	2556-2564	combined	_	_
19-78	2565-2566	\*	_	_
19-79	2567-2568	\*	_	_
19-80	2568-2569	\*	_	_
19-81	2569-2579	Patch-wise	_	_
19-82	2580-2589	attention	_	_
19-83	2589-2590	\*	_	_
19-84	2590-2591	\*	_	_
19-85	2591-2592	:	_	_
19-86	2593-2600	Similar	_	_
19-87	2601-2603	to	_	_
19-88	2604-2607	the	_	_
19-89	2608-2614	vision	_	_
19-90	2615-2626	transformer	_	_
19-91	2626-2627	,	_	_
19-92	2628-2630	it	_	_
19-93	2631-2639	extracts	_	_
19-94	2640-2651	rectangular	_	_
19-95	2652-2659	patches	_	_
19-96	2660-2664	from	_	_
19-97	2665-2668	the	_	_
19-98	2669-2680	spectrogram	_	_
19-99	2680-2681	,	_	_
19-100	2682-2684	so	_	_
19-101	2685-2694	attention	_	_
19-102	2695-2702	happens	_	_
19-103	2703-2707	both	_	_
19-104	2708-2710	in	_	_
19-105	2711-2714	the	_	_
19-106	2715-2719	time	_	_
19-107	2720-2723	and	_	_
19-108	2724-2733	frequency	_	_
19-109	2734-2740	domain	_	_
19-110	2741-2755	simultaneously	_	_
19-111	2755-2756	.	_	_
19-112	2758-2759	#	_	_
19-113	2759-2760	#	_	_
19-114	2761-2769	Training	_	_
19-115	2770-2771	a	_	_
19-116	2772-2777	model	_	_
19-117	2778-2782	from	_	_
19-118	2783-2790	scratch	_	_
19-119	2791-2793	To	_	_
19-120	2794-2799	train	_	_
19-121	2800-2803	KWT	_	_
19-122	2803-2804	-	_	_
19-123	2804-2805	3	_	_
19-124	2806-2810	from	_	_
19-125	2811-2818	scratch	_	_
19-126	2819-2821	on	_	_
19-127	2822-2828	Speech	_	_
19-128	2829-2837	Commands	_	_
19-129	2838-2840	V2	_	_
19-130	2840-2841	,	_	_
19-131	2842-2845	run	_	_
19-132	2849-2850	`	_	_
19-133	2850-2851	`	_	_
19-134	2851-2852	`	_	_
19-135	2852-2857	shell	_	_
19-136	2858-2860	sh	_	_
19-137	2861-2869	train.sh	_	_
19-138	2870-2871	`	_	_
19-139	2871-2872	`	_	_
19-140	2872-2873	`	_	_
19-141	2875-2881	Please	_	_
19-142	2882-2886	note	_	_
19-143	2887-2891	that	_	_
19-144	2892-2895	the	_	_
19-145	2896-2901	train	_	_
19-146	2902-2911	directory	_	_
19-147	2912-2913	(	_	_
19-148	2913-2918	given	_	_
19-149	2919-2921	by	_	_
19-150	2922-2925	the	_	_
19-151	2926-2934	argument	_	_
19-152	2936-2937	`	_	_
19-153	2937-2938	-	_	_
19-154	2938-2939	-	_	_
19-155	2939-2948	train\_dir	_	_
19-156	2948-2949	`	_	_
19-157	2949-2950	)	_	_
19-158	2951-2957	cannot	_	_
19-159	2958-2963	exist	_	_
19-160	2964-2969	prior	_	_
19-161	2970-2972	to	_	_
19-162	2973-2978	start	_	_
19-163	2979-2985	script	_	_
19-164	2985-2986	.	_	_

#Text=The model-specific arguments for KWT are:  ```shell --num\_layers 12 \\ #number of sequential transformer encoders --heads 3 \\ #number of attentions heads --d\_model 192 \\ #embedding dimension --mlp\_dim 768 \\ #mlp-dimension --dropout1 0. \\ #dropout in mlp/multi-head attention blocks --attention\_type 'time' \\ #attention type: 'time', 'freq', 'both' or 'patch' --patch\_size '1,40' \\ #spectrogram patch\_size, if patch attention is used --prenorm False \\ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in \[DeIT\](https://github.com/facebookresearch/deit).
20-1	2988-2991	The	_	_
20-2	2992-3006	model-specific	_	_
20-3	3007-3016	arguments	_	_
20-4	3017-3020	for	_	_
20-5	3021-3024	KWT	_	_
20-6	3025-3028	are	_	_
20-7	3028-3029	:	_	_
20-8	3031-3032	`	_	_
20-9	3032-3033	`	_	_
20-10	3033-3034	`	_	_
20-11	3034-3039	shell	_	_
20-12	3040-3041	-	_	_
20-13	3041-3042	-	_	_
20-14	3042-3052	num\_layers	_	_
20-15	3053-3055	12	_	_
20-16	3056-3057	\\	_	_
20-17	3058-3059	#	_	_
20-18	3059-3065	number	_	_
20-19	3066-3068	of	_	_
20-20	3069-3079	sequential	_	_
20-21	3080-3091	transformer	_	_
20-22	3092-3100	encoders	_	_
20-23	3101-3102	-	_	_
20-24	3102-3103	-	_	_
20-25	3103-3108	heads	_	_
20-26	3109-3110	3	_	_
20-27	3111-3112	\\	_	_
20-28	3113-3114	#	_	_
20-29	3114-3120	number	_	_
20-30	3121-3123	of	_	_
20-31	3124-3134	attentions	_	_
20-32	3135-3140	heads	_	_
20-33	3141-3142	-	_	_
20-34	3142-3143	-	_	_
20-35	3143-3150	d\_model	_	_
20-36	3151-3154	192	_	_
20-37	3155-3156	\\	_	_
20-38	3157-3158	#	_	_
20-39	3158-3167	embedding	_	_
20-40	3168-3177	dimension	_	_
20-41	3178-3179	-	_	_
20-42	3179-3180	-	_	_
20-43	3180-3187	mlp\_dim	_	_
20-44	3188-3191	768	_	_
20-45	3192-3193	\\	_	_
20-46	3194-3195	#	_	_
20-47	3195-3208	mlp-dimension	_	_
20-48	3209-3210	-	_	_
20-49	3210-3211	-	_	_
20-50	3211-3219	dropout1	_	_
20-51	3220-3221	0	_	_
20-52	3221-3222	.	_	_
20-53	3223-3224	\\	_	_
20-54	3225-3226	#	_	_
20-55	3226-3233	dropout	_	_
20-56	3234-3236	in	_	_
20-57	3237-3240	mlp	_	_
20-58	3240-3241	/	_	_
20-59	3241-3251	multi-head	_	_
20-60	3252-3261	attention	_	_
20-61	3262-3268	blocks	_	_
20-62	3269-3270	-	_	_
20-63	3270-3271	-	_	_
20-64	3271-3285	attention\_type	_	_
20-65	3286-3287	'	_	_
20-66	3287-3291	time	_	_
20-67	3291-3292	'	_	_
20-68	3293-3294	\\	_	_
20-69	3295-3296	#	_	_
20-70	3296-3305	attention	_	_
20-71	3306-3310	type	_	_
20-72	3310-3311	:	_	_
20-73	3312-3313	'	_	_
20-74	3313-3317	time	_	_
20-75	3317-3318	'	_	_
20-76	3318-3319	,	_	_
20-77	3320-3321	'	_	_
20-78	3321-3325	freq	_	_
20-79	3325-3326	'	_	_
20-80	3326-3327	,	_	_
20-81	3328-3329	'	_	_
20-82	3329-3333	both	_	_
20-83	3333-3334	'	_	_
20-84	3335-3337	or	_	_
20-85	3338-3339	'	_	_
20-86	3339-3344	patch	_	_
20-87	3344-3345	'	_	_
20-88	3346-3347	-	_	_
20-89	3347-3348	-	_	_
20-90	3348-3358	patch\_size	_	_
20-91	3359-3360	'	_	_
20-92	3360-3364	1,40	_	_
20-93	3364-3365	'	_	_
20-94	3366-3367	\\	_	_
20-95	3368-3369	#	_	_
20-96	3369-3380	spectrogram	_	_
20-97	3381-3391	patch\_size	_	_
20-98	3391-3392	,	_	_
20-99	3393-3395	if	_	_
20-100	3396-3401	patch	_	_
20-101	3402-3411	attention	_	_
20-102	3412-3414	is	_	_
20-103	3415-3419	used	_	_
20-104	3420-3421	-	_	_
20-105	3421-3422	-	_	_
20-106	3422-3429	prenorm	_	_
20-107	3430-3435	False	_	_
20-108	3436-3437	\\	_	_
20-109	3438-3439	#	_	_
20-110	3440-3442	if	_	_
20-111	3443-3448	False	_	_
20-112	3448-3449	,	_	_
20-113	3450-3453	use	_	_
20-114	3454-3462	postnorm	_	_
20-115	3463-3464	`	_	_
20-116	3464-3465	`	_	_
20-117	3465-3466	`	_	_
20-118	3468-3469	#	_	_
20-119	3469-3470	#	_	_
20-120	3471-3479	Training	_	_
20-121	3480-3484	with	_	_
20-122	3485-3497	distillation	_	_
20-123	3499-3501	We	_	_
20-124	3502-3508	employ	_	_
20-125	3509-3513	hard	_	_
20-126	3514-3526	distillation	_	_
20-127	3527-3531	from	_	_
20-128	3532-3533	a	_	_
20-129	3534-3547	convolutional	_	_
20-130	3548-3553	model	_	_
20-131	3554-3555	(	_	_
20-132	3555-3565	Att-MH-RNN	_	_
20-133	3565-3566	)	_	_
20-134	3566-3567	,	_	_
20-135	3568-3575	similar	_	_
20-136	3576-3578	to	_	_
20-137	3579-3582	the	_	_
20-138	3583-3591	approach	_	_
20-139	3592-3594	in	_	_
20-140	3595-3596	\[	_	_
20-141	3596-3600	DeIT	_	_
20-142	3600-3601	\]	_	_
20-143	3601-3602	(	_	_
20-144	3602-3607	https	_	_
20-145	3607-3608	:	_	_
20-146	3608-3609	/	_	_
20-147	3609-3610	/	_	_
20-148	3610-3620	github.com	_	_
20-149	3620-3621	/	_	_
20-150	3621-3637	facebookresearch	_	_
20-151	3637-3638	/	_	_
20-152	3638-3642	deit	_	_
20-153	3642-3643	)	_	_
20-154	3643-3644	.	_	_

#Text=To train KWT-3 with hard distillation from a pre-trained model, run  ```shell sh distill.sh ```  ## Run inference using a pre-trained model  Pre-trained weights for KWT-3, KWT-2 and KWT-1 are provided in .
21-1	3646-3648	To	_	_
21-2	3649-3654	train	_	_
21-3	3655-3658	KWT	_	_
21-4	3658-3659	-	_	_
21-5	3659-3660	3	_	_
21-6	3661-3665	with	_	_
21-7	3666-3670	hard	_	_
21-8	3671-3683	distillation	_	_
21-9	3684-3688	from	_	_
21-10	3689-3690	a	_	_
21-11	3691-3702	pre-trained	_	_
21-12	3703-3708	model	_	_
21-13	3708-3709	,	_	_
21-14	3710-3713	run	_	_
21-15	3715-3716	`	_	_
21-16	3716-3717	`	_	_
21-17	3717-3718	`	_	_
21-18	3718-3723	shell	_	_
21-19	3724-3726	sh	_	_
21-20	3727-3737	distill.sh	_	_
21-21	3738-3739	`	_	_
21-22	3739-3740	`	_	_
21-23	3740-3741	`	_	_
21-24	3743-3744	#	_	_
21-25	3744-3745	#	_	_
21-26	3746-3749	Run	_	_
21-27	3750-3759	inference	_	_
21-28	3760-3765	using	_	_
21-29	3766-3767	a	_	_
21-30	3768-3779	pre-trained	_	_
21-31	3780-3785	model	_	_
21-32	3787-3798	Pre-trained	_	_
21-33	3799-3806	weights	_	_
21-34	3807-3810	for	_	_
21-35	3811-3814	KWT	_	_
21-36	3814-3815	-	_	_
21-37	3815-3816	3	_	_
21-38	3816-3817	,	_	_
21-39	3818-3821	KWT	_	_
21-40	3821-3822	-	_	_
21-41	3822-3823	2	_	_
21-42	3824-3827	and	_	_
21-43	3828-3831	KWT	_	_
21-44	3831-3832	-	_	_
21-45	3832-3833	1	_	_
21-46	3834-3837	are	_	_
21-47	3838-3846	provided	_	_
21-48	3847-3849	in	_	_
21-49	3850-3851	.	_	_

#Text=/models\_data\_v2\_12\_labels.
22-1	3851-3852	/	_	_
22-2	3852-3866	models\_data\_v2	_	_
22-3	3866-3867	\_	_	_
22-4	3867-3869	12	_	_
22-5	3869-3870	\_	_	_
22-6	3870-3876	labels	_	_
22-7	3876-3877	.	_	_

#Text=\|Model name\|embedding dim\|mlp-dim\|heads\|depth\|#params\|V2-12 accuracy\|pre-trained\| \|:-:\|:-:\|:-:\|:-:\|:-:\|:-:\|:-:\|:-:\| \|KWT-1\|64\|128\|1\|12\|607K\|97.7\|\[here\](models\_data\_v2\_12\_labels/kwt1)\| \|KWT-2\|128\|256\|2\|12\|2.4M\|98.2\|\[here\](models\_data\_v2\_12\_labels/kwt2)\| \|KWT-3\|192\|768\|3\|12\|5.5M\|98.7\|\[here\](models\_data\_v2\_12\_labels/kwt3)\|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the \[KWS streaming work\](https://github.com/google-research/google-research/tree/master/kws\_streaming) by Google Research.
23-1	3879-3880	\|	_	_
23-2	3880-3885	Model	_	_
23-3	3886-3890	name	_	_
23-4	3890-3891	\|	_	_
23-5	3891-3900	embedding	_	_
23-6	3901-3904	dim	_	_
23-7	3904-3905	\|	_	_
23-8	3905-3912	mlp-dim	_	_
23-9	3912-3913	\|	_	_
23-10	3913-3918	heads	_	_
23-11	3918-3919	\|	_	_
23-12	3919-3924	depth	_	_
23-13	3924-3925	\|	_	_
23-14	3925-3926	#	_	_
23-15	3926-3932	params	_	_
23-16	3932-3933	\|	_	_
23-17	3933-3935	V2	_	_
23-18	3935-3936	-	_	_
23-19	3936-3938	12	_	_
23-20	3939-3947	accuracy	_	_
23-21	3947-3948	\|	_	_
23-22	3948-3959	pre-trained	_	_
23-23	3959-3960	\|	_	_
23-24	3961-3962	\|	_	_
23-25	3962-3963	:	_	_
23-26	3963-3964	-	_	_
23-27	3964-3965	:	_	_
23-28	3965-3966	\|	_	_
23-29	3966-3967	:	_	_
23-30	3967-3968	-	_	_
23-31	3968-3969	:	_	_
23-32	3969-3970	\|	_	_
23-33	3970-3971	:	_	_
23-34	3971-3972	-	_	_
23-35	3972-3973	:	_	_
23-36	3973-3974	\|	_	_
23-37	3974-3975	:	_	_
23-38	3975-3976	-	_	_
23-39	3976-3977	:	_	_
23-40	3977-3978	\|	_	_
23-41	3978-3979	:	_	_
23-42	3979-3980	-	_	_
23-43	3980-3981	:	_	_
23-44	3981-3982	\|	_	_
23-45	3982-3983	:	_	_
23-46	3983-3984	-	_	_
23-47	3984-3985	:	_	_
23-48	3985-3986	\|	_	_
23-49	3986-3987	:	_	_
23-50	3987-3988	-	_	_
23-51	3988-3989	:	_	_
23-52	3989-3990	\|	_	_
23-53	3990-3991	:	_	_
23-54	3991-3992	-	_	_
23-55	3992-3993	:	_	_
23-56	3993-3994	\|	_	_
23-57	3995-3996	\|	_	_
23-58	3996-3999	KWT	_	_
23-59	3999-4000	-	_	_
23-60	4000-4001	1	_	_
23-61	4001-4002	\|	_	_
23-62	4002-4004	64	_	_
23-63	4004-4005	\|	_	_
23-64	4005-4008	128	_	_
23-65	4008-4009	\|	_	_
23-66	4009-4010	1	_	_
23-67	4010-4011	\|	_	_
23-68	4011-4013	12	_	_
23-69	4013-4014	\|	_	_
23-70	4014-4018	607K	_	_
23-71	4018-4019	\|	_	_
23-72	4019-4023	97.7	_	_
23-73	4023-4024	\|	_	_
23-74	4024-4025	\[	_	_
23-75	4025-4029	here	_	_
23-76	4029-4030	\]	_	_
23-77	4030-4031	(	_	_
23-78	4031-4045	models\_data\_v2	_	_
23-79	4045-4046	\_	_	_
23-80	4046-4048	12	_	_
23-81	4048-4049	\_	_	_
23-82	4049-4055	labels	_	_
23-83	4055-4056	/	_	_
23-84	4056-4060	kwt1	_	_
23-85	4060-4061	)	_	_
23-86	4061-4062	\|	_	_
23-87	4063-4064	\|	_	_
23-88	4064-4067	KWT	_	_
23-89	4067-4068	-	_	_
23-90	4068-4069	2	_	_
23-91	4069-4070	\|	_	_
23-92	4070-4073	128	_	_
23-93	4073-4074	\|	_	_
23-94	4074-4077	256	_	_
23-95	4077-4078	\|	_	_
23-96	4078-4079	2	_	_
23-97	4079-4080	\|	_	_
23-98	4080-4082	12	_	_
23-99	4082-4083	\|	_	_
23-100	4083-4087	2.4M	_	_
23-101	4087-4088	\|	_	_
23-102	4088-4092	98.2	_	_
23-103	4092-4093	\|	_	_
23-104	4093-4094	\[	_	_
23-105	4094-4098	here	_	_
23-106	4098-4099	\]	_	_
23-107	4099-4100	(	_	_
23-108	4100-4114	models\_data\_v2	_	_
23-109	4114-4115	\_	_	_
23-110	4115-4117	12	_	_
23-111	4117-4118	\_	_	_
23-112	4118-4124	labels	_	_
23-113	4124-4125	/	_	_
23-114	4125-4129	kwt2	_	_
23-115	4129-4130	)	_	_
23-116	4130-4131	\|	_	_
23-117	4132-4133	\|	_	_
23-118	4133-4136	KWT	_	_
23-119	4136-4137	-	_	_
23-120	4137-4138	3	_	_
23-121	4138-4139	\|	_	_
23-122	4139-4142	192	_	_
23-123	4142-4143	\|	_	_
23-124	4143-4146	768	_	_
23-125	4146-4147	\|	_	_
23-126	4147-4148	3	_	_
23-127	4148-4149	\|	_	_
23-128	4149-4151	12	_	_
23-129	4151-4152	\|	_	_
23-130	4152-4156	5.5M	_	_
23-131	4156-4157	\|	_	_
23-132	4157-4161	98.7	_	_
23-133	4161-4162	\|	_	_
23-134	4162-4163	\[	_	_
23-135	4163-4167	here	_	_
23-136	4167-4168	\]	_	_
23-137	4168-4169	(	_	_
23-138	4169-4183	models\_data\_v2	_	_
23-139	4183-4184	\_	_	_
23-140	4184-4186	12	_	_
23-141	4186-4187	\_	_	_
23-142	4187-4193	labels	_	_
23-143	4193-4194	/	_	_
23-144	4194-4198	kwt3	_	_
23-145	4198-4199	)	_	_
23-146	4199-4200	\|	_	_
23-147	4202-4204	To	_	_
23-148	4205-4212	perform	_	_
23-149	4213-4222	inference	_	_
23-150	4223-4225	on	_	_
23-151	4226-4232	Google	_	_
23-152	4233-4239	Speech	_	_
23-153	4240-4248	Commands	_	_
23-154	4249-4251	v2	_	_
23-155	4252-4256	with	_	_
23-156	4257-4259	12	_	_
23-157	4260-4266	labels	_	_
23-158	4266-4267	,	_	_
23-159	4268-4271	run	_	_
23-160	4273-4274	`	_	_
23-161	4274-4275	`	_	_
23-162	4275-4276	`	_	_
23-163	4276-4281	shell	_	_
23-164	4282-4284	sh	_	_
23-165	4285-4292	eval.sh	_	_
23-166	4293-4294	`	_	_
23-167	4294-4295	`	_	_
23-168	4295-4296	`	_	_
23-169	4298-4299	#	_	_
23-170	4299-4300	#	_	_
23-171	4301-4317	Acknowledgements	_	_
23-172	4319-4322	The	_	_
23-173	4323-4327	code	_	_
23-174	4328-4335	heavily	_	_
23-175	4336-4343	borrows	_	_
23-176	4344-4348	from	_	_
23-177	4349-4352	the	_	_
23-178	4353-4354	\[	_	_
23-179	4354-4357	KWS	_	_
23-180	4358-4367	streaming	_	_
23-181	4368-4372	work	_	_
23-182	4372-4373	\]	_	_
23-183	4373-4374	(	_	_
23-184	4374-4379	https	_	_
23-185	4379-4380	:	_	_
23-186	4380-4381	/	_	_
23-187	4381-4382	/	_	_
23-188	4382-4392	github.com	_	_
23-189	4392-4393	/	_	_
23-190	4393-4408	google-research	_	_
23-191	4408-4409	/	_	_
23-192	4409-4424	google-research	_	_
23-193	4424-4425	/	_	_
23-194	4425-4429	tree	_	_
23-195	4429-4430	/	_	_
23-196	4430-4436	master	_	_
23-197	4436-4437	/	_	_
23-198	4437-4450	kws\_streaming	_	_
23-199	4450-4451	)	_	_
23-200	4452-4454	by	_	_
23-201	4455-4461	Google	_	_
23-202	4462-4470	Research	_	_
23-203	4470-4471	.	_	_

#Text=For a more detailed description of the code structure, see the original authors' \[README\](kws\_streaming/README.md).
24-1	4472-4475	For	_	_
24-2	4476-4477	a	_	_
24-3	4478-4482	more	_	_
24-4	4483-4491	detailed	_	_
24-5	4492-4503	description	_	_
24-6	4504-4506	of	_	_
24-7	4507-4510	the	_	_
24-8	4511-4515	code	_	_
24-9	4516-4525	structure	_	_
24-10	4525-4526	,	_	_
24-11	4527-4530	see	_	_
24-12	4531-4534	the	_	_
24-13	4535-4543	original	_	_
24-14	4544-4551	authors	_	_
24-15	4551-4552	'	_	_
24-16	4553-4554	\[	_	_
24-17	4554-4560	README	_	_
24-18	4560-4561	\]	_	_
24-19	4561-4562	(	_	_
24-20	4562-4575	kws\_streaming	_	_
24-21	4575-4576	/	_	_
24-22	4576-4585	README.md	_	_
24-23	4585-4586	)	_	_
24-24	4586-4587	.	_	_

#Text=We also exploit training techniques from \[DeiT\](https://github.com/facebookresearch/deit).
25-1	4589-4591	We	_	_
25-2	4592-4596	also	_	_
25-3	4597-4604	exploit	_	_
25-4	4605-4613	training	_	_
25-5	4614-4624	techniques	_	_
25-6	4625-4629	from	_	_
25-7	4630-4631	\[	_	_
25-8	4631-4635	DeiT	_	_
25-9	4635-4636	\]	_	_
25-10	4636-4637	(	_	_
25-11	4637-4642	https	_	_
25-12	4642-4643	:	_	_
25-13	4643-4644	/	_	_
25-14	4644-4645	/	_	_
25-15	4645-4655	github.com	_	_
25-16	4655-4656	/	_	_
25-17	4656-4672	facebookresearch	_	_
25-18	4672-4673	/	_	_
25-19	4673-4677	deit	_	_
25-20	4677-4678	)	_	_
25-21	4678-4679	.	_	_

#Text=We thank the authors for sharing their code.
26-1	4681-4683	We	_	_
26-2	4684-4689	thank	_	_
26-3	4690-4693	the	_	_
26-4	4694-4701	authors	_	_
26-5	4702-4705	for	_	_
26-6	4706-4713	sharing	_	_
26-7	4714-4719	their	_	_
26-8	4720-4724	code	_	_
26-9	4724-4725	.	_	_

#Text=Please consider citing them as well if you use our code.  ## License  The source files in this repository are released under the \[Apache 2.0\](LICENSE.txt) license.
27-1	4726-4732	Please	_	_
27-2	4733-4741	consider	_	_
27-3	4742-4748	citing	_	_
27-4	4749-4753	them	_	_
27-5	4754-4756	as	_	_
27-6	4757-4761	well	_	_
27-7	4762-4764	if	_	_
27-8	4765-4768	you	_	_
27-9	4769-4772	use	_	_
27-10	4773-4776	our	_	_
27-11	4777-4781	code	_	_
27-12	4781-4782	.	_	_
27-13	4784-4785	#	_	_
27-14	4785-4786	#	_	_
27-15	4787-4794	License	_	_
27-16	4796-4799	The	_	_
27-17	4800-4806	source	_	_
27-18	4807-4812	files	_	_
27-19	4813-4815	in	_	_
27-20	4816-4820	this	_	_
27-21	4821-4831	repository	_	_
27-22	4832-4835	are	_	_
27-23	4836-4844	released	_	_
27-24	4845-4850	under	_	_
27-25	4851-4854	the	_	_
27-26	4855-4856	\[	_	_
27-27	4856-4862	Apache	_	_
27-28	4863-4866	2.0	_	_
27-29	4866-4867	\]	_	_
27-30	4867-4868	(	_	_
27-31	4868-4879	LICENSE.txt	_	_
27-32	4879-4880	)	_	_
27-33	4881-4888	license	_	_
27-34	4888-4889	.	_	_

#Text=Some source files are derived from the \[KWS streaming repository\](https://github.com/google-research/google-research/tree/master/kws\_streaming) by Google Research.
28-1	4891-4895	Some	_	_
28-2	4896-4902	source	_	_
28-3	4903-4908	files	_	_
28-4	4909-4912	are	_	_
28-5	4913-4920	derived	_	_
28-6	4921-4925	from	_	_
28-7	4926-4929	the	_	_
28-8	4930-4931	\[	_	_
28-9	4931-4934	KWS	_	_
28-10	4935-4944	streaming	_	_
28-11	4945-4955	repository	_	_
28-12	4955-4956	\]	_	_
28-13	4956-4957	(	_	_
28-14	4957-4962	https	_	_
28-15	4962-4963	:	_	_
28-16	4963-4964	/	_	_
28-17	4964-4965	/	_	_
28-18	4965-4975	github.com	_	_
28-19	4975-4976	/	_	_
28-20	4976-4991	google-research	_	_
28-21	4991-4992	/	_	_
28-22	4992-5007	google-research	_	_
28-23	5007-5008	/	_	_
28-24	5008-5012	tree	_	_
28-25	5012-5013	/	_	_
28-26	5013-5019	master	_	_
28-27	5019-5020	/	_	_
28-28	5020-5033	kws\_streaming	_	_
28-29	5033-5034	)	_	_
28-30	5035-5037	by	_	_
28-31	5038-5044	Google	_	_
28-32	5045-5053	Research	_	_
28-33	5053-5054	.	_	_

#Text=These are also released under the Apache 2.0 license, the text of which can be seen in the LICENSE file on their repository.
29-1	5055-5060	These	_	_
29-2	5061-5064	are	_	_
29-3	5065-5069	also	_	_
29-4	5070-5078	released	_	_
29-5	5079-5084	under	_	_
29-6	5085-5088	the	_	_
29-7	5089-5095	Apache	_	_
29-8	5096-5099	2.0	_	_
29-9	5100-5107	license	_	_
29-10	5107-5108	,	_	_
29-11	5109-5112	the	_	_
29-12	5113-5117	text	_	_
29-13	5118-5120	of	_	_
29-14	5121-5126	which	_	_
29-15	5127-5130	can	_	_
29-16	5131-5133	be	_	_
29-17	5134-5138	seen	_	_
29-18	5139-5141	in	_	_
29-19	5142-5145	the	_	_
29-20	5146-5153	LICENSE	_	_
29-21	5154-5158	file	_	_
29-22	5159-5161	on	_	_
29-23	5162-5167	their	_	_
29-24	5168-5178	repository	_	_
29-25	5178-5179	.	_	_